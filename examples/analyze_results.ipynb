{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c716be8a",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6bcaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "os.makedirs('tables', exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25f895",
   "metadata": {},
   "source": [
    "## 2. Load All Experimental Data\n",
    "\n",
    "Load JSON summaries and CSV logs from all four environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "environments = {\n",
    "    'ant_omni': 'seed_variability_logs',\n",
    "    'walker2d': 'seed_variability_logs_walker2d',\n",
    "    'humanoid_omni': 'seed_variability_logs_humanoid_omni',\n",
    "    'halfcheetah_uni': 'seed_variability_logs_halfcheetah_uni'\n",
    "}\n",
    "\n",
    "configs = ['baseline', 'dns_ga_g300', 'dns_ga_g1000']\n",
    "seeds = list(range(42, 73))  # 31 seeds\n",
    "\n",
    "def load_json_summaries(environments):\n",
    "    \"\"\"Load all JSON summary files into a DataFrame.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for env_name, log_dir in environments.items():\n",
    "        if not os.path.exists(log_dir):\n",
    "            print(f\"Warning: {log_dir} does not exist yet. Skipping {env_name}.\")\n",
    "            continue\n",
    "            \n",
    "        for config in configs:\n",
    "            for seed in seeds:\n",
    "                exp_name = f\"{config}_seed{seed}\"\n",
    "                json_file = os.path.join(log_dir, f\"{exp_name}_summary.json\")\n",
    "                \n",
    "                if os.path.exists(json_file):\n",
    "                    with open(json_file, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                        data['environment'] = env_name\n",
    "                        data['config'] = config\n",
    "                        data['seed'] = seed\n",
    "                        all_data.append(data)\n",
    "                else:\n",
    "                    print(f\"Missing: {json_file}\")\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "# Load all data\n",
    "df_summary = load_json_summaries(environments)\n",
    "\n",
    "print(f\"Loaded {len(df_summary)} experiments\")\n",
    "print(f\"Environments: {df_summary['environment'].unique()}\")\n",
    "print(f\"Configs: {df_summary['config'].unique()}\")\n",
    "print(f\"Seeds per config: {df_summary.groupby(['environment', 'config']).size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486dc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_logs(environments):\n",
    "    \"\"\"Load all CSV logs with iteration-level data.\"\"\"\n",
    "    all_logs = []\n",
    "    \n",
    "    for env_name, log_dir in environments.items():\n",
    "        if not os.path.exists(log_dir):\n",
    "            continue\n",
    "            \n",
    "        for config in configs:\n",
    "            for seed in seeds:\n",
    "                exp_name = f\"{config}_seed{seed}\"\n",
    "                csv_file = os.path.join(log_dir, f\"{exp_name}.csv\")\n",
    "                \n",
    "                if os.path.exists(csv_file):\n",
    "                    df_log = pd.read_csv(csv_file)\n",
    "                    df_log['environment'] = env_name\n",
    "                    df_log['config'] = config\n",
    "                    df_log['seed'] = seed\n",
    "                    all_logs.append(df_log)\n",
    "    \n",
    "    if all_logs:\n",
    "        return pd.concat(all_logs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load all CSV logs\n",
    "df_logs = load_csv_logs(environments)\n",
    "\n",
    "if not df_logs.empty:\n",
    "    print(f\"Loaded {len(df_logs)} iteration records\")\n",
    "    print(f\"Columns: {df_logs.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"No CSV logs loaded yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336dccba",
   "metadata": {},
   "source": [
    "## 3. Summary Statistics\n",
    "\n",
    "Compute mean and standard deviation for each configuration across all seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45994ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_summary.empty:\n",
    "    # Group by environment and config\n",
    "    summary_stats = df_summary.groupby(['environment', 'config']).agg({\n",
    "        'final_qd_score': ['mean', 'std', 'count'],\n",
    "        'final_coverage': ['mean', 'std'],\n",
    "        'final_max_fitness': ['mean', 'std'],\n",
    "        'wall_time': ['mean', 'std']\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\n=== Summary Statistics ===\")\n",
    "    print(summary_stats)\n",
    "    \n",
    "    # Save to CSV\n",
    "    summary_stats.to_csv('tables/summary_statistics.csv')\n",
    "    print(\"\\nSaved to tables/summary_statistics.csv\")\n",
    "else:\n",
    "    print(\"No summary data available yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f5924",
   "metadata": {},
   "source": [
    "## 4. Figure 1-4: QD-Score Over Iterations (All Environments)\n",
    "\n",
    "One plot per environment showing mean ¬± std across all seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25dd2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_over_iterations(df_logs, env_name, metric, ylabel, filename):\n",
    "    \"\"\"Plot a metric over iterations for one environment.\"\"\"\n",
    "    df_env = df_logs[df_logs['environment'] == env_name]\n",
    "    \n",
    "    if df_env.empty:\n",
    "        print(f\"No data for {env_name}\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = {'baseline': '#1f77b4', 'dns_ga_g300': '#ff7f0e', 'dns_ga_g1000': '#2ca02c'}\n",
    "    labels = {'baseline': 'DNS Baseline', 'dns_ga_g300': 'DNS-GA (g=300)', 'dns_ga_g1000': 'DNS-GA (g=1000)'}\n",
    "    \n",
    "    for config in configs:\n",
    "        df_config = df_env[df_env['config'] == config]\n",
    "        \n",
    "        if df_config.empty:\n",
    "            continue\n",
    "        \n",
    "        # Group by iteration and compute mean/std\n",
    "        grouped = df_config.groupby('iteration')[metric].agg(['mean', 'std']).reset_index()\n",
    "        \n",
    "        ax.plot(grouped['iteration'], grouped['mean'], \n",
    "                label=labels[config], color=colors[config], linewidth=2)\n",
    "        ax.fill_between(grouped['iteration'], \n",
    "                       grouped['mean'] - grouped['std'],\n",
    "                       grouped['mean'] + grouped['std'],\n",
    "                       color=colors[config], alpha=0.2)\n",
    "    \n",
    "    ax.set_xlabel('Iterations', fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "    ax.set_title(f\"{ylabel} - {env_name.replace('_', ' ').title()}\", fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/{filename}', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: figures/{filename}\")\n",
    "\n",
    "# Generate QD-Score plots for all environments\n",
    "if not df_logs.empty:\n",
    "    for env_name in environments.keys():\n",
    "        plot_metric_over_iterations(df_logs, env_name, 'qd_score', 'QD-Score', \n",
    "                                   f'qd_score_{env_name}.png')\n",
    "else:\n",
    "    print(\"No iteration data available yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d8aa3f",
   "metadata": {},
   "source": [
    "## 5. Figure 5-8: Coverage Over Iterations (All Environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93598d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_logs.empty:\n",
    "    for env_name in environments.keys():\n",
    "        plot_metric_over_iterations(df_logs, env_name, 'coverage', 'Coverage (%)', \n",
    "                                   f'coverage_{env_name}.png')\n",
    "else:\n",
    "    print(\"No iteration data available yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77efb0e",
   "metadata": {},
   "source": [
    "## 6. Figure 9-12: Max Fitness Over Iterations (All Environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_logs.empty:\n",
    "    for env_name in environments.keys():\n",
    "        plot_metric_over_iterations(df_logs, env_name, 'max_fitness', 'Max Fitness', \n",
    "                                   f'max_fitness_{env_name}.png')\n",
    "else:\n",
    "    print(\"No iteration data available yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06a2b3",
   "metadata": {},
   "source": [
    "## 7. Table 1: Final Performance Comparison\n",
    "\n",
    "Mean ¬± std of final QD-score, coverage, and max fitness for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2245a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_summary.empty:\n",
    "    def format_mean_std(mean, std):\n",
    "        return f\"{mean:.2f} ¬± {std:.2f}\"\n",
    "    \n",
    "    table_data = []\n",
    "    \n",
    "    for env_name in environments.keys():\n",
    "        df_env = df_summary[df_summary['environment'] == env_name]\n",
    "        \n",
    "        if df_env.empty:\n",
    "            continue\n",
    "        \n",
    "        for config in configs:\n",
    "            df_config = df_env[df_env['config'] == config]\n",
    "            \n",
    "            if df_config.empty:\n",
    "                continue\n",
    "            \n",
    "            qd_mean = df_config['final_qd_score'].mean()\n",
    "            qd_std = df_config['final_qd_score'].std()\n",
    "            cov_mean = df_config['final_coverage'].mean()\n",
    "            cov_std = df_config['final_coverage'].std()\n",
    "            fit_mean = df_config['final_max_fitness'].mean()\n",
    "            fit_std = df_config['final_max_fitness'].std()\n",
    "            \n",
    "            table_data.append({\n",
    "                'Environment': env_name,\n",
    "                'Configuration': config,\n",
    "                'QD-Score': format_mean_std(qd_mean, qd_std),\n",
    "                'Coverage (%)': format_mean_std(cov_mean, cov_std),\n",
    "                'Max Fitness': format_mean_std(fit_mean, fit_std)\n",
    "            })\n",
    "    \n",
    "    df_table1 = pd.DataFrame(table_data)\n",
    "    print(\"\\n=== Table 1: Final Performance Comparison ===\")\n",
    "    print(df_table1.to_string(index=False))\n",
    "    \n",
    "    df_table1.to_csv('tables/table1_performance.csv', index=False)\n",
    "    print(\"\\nSaved to tables/table1_performance.csv\")\n",
    "else:\n",
    "    print(\"No summary data available yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da817a6",
   "metadata": {},
   "source": [
    "## 8. Table 2: Statistical Significance Tests\n",
    "\n",
    "Paired t-tests comparing DNS-GA variants against baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55923307",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_summary.empty:\n",
    "    def compute_effect_size(baseline, treatment):\n",
    "        \"\"\"Compute Cohen's d effect size.\"\"\"\n",
    "        pooled_std = np.sqrt((baseline.std()**2 + treatment.std()**2) / 2)\n",
    "        return (treatment.mean() - baseline.mean()) / pooled_std\n",
    "    \n",
    "    statistical_results = []\n",
    "    \n",
    "    for env_name in environments.keys():\n",
    "        df_env = df_summary[df_summary['environment'] == env_name]\n",
    "        \n",
    "        if df_env.empty:\n",
    "            continue\n",
    "        \n",
    "        baseline_data = df_env[df_env['config'] == 'baseline']\n",
    "        \n",
    "        for dns_config in ['dns_ga_g300', 'dns_ga_g1000']:\n",
    "            treatment_data = df_env[df_env['config'] == dns_config]\n",
    "            \n",
    "            if baseline_data.empty or treatment_data.empty:\n",
    "                continue\n",
    "            \n",
    "            # Ensure same seeds for paired test\n",
    "            baseline_sorted = baseline_data.sort_values('seed')\n",
    "            treatment_sorted = treatment_data.sort_values('seed')\n",
    "            \n",
    "            # Paired t-test on QD-score\n",
    "            t_stat, p_value = stats.ttest_rel(treatment_sorted['final_qd_score'], \n",
    "                                             baseline_sorted['final_qd_score'])\n",
    "            \n",
    "            effect_size = compute_effect_size(baseline_sorted['final_qd_score'],\n",
    "                                            treatment_sorted['final_qd_score'])\n",
    "            \n",
    "            improvement = ((treatment_sorted['final_qd_score'].mean() - \n",
    "                          baseline_sorted['final_qd_score'].mean()) / \n",
    "                          baseline_sorted['final_qd_score'].mean() * 100)\n",
    "            \n",
    "            statistical_results.append({\n",
    "                'Environment': env_name,\n",
    "                'Comparison': f'{dns_config} vs baseline',\n",
    "                'Improvement (%)': f\"{improvement:.2f}\",\n",
    "                't-statistic': f\"{t_stat:.3f}\",\n",
    "                'p-value': f\"{p_value:.4f}\",\n",
    "                'Significant': 'Yes' if p_value < 0.05 else 'No',\n",
    "                'Effect Size (d)': f\"{effect_size:.3f}\"\n",
    "            })\n",
    "    \n",
    "    df_table2 = pd.DataFrame(statistical_results)\n",
    "    print(\"\\n=== Table 2: Statistical Significance ===\")\n",
    "    print(df_table2.to_string(index=False))\n",
    "    \n",
    "    df_table2.to_csv('tables/table2_significance.csv', index=False)\n",
    "    print(\"\\nSaved to tables/table2_significance.csv\")\n",
    "else:\n",
    "    print(\"No summary data available yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf3614",
   "metadata": {},
   "source": [
    "## 9. Table 3: Computational Efficiency\n",
    "\n",
    "Wall time and overhead analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e82f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_summary.empty:\n",
    "    efficiency_data = []\n",
    "    \n",
    "    for env_name in environments.keys():\n",
    "        df_env = df_summary[df_summary['environment'] == env_name]\n",
    "        \n",
    "        if df_env.empty:\n",
    "            continue\n",
    "        \n",
    "        for config in configs:\n",
    "            df_config = df_env[df_env['config'] == config]\n",
    "            \n",
    "            if df_config.empty:\n",
    "                continue\n",
    "            \n",
    "            wall_time = df_config['wall_time'].mean()\n",
    "            wall_time_std = df_config['wall_time'].std()\n",
    "            \n",
    "            # GA overhead (if available)\n",
    "            if 'ga_overhead_percent' in df_config.columns:\n",
    "                ga_overhead = df_config['ga_overhead_percent'].mean()\n",
    "                ga_overhead_std = df_config['ga_overhead_percent'].std()\n",
    "            else:\n",
    "                ga_overhead = 0.0\n",
    "                ga_overhead_std = 0.0\n",
    "            \n",
    "            efficiency_data.append({\n",
    "                'Environment': env_name,\n",
    "                'Configuration': config,\n",
    "                'Wall Time (min)': f\"{wall_time:.2f} ¬± {wall_time_std:.2f}\",\n",
    "                'GA Overhead (%)': f\"{ga_overhead:.2f} ¬± {ga_overhead_std:.2f}\" if ga_overhead > 0 else 'N/A'\n",
    "            })\n",
    "    \n",
    "    df_table3 = pd.DataFrame(efficiency_data)\n",
    "    print(\"\\n=== Table 3: Computational Efficiency ===\")\n",
    "    print(df_table3.to_string(index=False))\n",
    "    \n",
    "    df_table3.to_csv('tables/table3_efficiency.csv', index=False)\n",
    "    print(\"\\nSaved to tables/table3_efficiency.csv\")\n",
    "else:\n",
    "    print(\"No summary data available yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20604005",
   "metadata": {},
   "source": [
    "## 10. Figure 13-14: Behavior Space Heatmaps\n",
    "\n",
    "Visualize behavior space coverage for humanoid_omni and halfcheetah_uni.\n",
    "Create 2√ó2 grids comparing baseline vs DNS-GA for interesting seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_behavior_space_heatmap(env_name, log_dir, seed, configs_to_plot, output_file):\n",
    "    \"\"\"Create 2√ó2 heatmap comparing configurations for one seed.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(log_dir):\n",
    "        print(f\"Directory {log_dir} does not exist yet\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    titles = {\n",
    "        'baseline': 'DNS Baseline',\n",
    "        'dns_ga_g300': 'DNS-GA (g=300)',\n",
    "        'dns_ga_g1000': 'DNS-GA (g=1000)'\n",
    "    }\n",
    "    \n",
    "    for idx, config in enumerate(configs_to_plot):\n",
    "        exp_name = f\"{config}_seed{seed}\"\n",
    "        repertoire_file = os.path.join(log_dir, f\"{exp_name}_repertoire.npz\")\n",
    "        \n",
    "        if not os.path.exists(repertoire_file):\n",
    "            axes[idx].text(0.5, 0.5, f'No data for {config}', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[idx].set_title(titles[config], fontsize=12, fontweight='bold')\n",
    "            continue\n",
    "        \n",
    "        # Load repertoire data\n",
    "        data = np.load(repertoire_file)\n",
    "        descriptors = data['descriptors']\n",
    "        fitnesses = data['fitnesses']\n",
    "        \n",
    "        # Filter valid solutions (non-empty cells)\n",
    "        valid_mask = ~np.isnan(fitnesses)\n",
    "        valid_desc = descriptors[valid_mask]\n",
    "        valid_fit = fitnesses[valid_mask]\n",
    "        \n",
    "        if len(valid_desc) == 0:\n",
    "            axes[idx].text(0.5, 0.5, 'No valid solutions', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[idx].set_title(titles[config], fontsize=12, fontweight='bold')\n",
    "            continue\n",
    "        \n",
    "        # Create 2D histogram\n",
    "        x_desc = valid_desc[:, 0]\n",
    "        y_desc = valid_desc[:, 1]\n",
    "        \n",
    "        # Use fitness as weights for coloring\n",
    "        h = axes[idx].hist2d(x_desc, y_desc, bins=50, cmap='viridis', \n",
    "                            weights=valid_fit, cmin=0.1)\n",
    "        \n",
    "        axes[idx].set_xlabel('Descriptor 1', fontsize=10)\n",
    "        axes[idx].set_ylabel('Descriptor 2', fontsize=10)\n",
    "        axes[idx].set_title(titles[config], fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.colorbar(h[3], ax=axes[idx], label='Fitness')\n",
    "    \n",
    "    # Hide unused subplot if only 3 configs\n",
    "    if len(configs_to_plot) == 3:\n",
    "        axes[3].axis('off')\n",
    "    \n",
    "    fig.suptitle(f\"Behavior Space Coverage - {env_name.replace('_', ' ').title()} (Seed {seed})\",\n",
    "                fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/{output_file}', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: figures/{output_file}\")\n",
    "\n",
    "# Generate heatmaps for humanoid_omni and halfcheetah_uni\n",
    "# Use seed 42 as the standard seed\n",
    "heatmap_envs = [\n",
    "    ('humanoid_omni', 'seed_variability_logs_humanoid_omni'),\n",
    "    ('halfcheetah_uni', 'seed_variability_logs_halfcheetah_uni')\n",
    "]\n",
    "\n",
    "for env_name, log_dir in heatmap_envs:\n",
    "    plot_behavior_space_heatmap(\n",
    "        env_name, log_dir, seed=42, \n",
    "        configs_to_plot=configs,\n",
    "        output_file=f'behavior_space_{env_name}.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3f083f",
   "metadata": {},
   "source": [
    "## 11. Additional Visualizations\n",
    "\n",
    "Generate supplementary figures for paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce174ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box_comparison(df_summary, metric, ylabel, filename):\n",
    "    \"\"\"Create box plots comparing all configurations across environments.\"\"\"\n",
    "    \n",
    "    if df_summary.empty:\n",
    "        print(f\"No data available for {filename}\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, env_name in enumerate(environments.keys()):\n",
    "        df_env = df_summary[df_summary['environment'] == env_name]\n",
    "        \n",
    "        if df_env.empty:\n",
    "            axes[idx].text(0.5, 0.5, f'No data for {env_name}', \n",
    "                          ha='center', va='center', fontsize=12)\n",
    "            axes[idx].set_title(env_name.replace('_', ' ').title(), fontsize=12, fontweight='bold')\n",
    "            continue\n",
    "        \n",
    "        # Prepare data for box plot\n",
    "        plot_data = []\n",
    "        labels = []\n",
    "        \n",
    "        for config in configs:\n",
    "            df_config = df_env[df_env['config'] == config]\n",
    "            if not df_config.empty:\n",
    "                plot_data.append(df_config[metric].values)\n",
    "                config_label = config.replace('baseline', 'Baseline').replace('dns_ga_g', 'DNS-GA g')\n",
    "                labels.append(config_label)\n",
    "        \n",
    "        bp = axes[idx].boxplot(plot_data, labels=labels, patch_artist=True)\n",
    "        \n",
    "        # Color boxes\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "        for patch, color in zip(bp['boxes'], colors[:len(plot_data)]):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.6)\n",
    "        \n",
    "        axes[idx].set_ylabel(ylabel, fontsize=10)\n",
    "        axes[idx].set_title(env_name.replace('_', ' ').title(), fontsize=12, fontweight='bold')\n",
    "        axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "        axes[idx].tick_params(axis='x', rotation=15)\n",
    "    \n",
    "    fig.suptitle(f\"{ylabel} Distribution Across All Seeds\", fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/{filename}', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: figures/{filename}\")\n",
    "\n",
    "# Generate box plots for final metrics\n",
    "if not df_summary.empty:\n",
    "    plot_box_comparison(df_summary, 'final_qd_score', 'Final QD-Score', 'box_qd_score_comparison.png')\n",
    "    plot_box_comparison(df_summary, 'final_coverage', 'Final Coverage (%)', 'box_coverage_comparison.png')\n",
    "    plot_box_comparison(df_summary, 'final_max_fitness', 'Final Max Fitness', 'box_fitness_comparison.png')\n",
    "else:\n",
    "    print(\"No summary data available for box plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326c3b43",
   "metadata": {},
   "source": [
    "## 12. Export Summary for Paper\n",
    "\n",
    "Generate formatted text snippets for paper writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df11345",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_summary.empty:\n",
    "    with open('tables/paper_snippets.txt', 'w') as f:\n",
    "        f.write(\"PAPER WRITING SNIPPETS\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        for env_name in environments.keys():\n",
    "            df_env = df_summary[df_summary['environment'] == env_name]\n",
    "            \n",
    "            if df_env.empty:\n",
    "                continue\n",
    "            \n",
    "            f.write(f\"\\n{env_name.upper()}\\n\")\n",
    "            f.write(\"-\"*60 + \"\\n\")\n",
    "            \n",
    "            baseline = df_env[df_env['config'] == 'baseline']\n",
    "            g300 = df_env[df_env['config'] == 'dns_ga_g300']\n",
    "            g1000 = df_env[df_env['config'] == 'dns_ga_g1000']\n",
    "            \n",
    "            if not baseline.empty and not g300.empty:\n",
    "                improvement_300 = ((g300['final_qd_score'].mean() - baseline['final_qd_score'].mean()) \n",
    "                                  / baseline['final_qd_score'].mean() * 100)\n",
    "                f.write(f\"DNS-GA (g=300) improves QD-score by {improvement_300:.1f}% over baseline\\n\")\n",
    "                f.write(f\"  Baseline: {baseline['final_qd_score'].mean():.2f} ¬± {baseline['final_qd_score'].std():.2f}\\n\")\n",
    "                f.write(f\"  DNS-GA g300: {g300['final_qd_score'].mean():.2f} ¬± {g300['final_qd_score'].std():.2f}\\n\")\n",
    "            \n",
    "            if not baseline.empty and not g1000.empty:\n",
    "                improvement_1000 = ((g1000['final_qd_score'].mean() - baseline['final_qd_score'].mean()) \n",
    "                                   / baseline['final_qd_score'].mean() * 100)\n",
    "                f.write(f\"\\nDNS-GA (g=1000) improves QD-score by {improvement_1000:.1f}% over baseline\\n\")\n",
    "                f.write(f\"  Baseline: {baseline['final_qd_score'].mean():.2f} ¬± {baseline['final_qd_score'].std():.2f}\\n\")\n",
    "                f.write(f\"  DNS-GA g1000: {g1000['final_qd_score'].mean():.2f} ¬± {g1000['final_qd_score'].std():.2f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(\"\\nPaper snippets saved to tables/paper_snippets.txt\")\n",
    "else:\n",
    "    print(\"No data available for paper snippets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e88f22",
   "metadata": {},
   "source": [
    "## 13. Generate All Outputs\n",
    "\n",
    "Run this cell to generate everything at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be73668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GENERATING ALL FIGURES AND TABLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if df_summary.empty:\n",
    "    print(\"\\n‚ö†Ô∏è  No experimental data loaded yet.\")\n",
    "    print(\"Waiting for experiments to complete...\\n\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Loaded {len(df_summary)} experiments\")\n",
    "    print(f\"‚úì Environments: {list(environments.keys())}\")\n",
    "    print(f\"\\nüìä Outputs:\")\n",
    "    print(\"  - figures/ (12 performance plots + 2 heatmaps + 3 box plots)\")\n",
    "    print(\"  - tables/ (3 statistical tables + summary + paper snippets)\")\n",
    "    print(\"\\nAll analysis complete! Check the output directories.\\n\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
