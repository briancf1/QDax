{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33399b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax[cuda] in /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: jaxlib<=0.8.0,>=0.8.0 in /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages (from jax[cuda]) (0.8.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages (from jax[cuda]) (0.5.3)\n",
      "Requirement already satisfied: numpy>=2.0 in /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages (from jax[cuda]) (2.3.4)\n",
      "Requirement already satisfied: opt_einsum in /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages (from jax[cuda]) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.13 in /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages (from jax[cuda]) (1.16.3)\n",
      "Requirement already satisfied: jaxlib<=0.8.0,>=0.8.0 in /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages (from jax[cuda]) (0.8.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages (from jax[cuda]) (0.5.3)\n",
      "Requirement already satisfied: numpy>=2.0 in /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages (from jax[cuda]) (2.3.4)\n",
      "Requirement already satisfied: opt_einsum in /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages (from jax[cuda]) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.13 in /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages (from jax[cuda]) (1.16.3)\n",
      "INFO: pip is looking at multiple versions of jax[cuda] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached jax-0.7.2-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is looking at multiple versions of jax[cuda] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached jax-0.7.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.7.2,>=0.7.2 (from jax[cuda])\n",
      "  Using cached jaxlib-0.7.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.3 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.7.1,>=0.7.1 (from jax[cuda])\n",
      "  Using cached jaxlib-0.7.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.3 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.7.0,>=0.7.0 (from jax[cuda])\n",
      "  Using cached jaxlib-0.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.3 kB)\n",
      "INFO: pip is still looking at multiple versions of jax[cuda] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.6.2,>=0.6.2 (from jax[cuda])\n",
      "  Using cached jaxlib-0.6.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.3 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.7.2,>=0.7.2 (from jax[cuda])\n",
      "  Using cached jaxlib-0.7.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.3 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.7.1,>=0.7.1 (from jax[cuda])\n",
      "  Using cached jaxlib-0.7.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.3 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.7.0,>=0.7.0 (from jax[cuda])\n",
      "  Using cached jaxlib-0.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.3 kB)\n",
      "INFO: pip is still looking at multiple versions of jax[cuda] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.6.2,>=0.6.2 (from jax[cuda])\n",
      "  Using cached jaxlib-0.6.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.3 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.6.1,>=0.6.1 (from jax[cuda])\n",
      "  Using cached jaxlib-0.6.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax[cuda])\n",
      "  Using cached jaxlib-0.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax[cuda])\n",
      "  Using cached jaxlib-0.5.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.2,>=0.5.1 (from jax[cuda])\n",
      "  Using cached jaxlib-0.5.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (978 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Using cached jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax[cuda])\n",
      "  Using cached jaxlib-0.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (978 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.38-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.0 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.36-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.0 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
      "  Using cached jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax[cuda])\n",
      "Collecting jaxlib<=0.6.1,>=0.6.1 (from jax[cuda])\n",
      "  Using cached jaxlib-0.6.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax[cuda])\n",
      "  Using cached jaxlib-0.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax[cuda])\n",
      "  Using cached jaxlib-0.5.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.2,>=0.5.1 (from jax[cuda])\n",
      "  Using cached jaxlib-0.5.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (978 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Using cached jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax[cuda])\n",
      "  Using cached jaxlib-0.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (978 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.38-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.0 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.36-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.0 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
      "  Using cached jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.35-cp312-cp312-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "  Using cached jaxlib-0.4.34-cp312-cp312-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
      "  Using cached jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.33-cp312-cp312-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.31-cp312-cp312-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.30-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.0 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.29-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.28-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.27-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.26-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.25-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached jax-0.4.24-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached jax-0.4.23-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached jaxlib-0.4.35-cp312-cp312-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "  Using cached jaxlib-0.4.34-cp312-cp312-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
      "  Using cached jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.33-cp312-cp312-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.31-cp312-cp312-macosx_11_0_arm64.whl.metadata (983 bytes)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax[cuda])\n",
      "  Using cached jaxlib-0.4.30-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.0 kB)\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.4.29-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.28-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.27-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.26-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.25-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached jax-0.4.24-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached jax-0.4.23-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached jax-0.4.22-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached jax-0.4.21-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.20-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.19-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.18-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.17-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.16-py3-none-any.whl.metadata (29 kB)\n",
      "  Using cached jax-0.4.14.tar.gz (1.3 MB)\n",
      "  Using cached jax-0.4.22-py3-none-any.whl.metadata (24 kB)\n",
      "  Using cached jax-0.4.21-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.20-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.19-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.18-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.17-py3-none-any.whl.metadata (23 kB)\n",
      "  Using cached jax-0.4.16-py3-none-any.whl.metadata (29 kB)\n",
      "  Using cached jax-0.4.14.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25h  Using cached jax-0.4.13.tar.gz (1.3 MB)\n",
      "\bdone\n",
      "\u001b[?25h  Using cached jax-0.4.13.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25h  Using cached jax-0.4.12.tar.gz (1.3 MB)\n",
      "\bdone\n",
      "\u001b[?25h  Using cached jax-0.4.12.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25h  Using cached jax-0.4.11.tar.gz (1.3 MB)\n",
      "\bdone\n",
      "\u001b[?25h  Using cached jax-0.4.11.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hdone\n",
      "\u001b[?25h  Using cached jax-0.4.10.tar.gz (1.3 MB)\n",
      "  Using cached jax-0.4.10.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25h  Using cached jax-0.4.9.tar.gz (1.3 MB)\n",
      "\bdone\n",
      "\u001b[?25h  Using cached jax-0.4.9.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25hdone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.8.tar.gz (1.2 MB)\n",
      "done\n",
      "\u001b[?25h  Using cached jax-0.4.8.tar.gz (1.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25h  Using cached jax-0.4.7.tar.gz (1.2 MB)\n",
      "\bdone\n",
      "\u001b[?25h  Using cached jax-0.4.7.tar.gz (1.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25h  Using cached jax-0.4.6.tar.gz (1.2 MB)\n",
      "\bdone\n",
      "\u001b[?25h  Using cached jax-0.4.6.tar.gz (1.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25h  Using cached jax-0.4.5.tar.gz (1.2 MB)\n",
      "\bdone\n",
      "\u001b[?25h  Using cached jax-0.4.5.tar.gz (1.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25h  Using cached jax-0.4.4.tar.gz (1.2 MB)\n",
      "\bdone\n",
      "\u001b[?25h  Using cached jax-0.4.4.tar.gz (1.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25h  Using cached jax-0.4.3.tar.gz (1.2 MB)\n",
      "\bdone\n",
      "\u001b[?25h  Using cached jax-0.4.3.tar.gz (1.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached jax-0.4.2.tar.gz (1.2 MB)\n",
      "done\n",
      "\u001b[?25h  Using cached jax-0.4.2.tar.gz (1.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l^C\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U \"jax[cuda]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fcbdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"git+https://github.com/briancf1/QDax.git#egg=qdax[examples]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413df8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository to get experiment scripts\n",
    "!git clone https://github.com/briancf1/QDax.git\n",
    "%cd QDax/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04508ef4",
   "metadata": {},
   "source": [
    "## STEP 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acef4b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import warp: No module named 'warp'\n",
      "Failed to import mujoco_warp: No module named 'warp'\n",
      "Setup complete!\n",
      "Current directory: /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/examples\n",
      "JAX devices: [CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import functools\n",
    "from multiprocessing import Process, Queue, Manager\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from qdax.core.dns_ga import DominatedNoveltySearchGA\n",
    "from qdax.core.dns import DominatedNoveltySearch\n",
    "import qdax.tasks.brax as environments\n",
    "from qdax.tasks.brax.env_creators import scoring_function_brax_envs as scoring_function\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.core.emitters.standard_emitters import MixingEmitter\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Create experiment logs directory\n",
    "os.makedirs(\"seed_variability_logs\", exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e6083",
   "metadata": {},
   "source": [
    "## Generate Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff749d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATED 31 RANDOM SEEDS\n",
      "================================================================================\n",
      "Seeds: [7817, 52731, 51809, 35457, 47644, 95781, 68031, 49336, 7978, 61378]... (showing first 10)\n",
      "Range: 317 to 99314\n",
      "Total: 31 seeds\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate 31 random seeds for robust statistical analysis\n",
    "np.random.seed(2024)  # Fixed seed for reproducibility of seed generation\n",
    "RANDOM_SEEDS = np.random.randint(1, 100000, size=31).tolist()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATED 31 RANDOM SEEDS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Seeds: {RANDOM_SEEDS[:10]}... (showing first 10)\")\n",
    "print(f\"Range: {min(RANDOM_SEEDS)} to {max(RANDOM_SEEDS)}\")\n",
    "print(f\"Total: {len(RANDOM_SEEDS)} seeds\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save seeds for reproducibility\n",
    "with open('seed_variability_logs/random_seeds.json', 'w') as f:\n",
    "    json.dump({'seeds': RANDOM_SEEDS, 'generation_seed': 2024}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ed96c",
   "metadata": {},
   "source": [
    "## Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db35f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "Fixed Parameters:\n",
      "  Environment: walker2d_uni\n",
      "  Iterations: 3000\n",
      "  Batch size: 100\n",
      "  ISO_SIGMA: 0.01\n",
      "  Population: 1024\n",
      "\n",
      "Main Configurations (31 seeds each):\n",
      "  â€¢ DNS_baseline: No GA\n",
      "  â€¢ DNS-GA_g300_gen2: 10 GA calls (every 300 iters, 2 gens)\n",
      "  â€¢ DNS-GA_g1000_gen4: 3 GA calls (every 1000 iters, 4 gens)\n",
      "\n",
      "Sanity Check Configuration (seed 42 only):\n",
      "  â€¢ DNS-GA_sanity_no_ga: g_n=99999 (no GA triggers)\n",
      "    Purpose: Verify DNS-GA without GA = baseline\n",
      "\n",
      "Total Experiments:\n",
      "  Main: 3 configs Ã— 31 seeds = 93\n",
      "  Sanity: 1\n",
      "  Total: 94\n",
      "  Estimated time (2 parallel): ~2.6 hours\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "FIXED_PARAMS = {\n",
    "    'batch_size': 100,\n",
    "    'env_name': 'walker2d_uni',\n",
    "    'episode_length': 100,\n",
    "    'num_iterations': 3000,\n",
    "    'policy_hidden_layer_sizes': (64, 64),\n",
    "    'population_size': 1024,\n",
    "    'k': 3,\n",
    "    'line_sigma': 0.05,\n",
    "    'iso_sigma': 0.01,  # Best performer from previous experiments\n",
    "}\n",
    "\n",
    "# Main experimental configurations (run with all 31 seeds)\n",
    "MAIN_CONFIGS = [\n",
    "    # Baseline (no GA)\n",
    "    {\n",
    "        'type': 'baseline',\n",
    "        'name': 'DNS_baseline',\n",
    "        'g_n': None,\n",
    "        'num_ga_children': None,\n",
    "        'num_ga_generations': None,\n",
    "    },\n",
    "    # Frequent GA calls (10 times during 3000 iterations)\n",
    "    {\n",
    "        'type': 'dns-ga',\n",
    "        'name': 'DNS-GA_g300_gen2',\n",
    "        'g_n': 300,\n",
    "        'num_ga_children': 2,\n",
    "        'num_ga_generations': 2,\n",
    "    },\n",
    "    # Rare but deep GA calls (3 times during 3000 iterations, seed 42's winner)\n",
    "    {\n",
    "        'type': 'dns-ga',\n",
    "        'name': 'DNS-GA_g1000_gen4',\n",
    "        'g_n': 1000,\n",
    "        'num_ga_children': 2,\n",
    "        'num_ga_generations': 4,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Sanity check: DNS-GA with g_n so large it never triggers (should match baseline)\n",
    "SANITY_CONFIG = {\n",
    "    'type': 'dns-ga',\n",
    "    'name': 'DNS-GA_sanity_no_ga',\n",
    "    'g_n': 99999,  # Never triggers within 3000 iterations\n",
    "    'num_ga_children': 2,\n",
    "    'num_ga_generations': 2,\n",
    "}\n",
    "\n",
    "# Sanity check runs with just seed 42\n",
    "SANITY_SEED = 42\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFixed Parameters:\")\n",
    "print(f\"  Environment: {FIXED_PARAMS['env_name']}\")\n",
    "print(f\"  Iterations: {FIXED_PARAMS['num_iterations']}\")\n",
    "print(f\"  Batch size: {FIXED_PARAMS['batch_size']}\")\n",
    "print(f\"  ISO_SIGMA: {FIXED_PARAMS['iso_sigma']}\")\n",
    "print(f\"  Population: {FIXED_PARAMS['population_size']}\")\n",
    "\n",
    "print(f\"\\nMain Configurations (31 seeds each):\")\n",
    "for config in MAIN_CONFIGS:\n",
    "    if config['type'] == 'baseline':\n",
    "        print(f\"  â€¢ {config['name']}: No GA\")\n",
    "    else:\n",
    "        ga_calls = FIXED_PARAMS['num_iterations'] // config['g_n']\n",
    "        print(f\"  â€¢ {config['name']}: {ga_calls} GA calls (every {config['g_n']} iters, {config['num_ga_generations']} gens)\")\n",
    "\n",
    "print(f\"\\nSanity Check Configuration (seed {SANITY_SEED} only):\")\n",
    "print(f\"  â€¢ {SANITY_CONFIG['name']}: g_n={SANITY_CONFIG['g_n']} (no GA triggers)\")\n",
    "print(f\"    Purpose: Verify DNS-GA without GA = baseline\")\n",
    "\n",
    "print(f\"\\nTotal Experiments:\")\n",
    "main_exp = len(MAIN_CONFIGS) * len(RANDOM_SEEDS)\n",
    "sanity_exp = 1\n",
    "total_exp = main_exp + sanity_exp\n",
    "print(f\"  Main: {len(MAIN_CONFIGS)} configs Ã— {len(RANDOM_SEEDS)} seeds = {main_exp}\")\n",
    "print(f\"  Sanity: {sanity_exp}\")\n",
    "print(f\"  Total: {total_exp}\")\n",
    "print(f\"  Estimated time (2 parallel): ~{(total_exp / 2) * 3.3 / 60:.1f} hours\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5390bd",
   "metadata": {},
   "source": [
    "## STEP 2: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67d76f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded!\n"
     ]
    }
   ],
   "source": [
    "def calculate_ga_overhead_evals(g_n, num_iterations, population_size, num_ga_children, num_ga_generations):\n",
    "    \"\"\"Calculate total evaluations performed by Competition-GA.\"\"\"\n",
    "    if g_n is None or g_n >= num_iterations:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    num_ga_calls = num_iterations // g_n\n",
    "    if num_ga_children == 1:\n",
    "        offspring_per_call = population_size * num_ga_generations\n",
    "    else:\n",
    "        offspring_per_call = population_size * num_ga_children * (num_ga_children**num_ga_generations - 1) // (num_ga_children - 1)\n",
    "    evals_per_ga_call = offspring_per_call\n",
    "    total_ga_evals = num_ga_calls * evals_per_ga_call\n",
    "    return total_ga_evals, num_ga_calls, evals_per_ga_call\n",
    "\n",
    "\n",
    "def setup_environment(env_name, episode_length, policy_hidden_layer_sizes, batch_size, seed):\n",
    "    \"\"\"Initialize environment and policy network.\"\"\"\n",
    "    env = environments.create(env_name, episode_length=episode_length)\n",
    "    reset_fn = jax.jit(env.reset)\n",
    "    key = jax.random.key(seed)\n",
    "    \n",
    "    policy_layer_sizes = policy_hidden_layer_sizes + (env.action_size,)\n",
    "    policy_network = MLP(\n",
    "        layer_sizes=policy_layer_sizes,\n",
    "        kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "        final_activation=jnp.tanh,\n",
    "    )\n",
    "    \n",
    "    key, subkey = jax.random.split(key)\n",
    "    keys = jax.random.split(subkey, num=batch_size)\n",
    "    fake_batch = jnp.zeros(shape=(batch_size, env.observation_size))\n",
    "    init_variables = jax.vmap(policy_network.init)(keys, fake_batch)\n",
    "    \n",
    "    return env, policy_network, reset_fn, init_variables, key\n",
    "\n",
    "\n",
    "def create_scoring_function(env, policy_network, reset_fn, episode_length, env_name):\n",
    "    \"\"\"Create scoring function for fitness evaluation.\"\"\"\n",
    "    def play_step_fn(env_state, policy_params, key):\n",
    "        actions = policy_network.apply(policy_params, env_state.obs)\n",
    "        state_desc = env_state.info[\"state_descriptor\"]\n",
    "        next_state = env.step(env_state, actions)\n",
    "        \n",
    "        transition = QDTransition(\n",
    "            obs=env_state.obs,\n",
    "            next_obs=next_state.obs,\n",
    "            rewards=next_state.reward,\n",
    "            dones=next_state.done,\n",
    "            actions=actions,\n",
    "            truncations=next_state.info[\"truncation\"],\n",
    "            state_desc=state_desc,\n",
    "            next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "        )\n",
    "        return next_state, policy_params, key, transition\n",
    "    \n",
    "    descriptor_extraction_fn = environments.descriptor_extractor[env_name]\n",
    "    scoring_fn = functools.partial(\n",
    "        scoring_function,\n",
    "        episode_length=episode_length,\n",
    "        play_reset_fn=reset_fn,\n",
    "        play_step_fn=play_step_fn,\n",
    "        descriptor_extractor=descriptor_extraction_fn,\n",
    "    )\n",
    "    \n",
    "    return scoring_fn\n",
    "\n",
    "\n",
    "def create_mutation_function(iso_sigma):\n",
    "    \"\"\"Create mutation function for Competition-GA.\"\"\"\n",
    "    def competition_ga_mutation_fn(genotype, key):\n",
    "        genotype_flat, tree_def = jax.tree_util.tree_flatten(genotype)\n",
    "        num_leaves = len(genotype_flat)\n",
    "        keys = jax.random.split(key, num_leaves)\n",
    "        keys_tree = jax.tree_util.tree_unflatten(tree_def, keys)\n",
    "        \n",
    "        def add_noise(x, k):\n",
    "            return x + jax.random.normal(k, shape=x.shape) * iso_sigma\n",
    "        \n",
    "        mutated = jax.tree_util.tree_map(add_noise, genotype, keys_tree)\n",
    "        return mutated\n",
    "    \n",
    "    return competition_ga_mutation_fn\n",
    "\n",
    "\n",
    "def calculate_cumulative_evals_for_log(config, log_df, batch_size, population_size):\n",
    "    \"\"\"Calculate cumulative evaluations at each logged iteration.\"\"\"\n",
    "    iterations = log_df['iteration'].values\n",
    "    evals = np.zeros(len(iterations))\n",
    "    \n",
    "    if config['type'] == 'baseline':\n",
    "        # Baseline: constant batch_size per iteration\n",
    "        evals = iterations * batch_size\n",
    "    else:\n",
    "        # DNS-GA: batch_size + periodic GA overhead\n",
    "        g_n = config['g_n']\n",
    "        num_ga_children = config['num_ga_children']\n",
    "        num_ga_generations = config['num_ga_generations']\n",
    "        \n",
    "        # Calculate GA overhead per call\n",
    "        if g_n >= len(iterations) * 100:  # Sanity check: g_n so large it never triggers\n",
    "            evals = iterations * batch_size\n",
    "        elif num_ga_children == 1:\n",
    "            ga_evals_per_call = population_size * num_ga_generations\n",
    "            for idx, iter_num in enumerate(iterations):\n",
    "                cumulative = iter_num * batch_size\n",
    "                num_ga_calls = iter_num // g_n\n",
    "                cumulative += num_ga_calls * ga_evals_per_call\n",
    "                evals[idx] = cumulative\n",
    "        else:\n",
    "            ga_evals_per_call = (population_size * num_ga_children * \n",
    "                                (num_ga_children**num_ga_generations - 1) // (num_ga_children - 1))\n",
    "            for idx, iter_num in enumerate(iterations):\n",
    "                cumulative = iter_num * batch_size\n",
    "                num_ga_calls = iter_num // g_n\n",
    "                cumulative += num_ga_calls * ga_evals_per_call\n",
    "                evals[idx] = cumulative\n",
    "    \n",
    "    return evals\n",
    "\n",
    "\n",
    "def interpolate_evals_to_milestone(qd_scores, evals, milestone):\n",
    "    \"\"\"Interpolate evaluations needed to reach a QD score milestone.\"\"\"\n",
    "    idx = np.searchsorted(qd_scores, milestone)\n",
    "    \n",
    "    if idx == 0:\n",
    "        return evals[0]\n",
    "    elif idx >= len(qd_scores):\n",
    "        return None  # Milestone not reached\n",
    "    else:\n",
    "        # Linear interpolation between two points\n",
    "        qd_low, qd_high = qd_scores[idx-1], qd_scores[idx]\n",
    "        eval_low, eval_high = evals[idx-1], evals[idx]\n",
    "        ratio = (milestone - qd_low) / (qd_high - qd_low)\n",
    "        return eval_low + ratio * (eval_high - eval_low)\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77ea26",
   "metadata": {},
   "source": [
    "## STEP 3: Parallel Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f8e8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel experiment runner ready!\n"
     ]
    }
   ],
   "source": [
    "def run_single_experiment(config, seed, fixed_params):\n",
    "    \"\"\"Run a single experiment with given config and seed.\"\"\"\n",
    "    exp_name = f\"{config['name']}_seed{seed}\"\n",
    "    \n",
    "    # Setup environment\n",
    "    env, policy_network, reset_fn, init_variables, key = setup_environment(\n",
    "        fixed_params['env_name'],\n",
    "        fixed_params['episode_length'],\n",
    "        fixed_params['policy_hidden_layer_sizes'],\n",
    "        fixed_params['batch_size'],\n",
    "        seed\n",
    "    )\n",
    "    \n",
    "    scoring_fn = create_scoring_function(env, policy_network, reset_fn, \n",
    "                                        fixed_params['episode_length'],\n",
    "                                        fixed_params['env_name'])\n",
    "    \n",
    "    reward_offset = environments.reward_offset[fixed_params['env_name']]\n",
    "    metrics_function = functools.partial(\n",
    "        default_qd_metrics,\n",
    "        qd_offset=reward_offset * fixed_params['episode_length'],\n",
    "    )\n",
    "    \n",
    "    # Create emitter\n",
    "    variation_fn = functools.partial(\n",
    "        isoline_variation,\n",
    "        iso_sigma=fixed_params['iso_sigma'],\n",
    "        line_sigma=fixed_params['line_sigma']\n",
    "    )\n",
    "    \n",
    "    mixing_emitter = MixingEmitter(\n",
    "        mutation_fn=None,\n",
    "        variation_fn=variation_fn,\n",
    "        variation_percentage=1.0,\n",
    "        batch_size=fixed_params['batch_size']\n",
    "    )\n",
    "    \n",
    "    # Create algorithm (DNS or DNS-GA)\n",
    "    if config['type'] == 'baseline':\n",
    "        algorithm = DominatedNoveltySearch(\n",
    "            scoring_function=scoring_fn,\n",
    "            emitter=mixing_emitter,\n",
    "            metrics_function=metrics_function,\n",
    "            population_size=fixed_params['population_size'],\n",
    "            k=fixed_params['k'],\n",
    "        )\n",
    "    else:\n",
    "        mutation_fn = create_mutation_function(fixed_params['iso_sigma'])\n",
    "        algorithm = DominatedNoveltySearchGA(\n",
    "            scoring_function=scoring_fn,\n",
    "            emitter=mixing_emitter,\n",
    "            metrics_function=metrics_function,\n",
    "            population_size=fixed_params['population_size'],\n",
    "            k=fixed_params['k'],\n",
    "            g_n=config['g_n'],\n",
    "            num_ga_children=config['num_ga_children'],\n",
    "            num_ga_generations=config['num_ga_generations'],\n",
    "            mutation_fn=mutation_fn,\n",
    "        )\n",
    "    \n",
    "    # Initialize\n",
    "    key, subkey = jax.random.split(key)\n",
    "    repertoire, emitter_state, init_metrics = algorithm.init(init_variables, subkey)\n",
    "    \n",
    "    # Setup logging\n",
    "    log_period = 100\n",
    "    num_loops = fixed_params['num_iterations'] // log_period\n",
    "    \n",
    "    metrics = {key: jnp.array([]) for key in [\"iteration\", \"qd_score\", \"coverage\", \"max_fitness\", \"time\"]}\n",
    "    init_metrics = jax.tree.map(lambda x: jnp.array([x]) if x.shape == () else x, init_metrics)\n",
    "    init_metrics[\"iteration\"] = jnp.array([0], dtype=jnp.int32)\n",
    "    init_metrics[\"time\"] = jnp.array([0.0])\n",
    "    metrics = jax.tree.map(\n",
    "        lambda metric, init_metric: jnp.concatenate([metric, init_metric], axis=0),\n",
    "        metrics, init_metrics\n",
    "    )\n",
    "    \n",
    "    log_filename = os.path.join(\"seed_variability_logs\", f\"{exp_name}_logs.csv\")\n",
    "    csv_logger = CSVLogger(log_filename, header=list(metrics.keys()))\n",
    "    csv_logger.log(jax.tree.map(lambda x: x[-1], metrics))\n",
    "    \n",
    "    # Main training loop\n",
    "    if config['type'] == 'baseline':\n",
    "        algorithm_scan_update = algorithm.scan_update\n",
    "        scan_state = (repertoire, emitter_state, key)\n",
    "    else:\n",
    "        algorithm_scan_update = algorithm.scan_update\n",
    "        scan_state = (repertoire, emitter_state, key, 1)  # generation_counter\n",
    "    \n",
    "    start_time_total = time.time()\n",
    "    \n",
    "    for i in range(num_loops):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scan_state, current_metrics = jax.lax.scan(\n",
    "            algorithm_scan_update,\n",
    "            scan_state,\n",
    "            (),\n",
    "            length=log_period,\n",
    "        )\n",
    "        \n",
    "        timelapse = time.time() - start_time\n",
    "        \n",
    "        current_metrics[\"iteration\"] = jnp.arange(\n",
    "            1 + log_period * i, 1 + log_period * (i + 1), dtype=jnp.int32\n",
    "        )\n",
    "        current_metrics[\"time\"] = jnp.repeat(timelapse, log_period)\n",
    "        metrics = jax.tree.map(\n",
    "            lambda metric, current_metric: jnp.concatenate([metric, current_metric], axis=0),\n",
    "            metrics, current_metrics\n",
    "        )\n",
    "        \n",
    "        csv_logger.log(jax.tree.map(lambda x: x[-1], metrics))\n",
    "    \n",
    "    total_time = time.time() - start_time_total\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ga_total_evals, ga_num_calls, ga_evals_per_call = calculate_ga_overhead_evals(\n",
    "        config.get('g_n'), fixed_params['num_iterations'], fixed_params['population_size'],\n",
    "        config.get('num_ga_children'), config.get('num_ga_generations')\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'config_name': config['name'],\n",
    "        'config_type': config['type'],\n",
    "        'seed': seed,\n",
    "        'g_n': config.get('g_n'),\n",
    "        'num_ga_generations': config.get('num_ga_generations'),\n",
    "        'final_qd_score': float(metrics['qd_score'][-1]),\n",
    "        'final_max_fitness': float(metrics['max_fitness'][-1]),\n",
    "        'final_coverage': float(metrics['coverage'][-1]),\n",
    "        'total_time': total_time,\n",
    "        'ga_overhead_evals': ga_total_evals,\n",
    "        'log_file': log_filename,\n",
    "    }\n",
    "\n",
    "\n",
    "def worker_process(task_queue, result_queue, fixed_params):\n",
    "    \"\"\"Worker process that runs experiments from the queue.\"\"\"\n",
    "    while True:\n",
    "        task = task_queue.get()\n",
    "        if task is None:  # Poison pill to stop worker\n",
    "            break\n",
    "        \n",
    "        exp_num, total_exp, config, seed = task\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n[Process {os.getpid()}] Starting experiment {exp_num}/{total_exp}: {config['name']}, seed={seed}\")\n",
    "            result = run_single_experiment(config, seed, fixed_params)\n",
    "            result['exp_num'] = exp_num\n",
    "            result_queue.put(('success', result))\n",
    "            print(f\"[Process {os.getpid()}] Completed: {config['name']}, seed={seed}, QD={result['final_qd_score']:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Process {os.getpid()}] ERROR: {config['name']}, seed={seed}: {e}\")\n",
    "            result_queue.put(('error', {'config_name': config['name'], 'seed': seed, 'error': str(e)}))\n",
    "\n",
    "\n",
    "def run_experiments_parallel(experiment_queue, fixed_params, num_workers=2):\n",
    "    \"\"\"Run experiments in parallel with specified number of workers.\"\"\"\n",
    "    manager = Manager()\n",
    "    task_queue = manager.Queue()\n",
    "    result_queue = manager.Queue()\n",
    "    \n",
    "    # Fill task queue\n",
    "    for task in experiment_queue:\n",
    "        task_queue.put(task)\n",
    "    \n",
    "    # Add poison pills to stop workers\n",
    "    for _ in range(num_workers):\n",
    "        task_queue.put(None)\n",
    "    \n",
    "    # Start worker processes\n",
    "    workers = []\n",
    "    for _ in range(num_workers):\n",
    "        p = Process(target=worker_process, args=(task_queue, result_queue, fixed_params))\n",
    "        p.start()\n",
    "        workers.append(p)\n",
    "    \n",
    "    # Collect results\n",
    "    all_results = []\n",
    "    errors = []\n",
    "    total_experiments = len(experiment_queue)\n",
    "    \n",
    "    for _ in range(total_experiments):\n",
    "        status, result = result_queue.get()\n",
    "        if status == 'success':\n",
    "            all_results.append(result)\n",
    "        else:\n",
    "            errors.append(result)\n",
    "    \n",
    "    # Wait for all workers to finish\n",
    "    for p in workers:\n",
    "        p.join()\n",
    "    \n",
    "    return all_results, errors\n",
    "\n",
    "print(\"Parallel experiment runner ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f11a8",
   "metadata": {},
   "source": [
    "## STEP 4: Build Experiment Queue and Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f08cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BUILDING EXPERIMENT QUEUE - 20251115_181236\n",
      "================================================================================\n",
      "\n",
      "Experiment Queue Summary:\n",
      "  SANITY CHECK FIRST: 2 experiments (DNS-GA g_n=99999 + baseline, both seed 42)\n",
      "  Main experiments: 3 configs Ã— 31 seeds = 93\n",
      "  Total: 95\n",
      "  Parallelization: 2 simultaneous experiments\n",
      "  Estimated time: ~2.6 hours\n",
      "================================================================================\n",
      "\n",
      "ðŸ” FIRST 2 EXPERIMENTS (SANITY CHECK):\n",
      "  1. DNS-GA_sanity_no_ga, seed=42\n",
      "  2. DNS_baseline, seed=42\n",
      "  â†’ These should have IDENTICAL final QD scores\n",
      "  â†’ If they differ by >0.5%, stop the run and investigate\n",
      "\n",
      "Next 5 experiments:\n",
      "  3. DNS_baseline, seed=7817\n",
      "  4. DNS_baseline, seed=52731\n",
      "  5. DNS_baseline, seed=51809\n",
      "  6. DNS_baseline, seed=35457\n",
      "  7. DNS_baseline, seed=47644\n",
      "\n",
      "Last 3 experiments:\n",
      "  93. DNS-GA_g1000_gen4, seed=53095\n",
      "  94. DNS-GA_g1000_gen4, seed=51931\n",
      "  95. DNS-GA_g1000_gen4, seed=35573\n",
      "\n",
      "================================================================================\n",
      "Ready to run experiments!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Build experiment queue\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"BUILDING EXPERIMENT QUEUE - {timestamp}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "experiment_queue = []\n",
    "exp_num = 0\n",
    "\n",
    "# SANITY CHECK: Run baseline seed 42 FIRST to establish reference\n",
    "baseline_config = [c for c in MAIN_CONFIGS if c['type'] == 'baseline'][0]\n",
    "exp_num += 1\n",
    "experiment_queue.append((exp_num, exp_num, baseline_config, SANITY_SEED))\n",
    "\n",
    "# Then run DNS-GA with g_n=99999 (should be slightly different but comparable)\n",
    "exp_num += 1\n",
    "experiment_queue.append((exp_num, exp_num, SANITY_CONFIG, SANITY_SEED))\n",
    "\n",
    "# Add main experiments (all configs Ã— all seeds)\n",
    "for config in MAIN_CONFIGS:\n",
    "    for seed in RANDOM_SEEDS:\n",
    "        exp_num += 1\n",
    "        experiment_queue.append((exp_num, exp_num, config, seed))\n",
    "\n",
    "total_experiments = len(experiment_queue)\n",
    "\n",
    "print(f\"\\nExperiment Queue Summary:\")\n",
    "print(f\"  SANITY CHECK FIRST: 2 experiments (DNS-GA g_n=99999 + baseline, both seed 42)\")\n",
    "print(f\"  Main experiments: {len(MAIN_CONFIGS)} configs Ã— {len(RANDOM_SEEDS)} seeds = {len(MAIN_CONFIGS) * len(RANDOM_SEEDS)}\")\n",
    "print(f\"  Total: {total_experiments}\")\n",
    "print(f\"  Execution: Sequential (Jupyter can't use multiprocessing)\")\n",
    "print(f\"  Estimated time: ~{total_experiments * 3.3 / 60:.1f} hours (~5.2 hours)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ” FIRST 2 EXPERIMENTS (SANITY CHECK):\")\n",
    "for i in range(min(2, len(experiment_queue))):\n",
    "    exp_num, _, config, seed = experiment_queue[i]\n",
    "    print(f\"  {exp_num}. {config['name']}, seed={seed}\")\n",
    "print(f\"  â†’ These should have IDENTICAL final QD scores\")\n",
    "print(f\"  â†’ If they differ by >0.5%, stop the run and investigate\")\n",
    "\n",
    "print(f\"\\nNext 5 experiments:\")\n",
    "for i in range(2, min(7, len(experiment_queue))):\n",
    "    exp_num, _, config, seed = experiment_queue[i]\n",
    "    print(f\"  {exp_num}. {config['name']}, seed={seed}\")\n",
    "\n",
    "print(\"\\nLast 3 experiments:\")\n",
    "for i in range(max(0, len(experiment_queue) - 3), len(experiment_queue)):\n",
    "    exp_num, _, config, seed = experiment_queue[i]\n",
    "    print(f\"  {exp_num}. {config['name']}, seed={seed}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Ready to run experiments!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db024292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING PARALLEL EXPERIMENT EXECUTION\n",
      "================================================================================\n",
      "Start time: 2025-11-15 18:12:41\n",
      "Running 2 experiments simultaneously...\n",
      "\n",
      "âš ï¸  SANITY CHECK RUNS FIRST (experiments 1-2)\n",
      "   Check results before continuing to main experiments!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'worker_process' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'worker_process' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m start_time_all = time.time()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Run experiments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m all_results, errors = \u001b[43mrun_experiments_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFIXED_PARAMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m total_time = time.time() - start_time_all\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 185\u001b[39m, in \u001b[36mrun_experiments_parallel\u001b[39m\u001b[34m(experiment_queue, fixed_params, num_workers)\u001b[39m\n\u001b[32m    182\u001b[39m total_experiments = \u001b[38;5;28mlen\u001b[39m(experiment_queue)\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_experiments):\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     status, result = \u001b[43mresult_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status == \u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    187\u001b[39m         all_results.append(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:2\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/managers.py:828\u001b[39m, in \u001b[36mBaseProxy._callmethod\u001b[39m\u001b[34m(self, methodname, args, kwds)\u001b[39m\n\u001b[32m    825\u001b[39m     conn = \u001b[38;5;28mself\u001b[39m._tls.connection\n\u001b[32m    827\u001b[39m conn.send((\u001b[38;5;28mself\u001b[39m._id, methodname, args, kwds))\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m kind, result = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kind == \u001b[33m'\u001b[39m\u001b[33m#RETURN\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    831\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/connection.py:250\u001b[39m, in \u001b[36m_ConnectionBase.recv\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler.loads(buf.getbuffer())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/connection.py:430\u001b[39m, in \u001b[36mConnection._recv_bytes\u001b[39m\u001b[34m(self, maxsize)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     size, = struct.unpack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, buf.getvalue())\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m size == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/connection.py:395\u001b[39m, in \u001b[36mConnection._recv\u001b[39m\u001b[34m(self, size, read)\u001b[39m\n\u001b[32m    393\u001b[39m remaining = size\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     chunk = \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m     n = \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# STEP 4A: Run Sanity Check First (Sequential, no multiprocessing issues)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4A: RUNNING SANITY CHECK FIRST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"Running experiments 1-2 sequentially to verify implementation...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time_all = time.time()\n",
    "all_results = []\n",
    "errors = []\n",
    "\n",
    "# Run first 2 experiments (sanity check) sequentially\n",
    "for i in range(min(2, len(experiment_queue))):\n",
    "    exp_num, total_exp, config, seed = experiment_queue[i]\n",
    "    \n",
    "    print(f\"\\nRunning experiment {exp_num}/{total_exp}: {config['name']}, seed={seed}\")\n",
    "    \n",
    "    try:\n",
    "        result = run_single_experiment(config, seed, FIXED_PARAMS)\n",
    "        result['exp_num'] = exp_num\n",
    "        all_results.append(result)\n",
    "        print(f\"âœ“ Completed: QD={result['final_qd_score']:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— ERROR: {e}\")\n",
    "        errors.append({'config_name': config['name'], 'seed': seed, 'error': str(e)})\n",
    "\n",
    "sanity_time = time.time() - start_time_all\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SANITY CHECK COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Time: {sanity_time / 60:.1f} minutes\")\n",
    "print(f\"Completed: {len(all_results)}/2\")\n",
    "\n",
    "# Display sanity check results immediately\n",
    "if len(all_results) == 2:\n",
    "    baseline_qd = all_results[0]['final_qd_score']\n",
    "    sanity_qd = all_results[1]['final_qd_score']\n",
    "    diff = abs(sanity_qd - baseline_qd)\n",
    "    pct_diff = (diff / baseline_qd) * 100\n",
    "    \n",
    "    print(f\"\\nðŸ” SANITY CHECK RESULTS:\")\n",
    "    print(f\"  Baseline (DNS):     {baseline_qd:.2f}\")\n",
    "    print(f\"  DNS-GA (g_n=99999): {sanity_qd:.2f}\")\n",
    "    print(f\"  Difference:         {diff:.2f} ({pct_diff:.3f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ SANITY CHECK LOGIC:\")\n",
    "    print(f\"   DNS-GA now uses: (gen > 0) AND (gen % g_n == 0)\")\n",
    "    print(f\"   With g_n=99999 and 3000 iterations, GA never triggers\")\n",
    "    print(f\"   Both configs use DominatedNoveltyRepertoire.add() logic\")\n",
    "    print(f\"   Only difference: DNS uses DNS class, DNS-GA uses DNS-GA class\")\n",
    "    \n",
    "    if pct_diff < 0.5:\n",
    "        print(f\"\\nâœ… SANITY CHECK PASSED!\")\n",
    "        print(f\"   Difference < 0.5% confirms identical behavior when GA disabled\")\n",
    "        print(f\"   Implementation is correct - proceeding with main experiments\")\n",
    "        proceed = True\n",
    "    elif pct_diff < 2.0:\n",
    "        print(f\"\\nâš ï¸  SANITY CHECK: Small difference ({pct_diff:.3f}%)\")\n",
    "        print(f\"   Likely due to JAX random key differences or numerical precision\")\n",
    "        print(f\"   Acceptable for stochastic algorithm - proceeding\")\n",
    "        proceed = True\n",
    "    else:\n",
    "        print(f\"\\nâŒ SANITY CHECK FAILED!\")\n",
    "        print(f\"   Difference = {pct_diff:.3f}% (expected < 2%)\")\n",
    "        print(f\"\\nâš ï¸  STOPPING: DNS-GA without GA should match baseline DNS\")\n",
    "        print(f\"   Investigate implementation bug before continuing\")\n",
    "        proceed = False\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Could not complete sanity check\")\n",
    "    proceed = False\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# STEP 4B: Run Main Experiments (Sequential to avoid multiprocessing issues)\n",
    "if proceed:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4B: RUNNING MAIN EXPERIMENTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Remaining: {len(experiment_queue) - 2} experiments\")\n",
    "    print(f\"Running sequentially (Jupyter notebooks can't pickle multiprocessing)\")\n",
    "    print(f\"Estimated time: ~{(len(experiment_queue) - 2) * 3.3 / 60:.1f} hours\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Run remaining experiments sequentially\n",
    "    for i in range(2, len(experiment_queue)):\n",
    "        exp_num, total_exp, config, seed = experiment_queue[i]\n",
    "        \n",
    "        print(f\"\\nRunning experiment {exp_num}/{total_exp}: {config['name']}, seed={seed}\")\n",
    "        \n",
    "        try:\n",
    "            result = run_single_experiment(config, seed, FIXED_PARAMS)\n",
    "            result['exp_num'] = exp_num\n",
    "            all_results.append(result)\n",
    "            print(f\"âœ“ Completed: QD={result['final_qd_score']:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— ERROR: {e}\")\n",
    "            errors.append({'config_name': config['name'], 'seed': seed, 'error': str(e)})\n",
    "        \n",
    "        # Progress update every 10 experiments\n",
    "        if (i - 1) % 10 == 0:\n",
    "            elapsed = time.time() - start_time_all\n",
    "            completed = len(all_results)\n",
    "            remaining = len(experiment_queue) - completed\n",
    "            avg_time = elapsed / completed if completed > 0 else 3.3 * 60\n",
    "            est_remaining = (remaining * avg_time) / 60\n",
    "            print(f\"\\nðŸ“Š Progress: {completed}/{len(experiment_queue)} ({completed/len(experiment_queue)*100:.1f}%)\")\n",
    "            print(f\"   Elapsed: {elapsed/3600:.1f}h, Remaining: ~{est_remaining/60:.1f}h\")\n",
    "\n",
    "    total_time = time.time() - start_time_all\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ALL EXPERIMENTS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Total time: {total_time / 60:.1f} minutes ({total_time / 3600:.2f} hours)\")\n",
    "    print(f\"Successful experiments: {len(all_results)}\")\n",
    "    print(f\"Failed experiments: {len(errors)}\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENTS STOPPED AFTER SANITY CHECK FAILURE\")\n",
    "    print(\"=\"*80)\n",
    "    total_time = sanity_time\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nErrors encountered:\")\n",
    "    for error in errors:\n",
    "        print(f\"  â€¢ {error['config_name']}, seed={error['seed']}: {error['error']}\")\n",
    "\n",
    "# Save all results\n",
    "results_file = f\"seed_variability_logs/all_results_{timestamp}.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'results': all_results,\n",
    "        'errors': errors,\n",
    "        'total_time': total_time,\n",
    "        'timestamp': timestamp,\n",
    "        'num_seeds': len(RANDOM_SEEDS),\n",
    "        'seeds': RANDOM_SEEDS,\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_file}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7ddee",
   "metadata": {},
   "source": [
    "## STEP 5: Convergence Efficiency Analysis\n",
    "\n",
    "**Goal**: Calculate how many evaluations each DNS-GA config needs to reach baseline's convergence score\n",
    "\n",
    "**Key Metric**: Evaluations-to-convergence (not performance at same iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e5815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results into DataFrame\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal experiments: {len(df)}\")\n",
    "print(f\"Configurations: {df['config_name'].unique()}\")\n",
    "print(f\"Seeds per config: {df.groupby('config_name')['seed'].count().to_dict()}\")\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.groupby('config_name')['final_qd_score'].agg(['mean', 'std', 'min', 'max']).round(2))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6041f",
   "metadata": {},
   "source": [
    "### Sanity Check: Verify g_n=99999 Matches Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SANITY CHECK: DNS-GA with g_n=99999 should match baseline\")\n",
    "print(\"=\"*80)\n",
    "print(\"This validates that DNS-GA without GA triggers = baseline DNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sanity_result = df[df['config_name'] == 'DNS-GA_sanity_no_ga']\n",
    "baseline_seed42 = df[(df['config_name'] == 'DNS_baseline') & (df['seed'] == 42)]\n",
    "\n",
    "if len(sanity_result) > 0 and len(baseline_seed42) > 0:\n",
    "    sanity_qd = sanity_result['final_qd_score'].values[0]\n",
    "    baseline_qd = baseline_seed42['final_qd_score'].values[0]\n",
    "    diff = abs(sanity_qd - baseline_qd)\n",
    "    pct_diff = (diff / baseline_qd) * 100\n",
    "    \n",
    "    print(f\"\\nSeed 42 comparison:\")\n",
    "    print(f\"  Baseline (DNS):        {baseline_qd:.2f}\")\n",
    "    print(f\"  DNS-GA (g_n=99999):    {sanity_qd:.2f}\")\n",
    "    print(f\"  Absolute difference:   {diff:.2f}\")\n",
    "    print(f\"  Percentage difference: {pct_diff:.3f}%\")\n",
    "    \n",
    "    if pct_diff < 0.5:\n",
    "        print(f\"\\nâœ… SANITY CHECK PASSED: Difference < 0.5%\")\n",
    "        print(f\"   DNS-GA without GA calls behaves identically to baseline\")\n",
    "        print(f\"   Implementation is correct - safe to trust main results\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ SANITY CHECK FAILED: Difference = {pct_diff:.3f}%\")\n",
    "        print(f\"   Expected near-identical performance\")\n",
    "        print(f\"   âš ï¸  WARNING: Implementation may have bugs!\")\n",
    "        print(f\"   Investigate before trusting main experiment results\")\n",
    "else:\n",
    "    print(\"\\nâœ— SANITY CHECK DATA MISSING\")\n",
    "    print(\"  Cannot validate implementation correctness\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f07a8",
   "metadata": {},
   "source": [
    "### Calculate Convergence Efficiency for Each Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91518b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CONVERGENCE EFFICIENCY CALCULATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFor each seed, calculate:\")\n",
    "print(\"  1. Baseline's final convergence score (QD @ 3000 iters)\")\n",
    "print(\"  2. Evaluations needed for DNS-GA to reach that score\")\n",
    "print(\"  3. Evaluation savings = Baseline evals - DNS-GA evals\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store convergence efficiency results\n",
    "convergence_results = []\n",
    "\n",
    "# Get unique configs (excluding sanity check)\n",
    "dns_ga_configs = [c for c in MAIN_CONFIGS if c['type'] == 'dns-ga']\n",
    "\n",
    "for seed in RANDOM_SEEDS:\n",
    "    # Get baseline convergence score for this seed\n",
    "    baseline_row = df[(df['config_name'] == 'DNS_baseline') & (df['seed'] == seed)]\n",
    "    \n",
    "    if len(baseline_row) == 0:\n",
    "        print(f\"Warning: No baseline data for seed {seed}\")\n",
    "        continue\n",
    "    \n",
    "    baseline_convergence_qd = baseline_row['final_qd_score'].values[0]\n",
    "    baseline_convergence_evals = FIXED_PARAMS['num_iterations'] * FIXED_PARAMS['batch_size']\n",
    "    baseline_log_file = baseline_row['log_file'].values[0]\n",
    "    \n",
    "    # Load baseline trajectory\n",
    "    if not os.path.exists(baseline_log_file):\n",
    "        print(f\"Warning: Log file missing for baseline seed {seed}\")\n",
    "        continue\n",
    "    \n",
    "    baseline_log_df = pd.read_csv(baseline_log_file)\n",
    "    \n",
    "    # For each DNS-GA config, find when it reaches baseline's convergence\n",
    "    for config in dns_ga_configs:\n",
    "        ga_row = df[(df['config_name'] == config['name']) & (df['seed'] == seed)]\n",
    "        \n",
    "        if len(ga_row) == 0:\n",
    "            continue\n",
    "        \n",
    "        ga_log_file = ga_row['log_file'].values[0]\n",
    "        \n",
    "        if not os.path.exists(ga_log_file):\n",
    "            continue\n",
    "        \n",
    "        ga_log_df = pd.read_csv(ga_log_file)\n",
    "        \n",
    "        # Calculate cumulative evaluations\n",
    "        ga_evals = calculate_cumulative_evals_for_log(config, ga_log_df, \n",
    "                                                      FIXED_PARAMS['batch_size'],\n",
    "                                                      FIXED_PARAMS['population_size'])\n",
    "        \n",
    "        # Find when DNS-GA reaches baseline's convergence score\n",
    "        evals_to_convergence = interpolate_evals_to_milestone(\n",
    "            ga_log_df['qd_score'].values,\n",
    "            ga_evals,\n",
    "            baseline_convergence_qd\n",
    "        )\n",
    "        \n",
    "        if evals_to_convergence is not None:\n",
    "            eval_savings = baseline_convergence_evals - evals_to_convergence\n",
    "            pct_savings = (eval_savings / baseline_convergence_evals) * 100\n",
    "            \n",
    "            convergence_results.append({\n",
    "                'seed': seed,\n",
    "                'config_name': config['name'],\n",
    "                'baseline_convergence_qd': baseline_convergence_qd,\n",
    "                'baseline_convergence_evals': baseline_convergence_evals,\n",
    "                'ga_evals_to_convergence': evals_to_convergence,\n",
    "                'eval_savings': eval_savings,\n",
    "                'pct_savings': pct_savings,\n",
    "                'converged': True,\n",
    "            })\n",
    "        else:\n",
    "            # DNS-GA didn't reach baseline's convergence score\n",
    "            convergence_results.append({\n",
    "                'seed': seed,\n",
    "                'config_name': config['name'],\n",
    "                'baseline_convergence_qd': baseline_convergence_qd,\n",
    "                'baseline_convergence_evals': baseline_convergence_evals,\n",
    "                'ga_evals_to_convergence': None,\n",
    "                'eval_savings': None,\n",
    "                'pct_savings': None,\n",
    "                'converged': False,\n",
    "            })\n",
    "\n",
    "convergence_df = pd.DataFrame(convergence_results)\n",
    "\n",
    "print(f\"\\nProcessed {len(convergence_results)} seed-config pairs\")\n",
    "print(f\"Converged: {convergence_df['converged'].sum()}\")\n",
    "print(f\"Did not converge: {(~convergence_df['converged']).sum()}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e3fee",
   "metadata": {},
   "source": [
    "### Statistical Summary: Convergence Efficiency Across All Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONVERGENCE EFFICIENCY STATISTICS (31 SEEDS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  {'='*60}\")\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        print(f\"  âœ— No seeds reached baseline convergence\")\n",
    "        continue\n",
    "    \n",
    "    # Success rate\n",
    "    success_rate = (len(converged_data) / len(config_data)) * 100\n",
    "    print(f\"  Success rate: {len(converged_data)}/{len(config_data)} seeds ({success_rate:.1f}%)\")\n",
    "    \n",
    "    # Evaluation savings statistics\n",
    "    eval_savings = converged_data['eval_savings'].values\n",
    "    pct_savings = converged_data['pct_savings'].values\n",
    "    \n",
    "    print(f\"\\n  Evaluation Savings:\")\n",
    "    print(f\"    Mean:   {np.mean(eval_savings):>10,.0f} evals ({np.mean(pct_savings):>6.2f}%)\")\n",
    "    print(f\"    Median: {np.median(eval_savings):>10,.0f} evals ({np.median(pct_savings):>6.2f}%)\")\n",
    "    print(f\"    Std:    {np.std(eval_savings):>10,.0f} evals ({np.std(pct_savings):>6.2f}%)\")\n",
    "    print(f\"    Min:    {np.min(eval_savings):>10,.0f} evals ({np.min(pct_savings):>6.2f}%)\")\n",
    "    print(f\"    Max:    {np.max(eval_savings):>10,.0f} evals ({np.max(pct_savings):>6.2f}%)\")\n",
    "    \n",
    "    # How many show positive savings?\n",
    "    positive_savings = np.sum(eval_savings > 0)\n",
    "    print(f\"\\n  Seeds with positive savings: {positive_savings}/{len(converged_data)} ({positive_savings/len(converged_data)*100:.1f}%)\")\n",
    "    \n",
    "    # Baseline comparison\n",
    "    baseline_evals = converged_data['baseline_convergence_evals'].values[0]\n",
    "    mean_ga_evals = np.mean(converged_data['ga_evals_to_convergence'].values)\n",
    "    print(f\"\\n  Mean evaluations to convergence:\")\n",
    "    print(f\"    Baseline:       {baseline_evals:>10,.0f} evals (always)\")\n",
    "    print(f\"    DNS-GA (mean):  {mean_ga_evals:>10,.0f} evals\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6397eca",
   "metadata": {},
   "source": [
    "### Key Question: Is Seed 42 an Outlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SEED 42 ANALYSIS: Outlier or Representative?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        print(f\"  No convergence data available\")\n",
    "        continue\n",
    "    \n",
    "    # Find seed 42's performance\n",
    "    seed42_data = converged_data[converged_data['seed'] == 42]\n",
    "    \n",
    "    if len(seed42_data) == 0:\n",
    "        print(f\"  Seed 42 did not converge for this config\")\n",
    "        continue\n",
    "    \n",
    "    seed42_savings_pct = seed42_data['pct_savings'].values[0]\n",
    "    \n",
    "    # Calculate percentile rank\n",
    "    all_savings = converged_data['pct_savings'].values\n",
    "    percentile = stats.percentileofscore(all_savings, seed42_savings_pct)\n",
    "    \n",
    "    # Is it an outlier? (>2 std from mean)\n",
    "    mean_savings = np.mean(all_savings)\n",
    "    std_savings = np.std(all_savings)\n",
    "    z_score = (seed42_savings_pct - mean_savings) / std_savings if std_savings > 0 else 0\n",
    "    \n",
    "    print(f\"  Seed 42 savings: {seed42_savings_pct:.2f}%\")\n",
    "    print(f\"  Mean savings:    {mean_savings:.2f}% Â± {std_savings:.2f}%\")\n",
    "    print(f\"  Percentile rank: {percentile:.1f}th\")\n",
    "    print(f\"  Z-score:         {z_score:.2f}\")\n",
    "    \n",
    "    if abs(z_score) > 2:\n",
    "        print(f\"  âš  OUTLIER: |Z-score| > 2 (unusual performance)\")\n",
    "    elif percentile > 75:\n",
    "        print(f\"  âœ“ ABOVE AVERAGE: Top {100-percentile:.0f}% performer\")\n",
    "    elif percentile < 25:\n",
    "        print(f\"  âœ— BELOW AVERAGE: Bottom {percentile:.0f}%\")\n",
    "    else:\n",
    "        print(f\"  ~ TYPICAL: Middle 50% of distribution\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5187f01",
   "metadata": {},
   "source": [
    "### Save Convergence Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1103804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save convergence analysis\n",
    "convergence_file = f\"seed_variability_logs/convergence_analysis_{timestamp}.csv\"\n",
    "convergence_df.to_csv(convergence_file, index=False)\n",
    "print(f\"Convergence analysis saved to: {convergence_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3d7ab",
   "metadata": {},
   "source": [
    "## STEP 6: Statistical Tests and Significance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec76a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PAIRED T-TESTS: Convergence Efficiency\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTesting whether DNS-GA reaches convergence with significantly fewer evaluations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  {'='*60}\")\n",
    "    \n",
    "    if len(converged_data) < 3:\n",
    "        print(f\"  âœ— Insufficient data for statistical test (n={len(converged_data)})\")\n",
    "        continue\n",
    "    \n",
    "    # Get matched pairs (same seeds)\n",
    "    baseline_evals = converged_data['baseline_convergence_evals'].values\n",
    "    ga_evals = converged_data['ga_evals_to_convergence'].values\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_value = stats.ttest_rel(ga_evals, baseline_evals)\n",
    "    \n",
    "    # Effect size (Cohen's d for paired samples)\n",
    "    differences = ga_evals - baseline_evals\n",
    "    mean_diff = np.mean(differences)\n",
    "    std_diff = np.std(differences, ddof=1)\n",
    "    cohens_d = mean_diff / std_diff if std_diff > 0 else 0\n",
    "    \n",
    "    # Mean evaluation savings\n",
    "    mean_savings = np.mean(converged_data['eval_savings'].values)\n",
    "    mean_savings_pct = np.mean(converged_data['pct_savings'].values)\n",
    "    \n",
    "    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "    \n",
    "    print(f\"\\n  Sample size: {len(converged_data)} seeds\")\n",
    "    print(f\"  Mean baseline evals:  {np.mean(baseline_evals):>10,.0f}\")\n",
    "    print(f\"  Mean DNS-GA evals:    {np.mean(ga_evals):>10,.0f}\")\n",
    "    print(f\"  Mean difference:      {mean_diff:>10,.0f} ({mean_savings_pct:+.2f}%)\")\n",
    "    print(f\"\\n  t-statistic: {t_stat:>7.3f}\")\n",
    "    print(f\"  p-value:     {p_value:>7.4f} {significance}\")\n",
    "    print(f\"  Cohen's d:   {cohens_d:>7.3f}\")\n",
    "    \n",
    "    # Interpret effect size\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect = \"medium\"\n",
    "    else:\n",
    "        effect = \"large\"\n",
    "    \n",
    "    print(f\"  Effect size: {effect}\")\n",
    "    \n",
    "    # Interpret results\n",
    "    if p_value < 0.05:\n",
    "        if mean_diff < 0:\n",
    "            print(f\"\\n  âœ“ SIGNIFICANT: DNS-GA reaches convergence with FEWER evaluations\")\n",
    "        else:\n",
    "            print(f\"\\n  âœ— SIGNIFICANT: DNS-GA requires MORE evaluations\")\n",
    "    else:\n",
    "        print(f\"\\n  ~ NOT SIGNIFICANT: No reliable difference in convergence speed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34024c",
   "metadata": {},
   "source": [
    "### Config Comparison: g300_gen2 vs g1000_gen4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CONFIG COMPARISON: Frequent (g300) vs Rare (g1000) GA Calls\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nQuestion: Does calling GA more frequently reduce seed dependency?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get data for both configs\n",
    "g300_data = convergence_df[convergence_df['config_name'] == 'DNS-GA_g300_gen2']\n",
    "g1000_data = convergence_df[convergence_df['config_name'] == 'DNS-GA_g1000_gen4']\n",
    "\n",
    "g300_converged = g300_data[g300_data['converged']]\n",
    "g1000_converged = g1000_data[g1000_data['converged']]\n",
    "\n",
    "print(f\"\\nSuccess Rates:\")\n",
    "print(f\"  g300_gen2 (10 GA calls):  {len(g300_converged)}/{len(g300_data)} seeds ({len(g300_converged)/len(g300_data)*100:.1f}%)\")\n",
    "print(f\"  g1000_gen4 (3 GA calls):   {len(g1000_converged)}/{len(g1000_data)} seeds ({len(g1000_converged)/len(g1000_data)*100:.1f}%)\")\n",
    "\n",
    "if len(g300_converged) > 0 and len(g1000_converged) > 0:\n",
    "    print(f\"\\nEvaluation Savings (converged seeds only):\")\n",
    "    print(f\"  g300_gen2:  {np.mean(g300_converged['pct_savings'].values):>6.2f}% Â± {np.std(g300_converged['pct_savings'].values):.2f}%\")\n",
    "    print(f\"  g1000_gen4: {np.mean(g1000_converged['pct_savings'].values):>6.2f}% Â± {np.std(g1000_converged['pct_savings'].values):.2f}%\")\n",
    "    \n",
    "    # Compare on common seeds (seeds that converged in both configs)\n",
    "    common_seeds = set(g300_converged['seed'].values) & set(g1000_converged['seed'].values)\n",
    "    \n",
    "    if len(common_seeds) >= 3:\n",
    "        g300_common = g300_converged[g300_converged['seed'].isin(common_seeds)]\n",
    "        g1000_common = g1000_converged[g1000_converged['seed'].isin(common_seeds)]\n",
    "        \n",
    "        # Sort by seed to ensure matching\n",
    "        g300_common = g300_common.sort_values('seed')\n",
    "        g1000_common = g1000_common.sort_values('seed')\n",
    "        \n",
    "        g300_evals = g300_common['ga_evals_to_convergence'].values\n",
    "        g1000_evals = g1000_common['ga_evals_to_convergence'].values\n",
    "        \n",
    "        # Paired t-test\n",
    "        t_stat, p_value = stats.ttest_rel(g300_evals, g1000_evals)\n",
    "        \n",
    "        mean_diff = np.mean(g300_evals) - np.mean(g1000_evals)\n",
    "        \n",
    "        print(f\"\\nDirect Comparison (n={len(common_seeds)} common seeds):\")\n",
    "        print(f\"  g300_gen2 mean:  {np.mean(g300_evals):>10,.0f} evals\")\n",
    "        print(f\"  g1000_gen4 mean: {np.mean(g1000_evals):>10,.0f} evals\")\n",
    "        print(f\"  Difference:      {mean_diff:>10,.0f} evals\")\n",
    "        print(f\"  t-statistic:     {t_stat:>7.3f}\")\n",
    "        print(f\"  p-value:         {p_value:>7.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            if mean_diff < 0:\n",
    "                print(f\"\\n  âœ“ g300_gen2 significantly FASTER to convergence\")\n",
    "            else:\n",
    "                print(f\"\\n  âœ“ g1000_gen4 significantly FASTER to convergence\")\n",
    "        else:\n",
    "            print(f\"\\n  ~ No significant difference between configs\")\n",
    "    else:\n",
    "        print(f\"\\n  Insufficient common seeds for comparison (n={len(common_seeds)})\")\n",
    "    \n",
    "    # Variance comparison (seed dependency)\n",
    "    print(f\"\\nVariance (seed dependency):\")\n",
    "    print(f\"  g300_gen2 std:  {np.std(g300_converged['pct_savings'].values):.2f}%\")\n",
    "    print(f\"  g1000_gen4 std: {np.std(g1000_converged['pct_savings'].values):.2f}%\")\n",
    "    \n",
    "    var_ratio = np.var(g300_converged['pct_savings'].values) / np.var(g1000_converged['pct_savings'].values)\n",
    "    print(f\"  Variance ratio: {var_ratio:.2f}\")\n",
    "    \n",
    "    if var_ratio < 0.8:\n",
    "        print(f\"  â†’ g300_gen2 shows LESS seed dependency\")\n",
    "    elif var_ratio > 1.2:\n",
    "        print(f\"  â†’ g1000_gen4 shows LESS seed dependency\")\n",
    "    else:\n",
    "        print(f\"  â†’ Similar seed dependency\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42248324",
   "metadata": {},
   "source": [
    "### Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78890074",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DISTRIBUTION ANALYSIS: Evaluation Savings\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    savings_pct = converged_data['pct_savings'].values\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  {'='*60}\")\n",
    "    \n",
    "    # Percentiles\n",
    "    print(f\"\\n  Percentiles of evaluation savings:\")\n",
    "    print(f\"    10th: {np.percentile(savings_pct, 10):>6.2f}%\")\n",
    "    print(f\"    25th: {np.percentile(savings_pct, 25):>6.2f}%\")\n",
    "    print(f\"    50th: {np.percentile(savings_pct, 50):>6.2f}% (median)\")\n",
    "    print(f\"    75th: {np.percentile(savings_pct, 75):>6.2f}%\")\n",
    "    print(f\"    90th: {np.percentile(savings_pct, 90):>6.2f}%\")\n",
    "    \n",
    "    # Distribution shape\n",
    "    skewness = stats.skew(savings_pct)\n",
    "    kurtosis = stats.kurtosis(savings_pct)\n",
    "    \n",
    "    print(f\"\\n  Distribution shape:\")\n",
    "    print(f\"    Skewness: {skewness:>6.3f}\", end=\"\")\n",
    "    if abs(skewness) < 0.5:\n",
    "        print(\" (approximately symmetric)\")\n",
    "    elif skewness < 0:\n",
    "        print(\" (left-skewed, negative tail)\")\n",
    "    else:\n",
    "        print(\" (right-skewed, positive tail)\")\n",
    "    \n",
    "    print(f\"    Kurtosis: {kurtosis:>6.3f}\", end=\"\")\n",
    "    if abs(kurtosis) < 0.5:\n",
    "        print(\" (normal-like tails)\")\n",
    "    elif kurtosis > 0:\n",
    "        print(\" (heavy tails, more outliers)\")\n",
    "    else:\n",
    "        print(\" (light tails, fewer outliers)\")\n",
    "    \n",
    "    # Normality test\n",
    "    _, norm_p = stats.normaltest(savings_pct)\n",
    "    print(f\"\\n  Normality test p-value: {norm_p:.4f}\", end=\"\")\n",
    "    if norm_p > 0.05:\n",
    "        print(\" (approximately normal)\")\n",
    "    else:\n",
    "        print(\" (not normally distributed)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd09450",
   "metadata": {},
   "source": [
    "## STEP 7: Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55e77f",
   "metadata": {},
   "source": [
    "### Plot 1: Boxplots of Evaluation Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cac138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot: Evaluation Savings Distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Prepare data for plotting\n",
    "plot_data = []\n",
    "plot_labels = []\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    if len(converged_data) > 0:\n",
    "        plot_data.append(converged_data['pct_savings'].values)\n",
    "        plot_labels.append(config['name'].replace('DNS-GA_', ''))\n",
    "\n",
    "bp = ax.boxplot(plot_data, labels=plot_labels, patch_artist=True)\n",
    "\n",
    "# Color boxplots\n",
    "colors = ['#ff9999', '#66b3ff']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "# Add horizontal line at 0%\n",
    "ax.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='No savings')\n",
    "\n",
    "# Highlight seed 42\n",
    "for i, config in enumerate(dns_ga_configs):\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    seed42_data = config_data[config_data['seed'] == 42]\n",
    "    \n",
    "    if len(seed42_data) > 0 and seed42_data['converged'].values[0]:\n",
    "        seed42_savings = seed42_data['pct_savings'].values[0]\n",
    "        ax.plot(i+1, seed42_savings, 'r*', markersize=15, label='Seed 42' if i == 0 else '')\n",
    "\n",
    "ax.set_xlabel('Configuration', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Evaluation Savings (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Convergence Efficiency: Evaluation Savings Distribution\\n(31 seeds per config)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'seed_variability_logs/boxplot_savings_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Boxplot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beae0d8",
   "metadata": {},
   "source": [
    "### Plot 2: Histograms of Evaluation Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms: Distribution of Evaluation Savings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for idx, config in enumerate(dns_ga_configs):\n",
    "    ax = axes[idx]\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        ax.text(0.5, 0.5, 'No convergence data', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=14)\n",
    "        ax.set_title(config['name'].replace('DNS-GA_', ''))\n",
    "        continue\n",
    "    \n",
    "    savings = converged_data['pct_savings'].values\n",
    "    \n",
    "    # Histogram\n",
    "    n, bins, patches = ax.hist(savings, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Add vertical lines for statistics\n",
    "    mean_val = np.mean(savings)\n",
    "    median_val = np.median(savings)\n",
    "    \n",
    "    ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}%')\n",
    "    ax.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}%')\n",
    "    ax.axvline(0, color='gray', linestyle='-', linewidth=1, alpha=0.5, label='No savings')\n",
    "    \n",
    "    # Highlight seed 42\n",
    "    seed42_data = converged_data[converged_data['seed'] == 42]\n",
    "    if len(seed42_data) > 0:\n",
    "        seed42_val = seed42_data['pct_savings'].values[0]\n",
    "        ax.axvline(seed42_val, color='purple', linestyle=':', linewidth=2, label=f'Seed 42: {seed42_val:.2f}%')\n",
    "    \n",
    "    ax.set_xlabel('Evaluation Savings (%)', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Seeds', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(config['name'].replace('DNS-GA_', ''), fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='upper left', fontsize=9)\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Distribution of Evaluation Savings Across 31 Seeds', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'seed_variability_logs/histograms_savings_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Histograms saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c03d30",
   "metadata": {},
   "source": [
    "### Plot 3: Success Rate Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88041d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: Success rate and mean savings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "config_names = []\n",
    "success_rates = []\n",
    "mean_savings = []\n",
    "positive_savings_pct = []\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    config_names.append(config['name'].replace('DNS-GA_', ''))\n",
    "    success_rates.append((len(converged_data) / len(config_data)) * 100)\n",
    "    \n",
    "    if len(converged_data) > 0:\n",
    "        mean_savings.append(np.mean(converged_data['pct_savings'].values))\n",
    "        positive_count = np.sum(converged_data['pct_savings'].values > 0)\n",
    "        positive_savings_pct.append((positive_count / len(converged_data)) * 100)\n",
    "    else:\n",
    "        mean_savings.append(0)\n",
    "        positive_savings_pct.append(0)\n",
    "\n",
    "# Plot 1: Success rate\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(config_names, success_rates, color=['#ff9999', '#66b3ff'], alpha=0.7, edgecolor='black')\n",
    "ax1.axhline(y=100, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax1.set_ylabel('Success Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Configuration', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Convergence Success Rate\\n(% of seeds reaching baseline convergence faster)', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax1.set_ylim(0, 110)\n",
    "ax1.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars1, success_rates):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "            f'{val:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 2: Mean evaluation savings\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(config_names, mean_savings, color=['#ff9999', '#66b3ff'], alpha=0.7, edgecolor='black')\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax2.set_ylabel('Mean Evaluation Savings (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Configuration', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Mean Convergence Efficiency\\n(converged seeds only)', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax2.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars2, mean_savings):\n",
    "    height = bar.get_height()\n",
    "    va = 'bottom' if height >= 0 else 'top'\n",
    "    offset = 0.5 if height >= 0 else -0.5\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + offset,\n",
    "            f'{val:+.2f}%', ha='center', va=va, fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'seed_variability_logs/success_rates_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Success rate charts saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc7556d",
   "metadata": {},
   "source": [
    "### Plot 4: Scatter Plot - Baseline Convergence vs Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Baseline QD convergence score vs Evaluation savings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors_map = {'DNS-GA_g300_gen2': '#ff9999', 'DNS-GA_g1000_gen4': '#66b3ff'}\n",
    "\n",
    "for idx, config in enumerate(dns_ga_configs):\n",
    "    ax = axes[idx]\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        ax.text(0.5, 0.5, 'No convergence data', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=14)\n",
    "        ax.set_title(config['name'].replace('DNS-GA_', ''))\n",
    "        continue\n",
    "    \n",
    "    baseline_qd = converged_data['baseline_convergence_qd'].values\n",
    "    pct_savings = converged_data['pct_savings'].values\n",
    "    seeds = converged_data['seed'].values\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(baseline_qd, pct_savings, alpha=0.6, s=80, \n",
    "              color=colors_map[config['name']], edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Highlight seed 42\n",
    "    seed42_data = converged_data[converged_data['seed'] == 42]\n",
    "    if len(seed42_data) > 0:\n",
    "        seed42_qd = seed42_data['baseline_convergence_qd'].values[0]\n",
    "        seed42_savings = seed42_data['pct_savings'].values[0]\n",
    "        ax.scatter(seed42_qd, seed42_savings, color='red', s=200, marker='*', \n",
    "                  edgecolors='black', linewidth=1, label='Seed 42', zorder=5)\n",
    "    \n",
    "    # Add horizontal line at 0\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(baseline_qd, pct_savings, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(baseline_qd, p(baseline_qd), \"r--\", alpha=0.5, linewidth=1.5, \n",
    "           label=f'Trend: y={z[0]:.4f}x+{z[1]:.2f}')\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = np.corrcoef(baseline_qd, pct_savings)[0, 1]\n",
    "    \n",
    "    ax.set_xlabel('Baseline Convergence QD Score', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Evaluation Savings (%)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{config[\"name\"].replace(\"DNS-GA_\", \"\")}\\n(correlation: {corr:.3f})', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Does Baseline Convergence Score Predict DNS-GA Efficiency?', \n",
    "            fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'seed_variability_logs/scatter_baseline_vs_savings_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Scatter plots saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072d892",
   "metadata": {},
   "source": [
    "## STEP 8: Final Research Conclusions\n",
    "\n",
    "**Core Question**: Is seed 42's 53% evaluation savings representative, or did we get lucky?\n",
    "\n",
    "This section synthesizes findings from Steps 5-7 to provide publication-ready conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3dcb5",
   "metadata": {},
   "source": [
    "### Question 1: Is Seed 42 an Outlier or Representative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"QUESTION 1: Is Seed 42 an Outlier or Representative?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  {'='*60}\")\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        print(f\"  No convergence data\")\n",
    "        continue\n",
    "    \n",
    "    # Find seed 42\n",
    "    seed42_data = converged_data[converged_data['seed'] == 42]\n",
    "    \n",
    "    if len(seed42_data) == 0:\n",
    "        print(f\"  Seed 42 did not converge\")\n",
    "        continue\n",
    "    \n",
    "    seed42_savings = seed42_data['pct_savings'].values[0]\n",
    "    all_savings = converged_data['pct_savings'].values\n",
    "    \n",
    "    # Statistics\n",
    "    mean_savings = np.mean(all_savings)\n",
    "    std_savings = np.std(all_savings)\n",
    "    median_savings = np.median(all_savings)\n",
    "    percentile = stats.percentileofscore(all_savings, seed42_savings)\n",
    "    z_score = (seed42_savings - mean_savings) / std_savings if std_savings > 0 else 0\n",
    "    \n",
    "    print(f\"\\n  Seed 42 Performance:\")\n",
    "    print(f\"    Evaluation savings: {seed42_savings:.2f}%\")\n",
    "    print(f\"    Percentile rank:    {percentile:.1f}th\")\n",
    "    print(f\"    Z-score:            {z_score:.2f}\")\n",
    "    \n",
    "    print(f\"\\n  Distribution Statistics (31 seeds):\")\n",
    "    print(f\"    Mean:   {mean_savings:.2f}% Â± {std_savings:.2f}%\")\n",
    "    print(f\"    Median: {median_savings:.2f}%\")\n",
    "    print(f\"    Range:  [{np.min(all_savings):.2f}%, {np.max(all_savings):.2f}%]\")\n",
    "    \n",
    "    # Classification\n",
    "    print(f\"\\n  **VERDICT**:\")\n",
    "    if abs(z_score) > 2:\n",
    "        print(f\"    ðŸ”´ OUTLIER: Seed 42 is >2 standard deviations from mean\")\n",
    "        print(f\"       Its {seed42_savings:.2f}% savings is NOT representative of typical performance\")\n",
    "    elif percentile > 75:\n",
    "        print(f\"    ðŸŸ¡ ABOVE AVERAGE: Seed 42 is in top {100-percentile:.0f}%\")\n",
    "        print(f\"       Performance is better than typical, but not impossibly rare\")\n",
    "    elif percentile < 25:\n",
    "        print(f\"    ðŸ”µ BELOW AVERAGE: Seed 42 is in bottom {percentile:.0f}%\")\n",
    "        print(f\"       Performance is worse than typical\")\n",
    "    else:\n",
    "        print(f\"    ðŸŸ¢ TYPICAL: Seed 42 is in middle 50% of distribution\")\n",
    "        print(f\"       Performance is representative of average behavior\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION:\")\n",
    "print(\"=\"*80)\n",
    "print(\"Seed 42's performance should be classified based on z-score and percentile.\")\n",
    "print(\"If |z| > 2, it's a statistical outlier and not representative.\")\n",
    "print(\"If percentile > 75, it's above average but within realistic range.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea82613b",
   "metadata": {},
   "source": [
    "### Question 2: What is Competition-GA's True Success Rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"QUESTION 2: Competition-GA's True Success Rate\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nDefining 'success' as: DNS-GA reaches baseline convergence with fewer evaluations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "success_summary = []\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  {'='*60}\")\n",
    "    \n",
    "    # Success rate 1: Convergence rate\n",
    "    convergence_rate = (len(converged_data) / len(config_data)) * 100\n",
    "    print(f\"\\n  1. Convergence Rate:\")\n",
    "    print(f\"     {len(converged_data)}/{len(config_data)} seeds ({convergence_rate:.1f}%)\")\n",
    "    print(f\"     â†’ Reached baseline's final QD score within 3000 iterations\")\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        print(f\"\\n  âš  No seeds converged - unable to calculate efficiency metrics\")\n",
    "        continue\n",
    "    \n",
    "    # Success rate 2: Positive savings rate\n",
    "    positive_savings = converged_data['pct_savings'] > 0\n",
    "    positive_rate = (positive_savings.sum() / len(converged_data)) * 100\n",
    "    print(f\"\\n  2. Efficiency Success Rate (converged seeds only):\")\n",
    "    print(f\"     {positive_savings.sum()}/{len(converged_data)} seeds ({positive_rate:.1f}%)\")\n",
    "    print(f\"     â†’ Converged faster than baseline\")\n",
    "    \n",
    "    # Success rate 3: Combined\n",
    "    combined_success = positive_savings.sum()\n",
    "    combined_rate = (combined_success / len(config_data)) * 100\n",
    "    print(f\"\\n  3. Overall Success Rate (all 31 seeds):\")\n",
    "    print(f\"     {combined_success}/{len(config_data)} seeds ({combined_rate:.1f}%)\")\n",
    "    print(f\"     â†’ Both converged AND faster than baseline\")\n",
    "    \n",
    "    # Evaluation savings statistics\n",
    "    all_savings = converged_data['pct_savings'].values\n",
    "    print(f\"\\n  Evaluation Savings (converged seeds):\")\n",
    "    print(f\"     Mean:   {np.mean(all_savings):>6.2f}%\")\n",
    "    print(f\"     Median: {np.median(all_savings):>6.2f}%\")\n",
    "    print(f\"     Std:    {np.std(all_savings):>6.2f}%\")\n",
    "    \n",
    "    # Best and worst performers\n",
    "    best_idx = np.argmax(all_savings)\n",
    "    worst_idx = np.argmin(all_savings)\n",
    "    best_seed = converged_data.iloc[best_idx]['seed']\n",
    "    worst_seed = converged_data.iloc[worst_idx]['seed']\n",
    "    \n",
    "    print(f\"\\n  Best performer:  Seed {int(best_seed)} ({all_savings[best_idx]:+.2f}%)\")\n",
    "    print(f\"  Worst performer: Seed {int(worst_seed)} ({all_savings[worst_idx]:+.2f}%)\")\n",
    "    \n",
    "    success_summary.append({\n",
    "        'config': config['name'],\n",
    "        'convergence_rate': convergence_rate,\n",
    "        'efficiency_rate': positive_rate,\n",
    "        'overall_success': combined_rate,\n",
    "        'mean_savings': np.mean(all_savings),\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: Success Rate Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "success_df = pd.DataFrame(success_summary)\n",
    "print(\"\\n\" + success_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION:\")\n",
    "print(\"=\"*80)\n",
    "print(\"Report 'Overall Success Rate' as the true success rate:\")\n",
    "print(\"  â†’ Percentage of all 31 seeds that both converged AND saved evaluations\")\n",
    "print(\"This is the most honest metric for publication.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e813974",
   "metadata": {},
   "source": [
    "### Question 3: Which Config is Better? (g300_gen2 vs g1000_gen4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eeead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"QUESTION 3: Config Comparison - Frequent vs Rare GA Calls\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nHypothesis: More frequent GA calls reduce seed dependency\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get data\n",
    "g300_data = convergence_df[convergence_df['config_name'] == 'DNS-GA_g300_gen2']\n",
    "g1000_data = convergence_df[convergence_df['config_name'] == 'DNS-GA_g1000_gen4']\n",
    "\n",
    "g300_converged = g300_data[g300_data['converged']]\n",
    "g1000_converged = g1000_data[g1000_data['converged']]\n",
    "\n",
    "print(f\"\\nConfiguration Details:\")\n",
    "print(f\"  g300_gen2:  GA every 300 iters Ã— 2 generations = 10 GA calls\")\n",
    "print(f\"  g1000_gen4: GA every 1000 iters Ã— 4 generations = 3 GA calls\")\n",
    "\n",
    "# 1. Success Rate Comparison\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"1. SUCCESS RATE (Overall: converged + faster)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "g300_success = (g300_converged['pct_savings'] > 0).sum()\n",
    "g1000_success = (g1000_converged['pct_savings'] > 0).sum()\n",
    "\n",
    "g300_success_rate = (g300_success / len(g300_data)) * 100\n",
    "g1000_success_rate = (g1000_success / len(g1000_data)) * 100\n",
    "\n",
    "print(f\"  g300_gen2:  {g300_success}/{len(g300_data)} = {g300_success_rate:.1f}%\")\n",
    "print(f\"  g1000_gen4: {g1000_success}/{len(g1000_data)} = {g1000_success_rate:.1f}%\")\n",
    "\n",
    "if g300_success_rate > g1000_success_rate:\n",
    "    diff = g300_success_rate - g1000_success_rate\n",
    "    print(f\"\\n  âœ“ g300_gen2 has {diff:.1f}% higher success rate\")\n",
    "    print(f\"    More frequent GA calls improve reliability\")\n",
    "elif g1000_success_rate > g300_success_rate:\n",
    "    diff = g1000_success_rate - g300_success_rate\n",
    "    print(f\"\\n  âœ“ g1000_gen4 has {diff:.1f}% higher success rate\")\n",
    "    print(f\"    Deeper but rarer GA calls are more effective\")\n",
    "else:\n",
    "    print(f\"\\n  ~ Equal success rates\")\n",
    "\n",
    "# 2. Mean Savings Comparison (converged seeds only)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"2. MEAN EVALUATION SAVINGS (converged seeds)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if len(g300_converged) > 0 and len(g1000_converged) > 0:\n",
    "    g300_mean = np.mean(g300_converged['pct_savings'].values)\n",
    "    g1000_mean = np.mean(g1000_converged['pct_savings'].values)\n",
    "    \n",
    "    print(f\"  g300_gen2:  {g300_mean:>6.2f}%\")\n",
    "    print(f\"  g1000_gen4: {g1000_mean:>6.2f}%\")\n",
    "    \n",
    "    if g300_mean > g1000_mean:\n",
    "        diff = g300_mean - g1000_mean\n",
    "        print(f\"\\n  âœ“ g300_gen2 saves {diff:.2f}% more evaluations on average\")\n",
    "    else:\n",
    "        diff = g1000_mean - g300_mean\n",
    "        print(f\"\\n  âœ“ g1000_gen4 saves {diff:.2f}% more evaluations on average\")\n",
    "else:\n",
    "    print(\"  Insufficient data for comparison\")\n",
    "\n",
    "# 3. Seed Dependency (Variance)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"3. SEED DEPENDENCY (variance of savings)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if len(g300_converged) > 0 and len(g1000_converged) > 0:\n",
    "    g300_std = np.std(g300_converged['pct_savings'].values)\n",
    "    g1000_std = np.std(g1000_converged['pct_savings'].values)\n",
    "    var_ratio = (g300_std**2) / (g1000_std**2)\n",
    "    \n",
    "    print(f\"  g300_gen2 std:  {g300_std:.2f}%\")\n",
    "    print(f\"  g1000_gen4 std: {g1000_std:.2f}%\")\n",
    "    print(f\"  Variance ratio: {var_ratio:.2f}\")\n",
    "    \n",
    "    if var_ratio < 0.8:\n",
    "        print(f\"\\n  âœ“ g300_gen2 shows 20%+ LESS seed dependency\")\n",
    "        print(f\"    More frequent GA calls â†’ more consistent performance\")\n",
    "    elif var_ratio > 1.25:\n",
    "        print(f\"\\n  âœ“ g1000_gen4 shows 20%+ LESS seed dependency\")\n",
    "        print(f\"    Deeper but rarer GA calls â†’ more consistent performance\")\n",
    "    else:\n",
    "        print(f\"\\n  ~ Similar seed dependency\")\n",
    "\n",
    "# 4. Statistical Test (Common Seeds)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"4. STATISTICAL COMPARISON (paired t-test)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "common_seeds = set(g300_converged['seed'].values) & set(g1000_converged['seed'].values)\n",
    "\n",
    "if len(common_seeds) >= 3:\n",
    "    g300_common = g300_converged[g300_converged['seed'].isin(common_seeds)].sort_values('seed')\n",
    "    g1000_common = g1000_converged[g1000_converged['seed'].isin(common_seeds)].sort_values('seed')\n",
    "    \n",
    "    g300_evals = g300_common['ga_evals_to_convergence'].values\n",
    "    g1000_evals = g1000_common['ga_evals_to_convergence'].values\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_rel(g300_evals, g1000_evals)\n",
    "    mean_diff = np.mean(g300_evals) - np.mean(g1000_evals)\n",
    "    \n",
    "    print(f\"  Common seeds: {len(common_seeds)}\")\n",
    "    print(f\"  Mean g300_gen2:  {np.mean(g300_evals):>10,.0f} evals\")\n",
    "    print(f\"  Mean g1000_gen4: {np.mean(g1000_evals):>10,.0f} evals\")\n",
    "    print(f\"  Difference:      {mean_diff:>10,.0f} evals\")\n",
    "    print(f\"  t-statistic:     {t_stat:>7.3f}\")\n",
    "    print(f\"  p-value:         {p_value:>7.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        if mean_diff < 0:\n",
    "            print(f\"\\n  âœ“âœ“ g300_gen2 is SIGNIFICANTLY faster (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"\\n  âœ“âœ“ g1000_gen4 is SIGNIFICANTLY faster (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"\\n  ~ No significant difference (p â‰¥ 0.05)\")\n",
    "else:\n",
    "    print(f\"  Insufficient common seeds (n={len(common_seeds)})\")\n",
    "\n",
    "# 5. Recommendation\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RECOMMENDATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nScore each config on 4 criteria:\")\n",
    "print(\"  1. Success rate (higher is better)\")\n",
    "print(\"  2. Mean savings (higher is better)\")\n",
    "print(\"  3. Low seed dependency (lower variance is better)\")\n",
    "print(\"  4. Statistical significance (p < 0.05 is decisive)\")\n",
    "print(\"\\nRecommend the config that wins more criteria.\")\n",
    "print(\"If tied, recommend based on success rate (most important for reliability).\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95900a62",
   "metadata": {},
   "source": [
    "### Question 4: What Predicts DNS-GA Success?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0874fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"QUESTION 4: Predictive Factors for DNS-GA Success\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAnalyzing correlation between baseline characteristics and DNS-GA efficiency\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  {'='*60}\")\n",
    "    \n",
    "    if len(converged_data) < 3:\n",
    "        print(f\"  Insufficient data (n={len(converged_data)})\")\n",
    "        continue\n",
    "    \n",
    "    # Test 1: Baseline convergence QD score vs savings\n",
    "    baseline_qd = converged_data['baseline_convergence_qd'].values\n",
    "    pct_savings = converged_data['pct_savings'].values\n",
    "    \n",
    "    corr_qd = np.corrcoef(baseline_qd, pct_savings)[0, 1]\n",
    "    \n",
    "    print(f\"\\n  1. Baseline Convergence QD vs Evaluation Savings\")\n",
    "    print(f\"     Correlation: {corr_qd:.3f}\")\n",
    "    \n",
    "    if abs(corr_qd) > 0.5:\n",
    "        direction = \"higher\" if corr_qd > 0 else \"lower\"\n",
    "        print(f\"     âœ“ STRONG: Seeds with {direction} baseline QD save more evaluations\")\n",
    "    elif abs(corr_qd) > 0.3:\n",
    "        direction = \"higher\" if corr_qd > 0 else \"lower\"\n",
    "        print(f\"     ~ MODERATE: Seeds with {direction} baseline QD tend to save more\")\n",
    "    else:\n",
    "        print(f\"     âœ— WEAK: Baseline QD is not a strong predictor\")\n",
    "    \n",
    "    # Test 2: Is there a threshold effect?\n",
    "    median_qd = np.median(baseline_qd)\n",
    "    high_qd_mask = baseline_qd > median_qd\n",
    "    low_qd_mask = baseline_qd <= median_qd\n",
    "    \n",
    "    high_qd_savings = pct_savings[high_qd_mask]\n",
    "    low_qd_savings = pct_savings[low_qd_mask]\n",
    "    \n",
    "    print(f\"\\n  2. High vs Low Baseline QD (threshold: {median_qd:.2f})\")\n",
    "    print(f\"     High QD seeds (n={len(high_qd_savings)}): {np.mean(high_qd_savings):>6.2f}% savings\")\n",
    "    print(f\"     Low QD seeds  (n={len(low_qd_savings)}):  {np.mean(low_qd_savings):>6.2f}% savings\")\n",
    "    \n",
    "    if len(high_qd_savings) >= 2 and len(low_qd_savings) >= 2:\n",
    "        t_stat, p_value = stats.ttest_ind(high_qd_savings, low_qd_savings)\n",
    "        print(f\"     t-test p-value: {p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            if np.mean(high_qd_savings) > np.mean(low_qd_savings):\n",
    "                print(f\"     âœ“ High baseline QD â†’ significantly MORE savings\")\n",
    "            else:\n",
    "                print(f\"     âœ“ Low baseline QD â†’ significantly MORE savings\")\n",
    "        else:\n",
    "            print(f\"     ~ No significant difference by baseline QD\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nCorrelation analysis reveals:\")\n",
    "print(\"  â€¢ If |corr| > 0.5: Baseline QD is a STRONG predictor\")\n",
    "print(\"  â€¢ If |corr| > 0.3: Baseline QD has MODERATE predictive power\")\n",
    "print(\"  â€¢ If |corr| < 0.3: Baseline QD is NOT a reliable predictor\")\n",
    "print(\"\\nPositive correlation: Better baseline performance â†’ more DNS-GA savings\")\n",
    "print(\"Negative correlation: Worse baseline performance â†’ more DNS-GA savings\")\n",
    "print(\"\\nThis helps identify when Competition-GA is most useful.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6483e8fc",
   "metadata": {},
   "source": [
    "### Publication-Ready Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf31b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PUBLICATION-READY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ“Š EXPERIMENTAL DESIGN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  â€¢ 31 random seeds (robust statistical power)\")\n",
    "print(f\"  â€¢ 2 DNS-GA configurations:\")\n",
    "print(f\"    - g300_gen2: Frequent GA (10 calls)\")\n",
    "print(f\"    - g1000_gen4: Rare but deep GA (3 calls)\")\n",
    "print(f\"  â€¢ 1 baseline (DNS without GA)\")\n",
    "print(f\"  â€¢ 3000 iterations per experiment\")\n",
    "print(f\"  â€¢ iso_sigma=0.01 (aggressive mutation)\")\n",
    "print(f\"  â€¢ Total: 94 experiments (~2.6 hours parallel)\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    overall_success = (converged_data['pct_savings'] > 0).sum()\n",
    "    overall_rate = (overall_success / len(config_data)) * 100\n",
    "    mean_savings = np.mean(converged_data['pct_savings'].values)\n",
    "    median_savings = np.median(converged_data['pct_savings'].values)\n",
    "    std_savings = np.std(converged_data['pct_savings'].values)\n",
    "    \n",
    "    print(f\"\\n  {config['name']}:\")\n",
    "    print(f\"    â€¢ Overall success rate: {overall_rate:.1f}% ({overall_success}/{len(config_data)} seeds)\")\n",
    "    print(f\"    â€¢ Mean evaluation savings: {mean_savings:+.2f}% Â± {std_savings:.2f}%\")\n",
    "    print(f\"    â€¢ Median evaluation savings: {median_savings:+.2f}%\")\n",
    "    \n",
    "    # Seed 42 analysis\n",
    "    seed42_data = converged_data[converged_data['seed'] == 42]\n",
    "    if len(seed42_data) > 0:\n",
    "        seed42_savings = seed42_data['pct_savings'].values[0]\n",
    "        z_score = (seed42_savings - mean_savings) / std_savings if std_savings > 0 else 0\n",
    "        percentile = stats.percentileofscore(converged_data['pct_savings'].values, seed42_savings)\n",
    "        \n",
    "        print(f\"    â€¢ Seed 42: {seed42_savings:.2f}% (z={z_score:.2f}, {percentile:.0f}th percentile)\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ RESEARCH QUESTIONS ANSWERED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Automatically generate answers based on data\n",
    "print(\"\\n  Q1: Is seed 42 an outlier?\")\n",
    "for config in dns_ga_configs:\n",
    "    converged_data = convergence_df[(convergence_df['config_name'] == config['name']) & \n",
    "                                   (convergence_df['converged'])]\n",
    "    if len(converged_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    seed42_data = converged_data[converged_data['seed'] == 42]\n",
    "    if len(seed42_data) > 0:\n",
    "        seed42_savings = seed42_data['pct_savings'].values[0]\n",
    "        mean_savings = np.mean(converged_data['pct_savings'].values)\n",
    "        std_savings = np.std(converged_data['pct_savings'].values)\n",
    "        z_score = (seed42_savings - mean_savings) / std_savings if std_savings > 0 else 0\n",
    "        \n",
    "        if abs(z_score) > 2:\n",
    "            verdict = \"YES - Statistical outlier\"\n",
    "        elif stats.percentileofscore(converged_data['pct_savings'].values, seed42_savings) > 75:\n",
    "            verdict = \"NO - Above average but not outlier\"\n",
    "        else:\n",
    "            verdict = \"NO - Typical performance\"\n",
    "        \n",
    "        print(f\"      {config['name']}: {verdict}\")\n",
    "\n",
    "print(\"\\n  Q2: True success rate?\")\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    if len(converged_data) > 0:\n",
    "        overall_success = (converged_data['pct_savings'] > 0).sum()\n",
    "        overall_rate = (overall_success / len(config_data)) * 100\n",
    "        print(f\"      {config['name']}: {overall_rate:.1f}%\")\n",
    "\n",
    "print(\"\\n  Q3: Which config is better?\")\n",
    "g300_data = convergence_df[convergence_df['config_name'] == 'DNS-GA_g300_gen2']\n",
    "g1000_data = convergence_df[convergence_df['config_name'] == 'DNS-GA_g1000_gen4']\n",
    "g300_converged = g300_data[g300_data['converged']]\n",
    "g1000_converged = g1000_data[g1000_data['converged']]\n",
    "\n",
    "if len(g300_converged) > 0 and len(g1000_converged) > 0:\n",
    "    g300_success = (g300_converged['pct_savings'] > 0).sum() / len(g300_data) * 100\n",
    "    g1000_success = (g1000_converged['pct_savings'] > 0).sum() / len(g1000_data) * 100\n",
    "    \n",
    "    if g300_success > g1000_success:\n",
    "        print(f\"      g300_gen2 (success rate: {g300_success:.1f}% vs {g1000_success:.1f}%)\")\n",
    "    elif g1000_success > g300_success:\n",
    "        print(f\"      g1000_gen4 (success rate: {g1000_success:.1f}% vs {g300_success:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"      Tie (both {g300_success:.1f}%)\")\n",
    "\n",
    "print(\"\\n  Q4: What predicts success?\")\n",
    "for config in dns_ga_configs:\n",
    "    converged_data = convergence_df[(convergence_df['config_name'] == config['name']) & \n",
    "                                   (convergence_df['converged'])]\n",
    "    if len(converged_data) >= 3:\n",
    "        corr = np.corrcoef(converged_data['baseline_convergence_qd'].values,\n",
    "                          converged_data['pct_savings'].values)[0, 1]\n",
    "        \n",
    "        if abs(corr) > 0.5:\n",
    "            strength = \"Strong\"\n",
    "        elif abs(corr) > 0.3:\n",
    "            strength = \"Moderate\"\n",
    "        else:\n",
    "            strength = \"Weak\"\n",
    "        \n",
    "        direction = \"positive\" if corr > 0 else \"negative\"\n",
    "        print(f\"      {config['name']}: {strength} {direction} correlation (r={corr:.3f})\")\n",
    "\n",
    "print(\"\\nðŸ’¡ IMPLICATIONS FOR PUBLICATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "  1. HONEST TITLE: Use 'overall success rate' in abstract/title\n",
    "     Example: \"Competition-GA accelerates convergence in X% of cases\"\n",
    "  \n",
    "  2. KEY CLAIM: Report mean savings WITH standard deviation\n",
    "     Example: \"On successful seeds, Competition-GA saves X% Â± Y% evaluations\"\n",
    "  \n",
    "  3. LIMITATIONS: Acknowledge seed dependency\n",
    "     Example: \"Success rate varies significantly with random seed (X% overall)\"\n",
    "  \n",
    "  4. RECOMMENDATION: Specify when Competition-GA is beneficial\n",
    "     - If correlation is strong: \"Most effective when baseline QD is high/low\"\n",
    "     - If success rate < 50%: \"Recommend exploratory use, not production\"\n",
    "     - If success rate > 50%: \"Reliable improvement in majority of cases\"\n",
    "  \n",
    "  5. FUTURE WORK: Address seed dependency\n",
    "     - Investigate seed characteristics that predict success\n",
    "     - Develop adaptive GA triggering based on convergence trajectory\n",
    "     - Test ensemble methods across multiple seeds\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“ STEP 8 COMPLETE - All research questions answered!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4698682",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ NOTEBOOK COMPLETE\n",
    "\n",
    "**All 8 steps implemented:**\n",
    "- âœ… Step 1: Setup and Configuration\n",
    "- âœ… Step 2: Helper Functions\n",
    "- âœ… Step 3: Parallel Experiment Runner\n",
    "- âœ… Step 4: Build Queue and Execute (94 experiments)\n",
    "- âœ… Step 5: Convergence Efficiency Analysis\n",
    "- âœ… Step 6: Statistical Tests\n",
    "- âœ… Step 7: Visualizations\n",
    "- âœ… Step 8: Final Research Conclusions\n",
    "\n",
    "**To run the full experiment:**\n",
    "1. Execute all cells in order (Runtime â†’ Run all)\n",
    "2. Experiments will take ~2.6 hours (2 parallel workers)\n",
    "3. Results saved to `seed_variability_logs/`\n",
    "4. All visualizations and statistics auto-generated\n",
    "\n",
    "**Key outputs:**\n",
    "- `random_seeds.json` - The 31 random seeds used\n",
    "- `all_results_{timestamp}.json` - Raw experiment results\n",
    "- `convergence_analysis_{timestamp}.csv` - Convergence efficiency per seed\n",
    "- `boxplot_savings_{timestamp}.png` - Distribution visualization\n",
    "- `histograms_savings_{timestamp}.png` - Detailed distributions\n",
    "- `success_rates_{timestamp}.png` - Success rate comparison\n",
    "- `scatter_baseline_vs_savings_{timestamp}.png` - Predictive analysis\n",
    "\n",
    "**Research questions answered:**\n",
    "1. Is seed 42 an outlier? â†’ Statistical analysis with z-scores\n",
    "2. True success rate? â†’ Overall % of seeds with positive savings\n",
    "3. Better config? â†’ g300_gen2 vs g1000_gen4 comparison\n",
    "4. Predictive factors? â†’ Correlation analysis\n",
    "\n",
    "Ready to discover Competition-GA's true performance across 31 seeds! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
