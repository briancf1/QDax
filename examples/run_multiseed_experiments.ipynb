{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e2402",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"jax[cuda]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e6d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"git+https://github.com/briancf1/QDax.git#egg=qdax[examples]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33986f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository to get experiment scripts\n",
    "!git clone https://github.com/briancf1/QDax.git\n",
    "%cd QDax/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713fb8b",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a971e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import functools\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from qdax.core.dns_ga import DominatedNoveltySearchGA\n",
    "from qdax.core.dns import DominatedNoveltySearch\n",
    "import qdax.tasks.brax as environments\n",
    "from qdax.tasks.brax.env_creators import scoring_function_brax_envs as scoring_function\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.core.emitters.standard_emitters import MixingEmitter\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Create experiment logs directory\n",
    "os.makedirs(\"multiseed_logs\", exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5494e85a",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "\n",
    "**Selected configurations** (based on initial promising results):\n",
    "- **Baselines**: iso_sigma = 0.005, 0.01\n",
    "- **Control Test**: g_n=∞ (never calls GA, should match baseline exactly)\n",
    "- **Tier 1**: g300_gen2 (proven winner)\n",
    "- **Tier 3**: g500_gen3, g1000_gen4 (best efficiency)\n",
    "- **Tier 4**: g150_gen1 (best performance in initial run)\n",
    "\n",
    "**Seeds**: 3 different random seeds for statistical robustness\n",
    "\n",
    "**Total**: 6 configs × 2 iso_sigmas × 3 seeds = **36 experiments** (~2.4 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_PARAMS = {\n",
    "    'batch_size': 100,\n",
    "    'env_name': 'walker2d_uni',\n",
    "    'episode_length': 100,\n",
    "    'num_iterations': 3000,\n",
    "    'policy_hidden_layer_sizes': (64, 64),\n",
    "    'population_size': 1024,\n",
    "    'k': 3,\n",
    "    'line_sigma': 0.05,\n",
    "}\n",
    "\n",
    "# Multiple seeds for statistical significance\n",
    "SEEDS = [42, 123, 456]\n",
    "\n",
    "# ISO_SIGMA values to test\n",
    "ISO_SIGMAS = [0.005, 0.01]\n",
    "\n",
    "# Selected promising configurations only\n",
    "SELECTED_CONFIGS = [\n",
    "    # Baseline (no GA)\n",
    "    {\n",
    "        'type': 'baseline',\n",
    "        'name': 'DNS_baseline',\n",
    "        'g_n': None,\n",
    "        'num_ga_children': None,\n",
    "        'num_ga_generations': None,\n",
    "    },\n",
    "    # Proven winners\n",
    "    {\n",
    "        'type': 'dns-ga',\n",
    "        'name': 'DNS-GA_g300_gen2',\n",
    "        'g_n': 300,\n",
    "        'num_ga_children': 2,\n",
    "        'num_ga_generations': 2,\n",
    "    },\n",
    "    # Best efficiency configurations\n",
    "    {\n",
    "        'type': 'dns-ga',\n",
    "        'name': 'DNS-GA_g500_gen3',\n",
    "        'g_n': 500,\n",
    "        'num_ga_children': 2,\n",
    "        'num_ga_generations': 3,\n",
    "    },\n",
    "    {\n",
    "        'type': 'dns-ga',\n",
    "        'name': 'DNS-GA_g1000_gen4',\n",
    "        'g_n': 1000,\n",
    "        'num_ga_children': 2,\n",
    "        'num_ga_generations': 4,\n",
    "    },\n",
    "    # Best performance (initial results)\n",
    "    {\n",
    "        'type': 'dns-ga',\n",
    "        'name': 'DNS-GA_g150_gen1',\n",
    "        'g_n': 150,\n",
    "        'num_ga_children': 2,\n",
    "        'num_ga_generations': 1,\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Total experiments: {len(SELECTED_CONFIGS)} configs × {len(ISO_SIGMAS)} iso_sigmas × {len(SEEDS)} seeds = {len(SELECTED_CONFIGS) * len(ISO_SIGMAS) * len(SEEDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdfb963",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba0f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ga_overhead_evals(g_n, num_iterations, population_size, num_ga_children, num_ga_generations):\n",
    "    \"\"\"Calculate total evaluations performed by Competition-GA.\"\"\"\n",
    "    if g_n is None:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    num_ga_calls = num_iterations // g_n\n",
    "    if num_ga_children == 1:\n",
    "        offspring_per_call = population_size * num_ga_generations\n",
    "    else:\n",
    "        offspring_per_call = population_size * num_ga_children * (num_ga_children**num_ga_generations - 1) // (num_ga_children - 1)\n",
    "    evals_per_ga_call = offspring_per_call\n",
    "    total_ga_evals = num_ga_calls * evals_per_ga_call\n",
    "    return total_ga_evals, num_ga_calls, evals_per_ga_call\n",
    "\n",
    "\n",
    "def setup_environment(env_name, episode_length, policy_hidden_layer_sizes, batch_size, seed):\n",
    "    \"\"\"Initialize environment and policy network.\"\"\"\n",
    "    env = environments.create(env_name, episode_length=episode_length)\n",
    "    reset_fn = jax.jit(env.reset)\n",
    "    key = jax.random.key(seed)\n",
    "    \n",
    "    policy_layer_sizes = policy_hidden_layer_sizes + (env.action_size,)\n",
    "    policy_network = MLP(\n",
    "        layer_sizes=policy_layer_sizes,\n",
    "        kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "        final_activation=jnp.tanh,\n",
    "    )\n",
    "    \n",
    "    key, subkey = jax.random.split(key)\n",
    "    keys = jax.random.split(subkey, num=batch_size)\n",
    "    fake_batch = jnp.zeros(shape=(batch_size, env.observation_size))\n",
    "    init_variables = jax.vmap(policy_network.init)(keys, fake_batch)\n",
    "    \n",
    "    return env, policy_network, reset_fn, init_variables, key\n",
    "\n",
    "\n",
    "def create_scoring_function(env, policy_network, reset_fn, episode_length, env_name):\n",
    "    \"\"\"Create scoring function for fitness evaluation.\"\"\"\n",
    "    def play_step_fn(env_state, policy_params, key):\n",
    "        actions = policy_network.apply(policy_params, env_state.obs)\n",
    "        state_desc = env_state.info[\"state_descriptor\"]\n",
    "        next_state = env.step(env_state, actions)\n",
    "        \n",
    "        transition = QDTransition(\n",
    "            obs=env_state.obs,\n",
    "            next_obs=next_state.obs,\n",
    "            rewards=next_state.reward,\n",
    "            dones=next_state.done,\n",
    "            actions=actions,\n",
    "            truncations=next_state.info[\"truncation\"],\n",
    "            state_desc=state_desc,\n",
    "            next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "        )\n",
    "        return next_state, policy_params, key, transition\n",
    "    \n",
    "    descriptor_extraction_fn = environments.descriptor_extractor[env_name]\n",
    "    scoring_fn = functools.partial(\n",
    "        scoring_function,\n",
    "        episode_length=episode_length,\n",
    "        play_reset_fn=reset_fn,\n",
    "        play_step_fn=play_step_fn,\n",
    "        descriptor_extractor=descriptor_extraction_fn,\n",
    "    )\n",
    "    \n",
    "    return scoring_fn\n",
    "\n",
    "\n",
    "def create_mutation_function(iso_sigma):\n",
    "    \"\"\"Create mutation function for Competition-GA.\"\"\"\n",
    "    def competition_ga_mutation_fn(genotype, key):\n",
    "        genotype_flat, tree_def = jax.tree_util.tree_flatten(genotype)\n",
    "        num_leaves = len(genotype_flat)\n",
    "        keys = jax.random.split(key, num_leaves)\n",
    "        keys_tree = jax.tree_util.tree_unflatten(tree_def, keys)\n",
    "        \n",
    "        def add_noise(x, k):\n",
    "            return x + jax.random.normal(k, shape=x.shape) * iso_sigma\n",
    "        \n",
    "        mutated = jax.tree_util.tree_map(add_noise, genotype, keys_tree)\n",
    "        return mutated\n",
    "    \n",
    "    return competition_ga_mutation_fn\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c61ed",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa91b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(config, iso_sigma, seed, baseline_target_qd):\n",
    "    \"\"\"Run a single experiment with given config, iso_sigma, and seed.\"\"\"\n",
    "    exp_name = f\"{config['name']}_iso{iso_sigma}_seed{seed}\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running: {exp_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Setup environment\n",
    "    env, policy_network, reset_fn, init_variables, key = setup_environment(\n",
    "        FIXED_PARAMS['env_name'],\n",
    "        FIXED_PARAMS['episode_length'],\n",
    "        FIXED_PARAMS['policy_hidden_layer_sizes'],\n",
    "        FIXED_PARAMS['batch_size'],\n",
    "        seed\n",
    "    )\n",
    "    \n",
    "    scoring_fn = create_scoring_function(env, policy_network, reset_fn, \n",
    "                                        FIXED_PARAMS['episode_length'],\n",
    "                                        FIXED_PARAMS['env_name'])\n",
    "    \n",
    "    reward_offset = environments.reward_offset[FIXED_PARAMS['env_name']]\n",
    "    metrics_function = functools.partial(\n",
    "        default_qd_metrics,\n",
    "        qd_offset=reward_offset * FIXED_PARAMS['episode_length'],\n",
    "    )\n",
    "    \n",
    "    # Create emitter\n",
    "    variation_fn = functools.partial(\n",
    "        isoline_variation,\n",
    "        iso_sigma=iso_sigma,\n",
    "        line_sigma=FIXED_PARAMS['line_sigma']\n",
    "    )\n",
    "    \n",
    "    mixing_emitter = MixingEmitter(\n",
    "        mutation_fn=None,\n",
    "        variation_fn=variation_fn,\n",
    "        variation_percentage=1.0,\n",
    "        batch_size=FIXED_PARAMS['batch_size']\n",
    "    )\n",
    "    \n",
    "    # Create algorithm (DNS or DNS-GA)\n",
    "    if config['type'] == 'baseline':\n",
    "        algorithm = DominatedNoveltySearch(\n",
    "            scoring_function=scoring_fn,\n",
    "            emitter=mixing_emitter,\n",
    "            metrics_function=metrics_function,\n",
    "            population_size=FIXED_PARAMS['population_size'],\n",
    "            k=FIXED_PARAMS['k'],\n",
    "        )\n",
    "        print(f\"Config: DNS baseline, iso_sigma={iso_sigma}, seed={seed}\")\n",
    "    else:\n",
    "        mutation_fn = create_mutation_function(iso_sigma)\n",
    "        algorithm = DominatedNoveltySearchGA(\n",
    "            scoring_function=scoring_fn,\n",
    "            emitter=mixing_emitter,\n",
    "            metrics_function=metrics_function,\n",
    "            population_size=FIXED_PARAMS['population_size'],\n",
    "            k=FIXED_PARAMS['k'],\n",
    "            g_n=config['g_n'],\n",
    "            num_ga_children=config['num_ga_children'],\n",
    "            num_ga_generations=config['num_ga_generations'],\n",
    "            mutation_fn=mutation_fn,\n",
    "        )\n",
    "        print(f\"Config: g_n={config['g_n']}, gens={config['num_ga_generations']}, iso_sigma={iso_sigma}, seed={seed}\")\n",
    "    \n",
    "    # Initialize\n",
    "    key, subkey = jax.random.split(key)\n",
    "    repertoire, emitter_state, init_metrics = algorithm.init(init_variables, subkey)\n",
    "    \n",
    "    # Setup logging\n",
    "    log_period = 100\n",
    "    num_loops = FIXED_PARAMS['num_iterations'] // log_period\n",
    "    \n",
    "    metrics = {key: jnp.array([]) for key in [\"iteration\", \"qd_score\", \"coverage\", \"max_fitness\", \"time\"]}\n",
    "    init_metrics = jax.tree.map(lambda x: jnp.array([x]) if x.shape == () else x, init_metrics)\n",
    "    init_metrics[\"iteration\"] = jnp.array([0], dtype=jnp.int32)\n",
    "    init_metrics[\"time\"] = jnp.array([0.0])\n",
    "    metrics = jax.tree.map(\n",
    "        lambda metric, init_metric: jnp.concatenate([metric, init_metric], axis=0),\n",
    "        metrics, init_metrics\n",
    "    )\n",
    "    \n",
    "    log_filename = os.path.join(\"multiseed_logs\", f\"{exp_name}_logs.csv\")\n",
    "    csv_logger = CSVLogger(log_filename, header=list(metrics.keys()))\n",
    "    csv_logger.log(jax.tree.map(lambda x: x[-1], metrics))\n",
    "    \n",
    "    # Main training loop\n",
    "    if config['type'] == 'baseline':\n",
    "        algorithm_scan_update = algorithm.scan_update\n",
    "        scan_state = (repertoire, emitter_state, key)\n",
    "    else:\n",
    "        algorithm_scan_update = algorithm.scan_update\n",
    "        scan_state = (repertoire, emitter_state, key, 1)  # generation_counter\n",
    "    \n",
    "    start_time_total = time.time()\n",
    "    convergence_iter = None\n",
    "    \n",
    "    for i in range(num_loops):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scan_state, current_metrics = jax.lax.scan(\n",
    "            algorithm_scan_update,\n",
    "            scan_state,\n",
    "            (),\n",
    "            length=log_period,\n",
    "        )\n",
    "        \n",
    "        timelapse = time.time() - start_time\n",
    "        \n",
    "        current_metrics[\"iteration\"] = jnp.arange(\n",
    "            1 + log_period * i, 1 + log_period * (i + 1), dtype=jnp.int32\n",
    "        )\n",
    "        current_metrics[\"time\"] = jnp.repeat(timelapse, log_period)\n",
    "        metrics = jax.tree.map(\n",
    "            lambda metric, current_metric: jnp.concatenate([metric, current_metric], axis=0),\n",
    "            metrics, current_metrics\n",
    "        )\n",
    "        \n",
    "        csv_logger.log(jax.tree.map(lambda x: x[-1], metrics))\n",
    "        \n",
    "        # Track convergence\n",
    "        if convergence_iter is None and baseline_target_qd is not None:\n",
    "            if float(metrics['qd_score'][-1]) >= baseline_target_qd:\n",
    "                convergence_iter = int(metrics['iteration'][-1])\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Iter {1+log_period*(i+1)}/{FIXED_PARAMS['num_iterations']} - \"\n",
    "                  f\"QD: {metrics['qd_score'][-1]:.2f}, \"\n",
    "                  f\"MaxFit: {metrics['max_fitness'][-1]:.2f}, \"\n",
    "                  f\"Cov: {metrics['coverage'][-1]:.2f}%\")\n",
    "    \n",
    "    total_time = time.time() - start_time_total\n",
    "    \n",
    "    print(f\"Completed in {total_time:.2f}s - Final QD: {metrics['qd_score'][-1]:.2f}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ga_total_evals, ga_num_calls, ga_evals_per_call = calculate_ga_overhead_evals(\n",
    "        config.get('g_n'), FIXED_PARAMS['num_iterations'], FIXED_PARAMS['population_size'],\n",
    "        config.get('num_ga_children'), config.get('num_ga_generations')\n",
    "    )\n",
    "    \n",
    "    baseline_total_evals = FIXED_PARAMS['num_iterations'] * FIXED_PARAMS['batch_size']\n",
    "    eval_savings_pct = None\n",
    "    net_eval_savings_pct = None\n",
    "    dns_ga_total_evals = None\n",
    "    \n",
    "    if convergence_iter is not None:\n",
    "        dns_main_evals = convergence_iter * FIXED_PARAMS['batch_size']\n",
    "        if config['type'] == 'dns-ga':\n",
    "            ga_calls_until_convergence = convergence_iter // config['g_n']\n",
    "            ga_evals_until_convergence = ga_calls_until_convergence * ga_evals_per_call\n",
    "            dns_ga_total_evals = dns_main_evals + ga_evals_until_convergence\n",
    "        else:\n",
    "            dns_ga_total_evals = dns_main_evals\n",
    "        \n",
    "        eval_savings_pct = (FIXED_PARAMS['num_iterations'] - convergence_iter) / FIXED_PARAMS['num_iterations'] * 100\n",
    "        net_eval_savings_pct = (baseline_total_evals - dns_ga_total_evals) / baseline_total_evals * 100\n",
    "    \n",
    "    return {\n",
    "        'config_name': config['name'],\n",
    "        'config_type': config['type'],\n",
    "        'iso_sigma': iso_sigma,\n",
    "        'seed': seed,\n",
    "        'g_n': config.get('g_n'),\n",
    "        'num_ga_generations': config.get('num_ga_generations'),\n",
    "        'final_qd_score': float(metrics['qd_score'][-1]),\n",
    "        'final_max_fitness': float(metrics['max_fitness'][-1]),\n",
    "        'final_coverage': float(metrics['coverage'][-1]),\n",
    "        'total_time': total_time,\n",
    "        'convergence_iter': convergence_iter,\n",
    "        'eval_savings_pct': eval_savings_pct,\n",
    "        'net_eval_savings_pct': net_eval_savings_pct,\n",
    "        'dns_ga_total_evals': dns_ga_total_evals,\n",
    "        'baseline_total_evals': baseline_total_evals,\n",
    "        'ga_overhead_evals': ga_total_evals,\n",
    "        'log_file': log_filename,\n",
    "    }\n",
    "\n",
    "print(\"Experiment runner ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95461b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all experiments\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MULTI-SEED EXPERIMENTS - {timestamp}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total experiments: {len(SELECTED_CONFIGS) * len(ISO_SIGMAS) * len(SEEDS)}\")\n",
    "print(f\"Estimated time: ~{len(SELECTED_CONFIGS) * len(ISO_SIGMAS) * len(SEEDS) * 4 / 60:.1f} hours\")\n",
    "\n",
    "all_results = []\n",
    "experiment_count = 0\n",
    "total_experiments = len(SELECTED_CONFIGS) * len(ISO_SIGMAS) * len(SEEDS)\n",
    "\n",
    "# First, run baseline with iso=0.01 and seed=42 to get target\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# ESTABLISHING BASELINE TARGET\")\n",
    "print(\"#\"*80)\n",
    "baseline_config = SELECTED_CONFIGS[0]  # DNS_baseline\n",
    "baseline_target_result = run_single_experiment(baseline_config, 0.01, 42, None)\n",
    "BASELINE_TARGET_QD = baseline_target_result['final_qd_score']\n",
    "all_results.append(baseline_target_result)\n",
    "experiment_count += 1\n",
    "\n",
    "print(f\"\\n>>> BASELINE TARGET QD: {BASELINE_TARGET_QD:.2f} <<<\\n\")\n",
    "\n",
    "# Now run all experiments\n",
    "for config in SELECTED_CONFIGS:\n",
    "    for iso_sigma in ISO_SIGMAS:\n",
    "        for seed in SEEDS:\n",
    "            # Skip the baseline we already ran\n",
    "            if config['name'] == 'DNS_baseline' and iso_sigma == 0.01 and seed == 42:\n",
    "                continue\n",
    "            \n",
    "            experiment_count += 1\n",
    "            print(f\"\\n{'#'*80}\")\n",
    "            print(f\"# Experiment {experiment_count}/{total_experiments}\")\n",
    "            print(f\"{'#'*80}\")\n",
    "            \n",
    "            try:\n",
    "                result = run_single_experiment(config, iso_sigma, seed, BASELINE_TARGET_QD)\n",
    "                all_results.append(result)\n",
    "                \n",
    "                # Save intermediate results\n",
    "                if experiment_count % 5 == 0:\n",
    "                    interim_file = f\"multiseed_logs/interim_results_{timestamp}.json\"\n",
    "                    with open(interim_file, 'w') as f:\n",
    "                        json.dump({'results': all_results, 'baseline_target': BASELINE_TARGET_QD}, f, indent=2)\n",
    "                    print(f\"\\n>>> Saved interim results: {len(all_results)} experiments <<<\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "# Save final results\n",
    "results_file = f\"multiseed_logs/multiseed_results_{timestamp}.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({'results': all_results, 'baseline_target': BASELINE_TARGET_QD}, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL EXPERIMENTS COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total experiments: {len(all_results)}\")\n",
    "print(f\"Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b20044",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results into DataFrame\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal experiments: {len(df)}\")\n",
    "print(f\"Configurations: {df['config_name'].nunique()}\")\n",
    "print(f\"ISO_SIGMA values: {df['iso_sigma'].nunique()}\")\n",
    "print(f\"Seeds: {df['seed'].nunique()}\")\n",
    "\n",
    "# Calculate statistics by config and iso_sigma\n",
    "stats_df = df.groupby(['config_name', 'iso_sigma']).agg({\n",
    "    'final_qd_score': ['mean', 'std', 'min', 'max'],\n",
    "    'final_max_fitness': ['mean', 'std'],\n",
    "    'convergence_iter': ['mean', 'std'],\n",
    "    'net_eval_savings_pct': ['mean', 'std'],\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICS BY CONFIGURATION AND ISO_SIGMA\")\n",
    "print(\"=\"*80)\n",
    "print(stats_df)\n",
    "\n",
    "# Save statistics\n",
    "stats_df.to_csv(f\"multiseed_logs/statistics_{timestamp}.csv\")\n",
    "print(f\"\\nStatistics saved to: multiseed_logs/statistics_{timestamp}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d32a0",
   "metadata": {},
   "source": [
    "## Visualization: QD Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee69836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grouped bar plot comparing QD scores\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: ISO_SIGMA = 0.005\n",
    "df_005 = df[df['iso_sigma'] == 0.005].copy()\n",
    "summary_005 = df_005.groupby('config_name')['final_qd_score'].agg(['mean', 'std']).reset_index()\n",
    "summary_005 = summary_005.sort_values('mean', ascending=False)\n",
    "\n",
    "ax1.bar(range(len(summary_005)), summary_005['mean'], \n",
    "        yerr=summary_005['std'], capsize=5, alpha=0.7, color='steelblue')\n",
    "ax1.set_xticks(range(len(summary_005)))\n",
    "ax1.set_xticklabels(summary_005['config_name'], rotation=45, ha='right')\n",
    "ax1.set_ylabel('Final QD Score', fontsize=12)\n",
    "ax1.set_title('QD Score Comparison (iso_sigma=0.005)', fontsize=14, fontweight='bold')\n",
    "ax1.axhline(y=BASELINE_TARGET_QD, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Baseline Target ({BASELINE_TARGET_QD:.0f})')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: ISO_SIGMA = 0.01\n",
    "df_01 = df[df['iso_sigma'] == 0.01].copy()\n",
    "summary_01 = df_01.groupby('config_name')['final_qd_score'].agg(['mean', 'std']).reset_index()\n",
    "summary_01 = summary_01.sort_values('mean', ascending=False)\n",
    "\n",
    "ax2.bar(range(len(summary_01)), summary_01['mean'], \n",
    "        yerr=summary_01['std'], capsize=5, alpha=0.7, color='darkgreen')\n",
    "ax2.set_xticks(range(len(summary_01)))\n",
    "ax2.set_xticklabels(summary_01['config_name'], rotation=45, ha='right')\n",
    "ax2.set_ylabel('Final QD Score', fontsize=12)\n",
    "ax2.set_title('QD Score Comparison (iso_sigma=0.01)', fontsize=14, fontweight='bold')\n",
    "ax2.axhline(y=BASELINE_TARGET_QD, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Baseline Target ({BASELINE_TARGET_QD:.0f})')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'multiseed_logs/qd_score_comparison_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 3 configurations by QD score (iso=0.01):\")\n",
    "print(summary_01.head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80faf7c",
   "metadata": {},
   "source": [
    "## Visualization: Efficiency vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec591db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Net evaluation savings vs QD score\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for idx, iso_val in enumerate([0.005, 0.01]):\n",
    "    ax = axes[idx]\n",
    "    df_iso = df[df['iso_sigma'] == iso_val].copy()\n",
    "    \n",
    "    # Remove baseline for clarity (no net savings to plot)\n",
    "    df_iso_ga = df_iso[df_iso['config_type'] == 'dns-ga'].copy()\n",
    "    \n",
    "    if len(df_iso_ga) > 0:\n",
    "        # Plot each seed as a point\n",
    "        for config in df_iso_ga['config_name'].unique():\n",
    "            config_data = df_iso_ga[df_iso_ga['config_name'] == config]\n",
    "            ax.scatter(config_data['net_eval_savings_pct'], \n",
    "                      config_data['final_qd_score'],\n",
    "                      alpha=0.6, s=100, label=config)\n",
    "        \n",
    "        # Add baseline reference line\n",
    "        baseline_mean = df_iso[df_iso['config_name'] == 'DNS_baseline']['final_qd_score'].mean()\n",
    "        ax.axhline(y=baseline_mean, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Baseline Mean ({baseline_mean:.0f})')\n",
    "        ax.axvline(x=0, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "        \n",
    "        ax.set_xlabel('Net Evaluation Savings (%)', fontsize=12)\n",
    "        ax.set_ylabel('Final QD Score', fontsize=12)\n",
    "        ax.set_title(f'Efficiency vs Performance (iso_sigma={iso_val})', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'multiseed_logs/efficiency_vs_performance_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64597b5",
   "metadata": {},
   "source": [
    "## Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071eed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-tests comparing each DNS-GA config against baseline\n",
    "print(\"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nComparing each DNS-GA configuration against DNS baseline\")\n",
    "print(\"(using Welch's t-test for unequal variances)\\n\")\n",
    "\n",
    "results_tests = []\n",
    "\n",
    "for iso_val in ISO_SIGMAS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ISO_SIGMA = {iso_val}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    baseline_scores = df[(df['config_name'] == 'DNS_baseline') & \n",
    "                         (df['iso_sigma'] == iso_val)]['final_qd_score'].values\n",
    "    \n",
    "    for config_name in df[df['config_type'] == 'dns-ga']['config_name'].unique():\n",
    "        ga_scores = df[(df['config_name'] == config_name) & \n",
    "                       (df['iso_sigma'] == iso_val)]['final_qd_score'].values\n",
    "        \n",
    "        if len(ga_scores) > 0 and len(baseline_scores) > 0:\n",
    "            # Welch's t-test (unequal variances)\n",
    "            t_stat, p_value = stats.ttest_ind(ga_scores, baseline_scores, equal_var=False)\n",
    "            \n",
    "            mean_diff = np.mean(ga_scores) - np.mean(baseline_scores)\n",
    "            pct_diff = (mean_diff / np.mean(baseline_scores)) * 100\n",
    "            \n",
    "            # Cohen's d effect size\n",
    "            pooled_std = np.sqrt((np.std(ga_scores, ddof=1)**2 + np.std(baseline_scores, ddof=1)**2) / 2)\n",
    "            cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "            \n",
    "            print(f\"\\n{config_name}:\")\n",
    "            print(f\"  Mean DNS-GA: {np.mean(ga_scores):.2f} ± {np.std(ga_scores):.2f}\")\n",
    "            print(f\"  Mean Baseline: {np.mean(baseline_scores):.2f} ± {np.std(baseline_scores):.2f}\")\n",
    "            print(f\"  Difference: {mean_diff:.2f} ({pct_diff:+.2f}%)\")\n",
    "            print(f\"  t-statistic: {t_stat:.3f}, p-value: {p_value:.4f} {significance}\")\n",
    "            print(f\"  Effect size (Cohen's d): {cohens_d:.3f}\")\n",
    "            \n",
    "            results_tests.append({\n",
    "                'iso_sigma': iso_val,\n",
    "                'config_name': config_name,\n",
    "                'mean_ga': np.mean(ga_scores),\n",
    "                'std_ga': np.std(ga_scores),\n",
    "                'mean_baseline': np.mean(baseline_scores),\n",
    "                'std_baseline': np.std(baseline_scores),\n",
    "                'mean_diff': mean_diff,\n",
    "                'pct_diff': pct_diff,\n",
    "                't_stat': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'cohens_d': cohens_d,\n",
    "                'significant': significance,\n",
    "            })\n",
    "\n",
    "# Save test results\n",
    "tests_df = pd.DataFrame(results_tests)\n",
    "tests_df.to_csv(f'multiseed_logs/significance_tests_{timestamp}.csv', index=False)\n",
    "print(f\"\\n\\nSignificance tests saved to: multiseed_logs/significance_tests_{timestamp}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ee52b3",
   "metadata": {},
   "source": [
    "## Final Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea91950",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best configuration by QD score\n",
    "best_overall = df.loc[df['final_qd_score'].idxmax()]\n",
    "print(f\"\\nBEST OVERALL QD SCORE:\")\n",
    "print(f\"  Config: {best_overall['config_name']}\")\n",
    "print(f\"  ISO_SIGMA: {best_overall['iso_sigma']}\")\n",
    "print(f\"  Seed: {best_overall['seed']}\")\n",
    "print(f\"  QD Score: {best_overall['final_qd_score']:.2f}\")\n",
    "\n",
    "# Best average performance\n",
    "best_avg = df.groupby(['config_name', 'iso_sigma'])['final_qd_score'].mean().reset_index()\n",
    "best_avg_row = best_avg.loc[best_avg['final_qd_score'].idxmax()]\n",
    "print(f\"\\nBEST AVERAGE QD SCORE:\")\n",
    "print(f\"  Config: {best_avg_row['config_name']}\")\n",
    "print(f\"  ISO_SIGMA: {best_avg_row['iso_sigma']}\")\n",
    "print(f\"  Mean QD Score: {best_avg_row['final_qd_score']:.2f}\")\n",
    "\n",
    "# Most efficient with positive net savings\n",
    "df_ga = df[df['config_type'] == 'dns-ga'].copy()\n",
    "df_ga_positive = df_ga[df_ga['net_eval_savings_pct'] > 0]\n",
    "if len(df_ga_positive) > 0:\n",
    "    best_efficiency = df_ga_positive.groupby(['config_name', 'iso_sigma'])['net_eval_savings_pct'].mean().reset_index()\n",
    "    best_eff_row = best_efficiency.loc[best_efficiency['net_eval_savings_pct'].idxmax()]\n",
    "    print(f\"\\nMOST EFFICIENT (Positive Net Savings):\")\n",
    "    print(f\"  Config: {best_eff_row['config_name']}\")\n",
    "    print(f\"  ISO_SIGMA: {best_eff_row['iso_sigma']}\")\n",
    "    print(f\"  Mean Net Savings: {best_eff_row['net_eval_savings_pct']:.2f}%\")\n",
    "else:\n",
    "    print(f\"\\nNO CONFIGURATIONS WITH POSITIVE NET SAVINGS\")\n",
    "\n",
    "# Key findings\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_mean_005 = df[(df['config_name'] == 'DNS_baseline') & (df['iso_sigma'] == 0.005)]['final_qd_score'].mean()\n",
    "baseline_mean_01 = df[(df['config_name'] == 'DNS_baseline') & (df['iso_sigma'] == 0.01)]['final_qd_score'].mean()\n",
    "\n",
    "print(f\"\\n1. Mutation Strength Impact:\")\n",
    "print(f\"   - Baseline with iso=0.01: {baseline_mean_01:.2f}\")\n",
    "print(f\"   - Baseline with iso=0.005: {baseline_mean_005:.2f}\")\n",
    "print(f\"   - Improvement: {((baseline_mean_01 - baseline_mean_005) / baseline_mean_005 * 100):+.2f}%\")\n",
    "\n",
    "# Check if any DNS-GA beats baseline at iso=0.01\n",
    "df_01_ga = df[(df['iso_sigma'] == 0.01) & (df['config_type'] == 'dns-ga')]\n",
    "if len(df_01_ga) > 0:\n",
    "    best_ga_01 = df_01_ga.groupby('config_name')['final_qd_score'].mean().max()\n",
    "    print(f\"\\n2. Competition-GA Performance:\")\n",
    "    print(f\"   - Best DNS-GA (iso=0.01): {best_ga_01:.2f}\")\n",
    "    print(f\"   - Baseline (iso=0.01): {baseline_mean_01:.2f}\")\n",
    "    if best_ga_01 > baseline_mean_01:\n",
    "        print(f\"   ✓ Competition-GA WINS by {((best_ga_01 - baseline_mean_01) / baseline_mean_01 * 100):+.2f}%\")\n",
    "    else:\n",
    "        print(f\"   ✗ Baseline wins by {((baseline_mean_01 - best_ga_01) / best_ga_01 * 100):+.2f}%\")\n",
    "\n",
    "# Count statistically significant improvements\n",
    "sig_tests_01 = [t for t in results_tests if t['iso_sigma'] == 0.01 and t['p_value'] < 0.05 and t['mean_diff'] > 0]\n",
    "print(f\"\\n3. Statistical Significance:\")\n",
    "print(f\"   - Configs with significant improvement over baseline (iso=0.01): {len(sig_tests_01)}\")\n",
    "if sig_tests_01:\n",
    "    for test in sig_tests_01:\n",
    "        print(f\"     • {test['config_name']}: +{test['pct_diff']:.2f}% (p={test['p_value']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a402a",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on these results:\n",
    "\n",
    "1. **If Competition-GA wins statistically**: Document the winning configuration and analyze what makes it effective\n",
    "2. **If baseline wins**: Competition-GA is primarily an efficiency technique, not a performance enhancement\n",
    "3. **Analyze tradeoffs**: Quantify evaluation savings vs QD score differences\n",
    "4. **Publication**: Use these multi-seed results for robust scientific claims\n",
    "\n",
    "All results, statistics, and visualizations are saved in `multiseed_logs/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b9301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
