{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61ca06a",
   "metadata": {},
   "source": [
    "## Humanoid Omni Environment - Full 31-Seed Study\n",
    "\n",
    "**Environment**: humanoid_omni (biped, 17 DoF, final xy position descriptor)\n",
    "**Purpose**: Test Competition-GA generalization to complex bipedal locomotion\n",
    "**Timeline**: Start at 3:15am, finish ~12:15pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29df50b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import warp: No module named 'warp'\n",
      "Failed to import mujoco_warp: No module named 'warp'\n",
      "Setup complete!\n",
      "Current directory: /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/examples\n",
      "JAX devices: [CpuDevice(id=0)]\n",
      "Start time: 2025-11-16 07:35:20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import functools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from qdax.core.dns_ga import DominatedNoveltySearchGA\n",
    "from qdax.core.dns import DominatedNoveltySearch\n",
    "import qdax.tasks.brax as environments\n",
    "from qdax.tasks.brax.env_creators import scoring_function_brax_envs as scoring_function\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.core.emitters.standard_emitters import MixingEmitter\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Create experiment logs directory\n",
    "os.makedirs(\"seed_variability_logs_humanoid_omni\", exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6db60a",
   "metadata": {},
   "source": [
    "## Generate Random Seeds (SAME as ant_omni for consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420c34c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "USING SAME 31 RANDOM SEEDS AS ANT_OMNI\n",
      "================================================================================\n",
      "Seeds: [7817, 52731, 51809, 35457, 47644, 95781, 68031, 49336, 7978, 61378]... (showing first 10)\n",
      "Total: 31 seeds\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Use SAME 31 seeds as ant_omni for direct comparison\n",
    "np.random.seed(2024)\n",
    "RANDOM_SEEDS = np.random.randint(1, 100000, size=31).tolist()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"USING SAME 31 RANDOM SEEDS AS ANT_OMNI\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Seeds: {RANDOM_SEEDS[:10]}... (showing first 10)\")\n",
    "print(f\"Total: {len(RANDOM_SEEDS)} seeds\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save seeds\n",
    "with open('seed_variability_logs_humanoid_omni/random_seeds.json', 'w') as f:\n",
    "    json.dump({'seeds': RANDOM_SEEDS, 'generation_seed': 2024}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d183fbae",
   "metadata": {},
   "source": [
    "## Experiment Configuration - Humanoid Omni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16cf533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HUMANOID OMNI CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "Environment: humanoid_omni\n",
      "  Type: Bipedal humanoid\n",
      "  DoF: 17 (most complex)\n",
      "  Descriptor: Final xy position (2D)\n",
      "  Iterations: 3000\n",
      "  Seeds: 31\n",
      "\n",
      "Configurations:\n",
      "  â€¢ DNS_baseline: No GA\n",
      "  â€¢ DNS-GA_g300_gen2: 10 GA calls\n",
      "  â€¢ DNS-GA_g1000_gen4: 3 GA calls\n",
      "\n",
      "Total Experiments: 93\n",
      "Estimated time (2-parallel): ~10.5 hours\n",
      "Expected completion: ~12:15pm\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "FIXED_PARAMS = {\n",
    "    'batch_size': 256,  # Match ant_omni and walker2d_uni for consistency\n",
    "    'env_name': 'humanoid_omni',  # Bipedal humanoid, 17 DoF, xy position descriptor\n",
    "    'episode_length': 25,\n",
    "    'num_iterations': 3000,\n",
    "    'policy_hidden_layer_sizes': (64, 64),\n",
    "    'population_size': 1024,\n",
    "    'k': 3,\n",
    "    'line_sigma': 0.05,\n",
    "    'iso_sigma': 0.01,\n",
    "}\n",
    "\n",
    "MAIN_CONFIGS = [\n",
    "    # Baseline\n",
    "    {\n",
    "        'type': 'baseline',\n",
    "        'name': 'DNS_baseline',\n",
    "        'g_n': None,\n",
    "        'num_ga_children': None,\n",
    "        'num_ga_generations': None,\n",
    "    },\n",
    "    # Frequent GA\n",
    "    {\n",
    "        'type': 'dns-ga',\n",
    "        'name': 'DNS-GA_g300_gen2',\n",
    "        'g_n': 300,\n",
    "        'num_ga_children': 2,\n",
    "        'num_ga_generations': 2,\n",
    "    },\n",
    "    # Rare but deep GA\n",
    "    {\n",
    "        'type': 'dns-ga',\n",
    "        'name': 'DNS-GA_g1000_gen4',\n",
    "        'g_n': 1000,\n",
    "        'num_ga_children': 2,\n",
    "        'num_ga_generations': 4,\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"HUMANOID OMNI CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nEnvironment: {FIXED_PARAMS['env_name']}\")\n",
    "print(f\"  Type: Bipedal humanoid\")\n",
    "print(f\"  DoF: 17 (most complex)\")\n",
    "print(f\"  Descriptor: Final xy position (2D)\")\n",
    "print(f\"  Iterations: {FIXED_PARAMS['num_iterations']}\")\n",
    "print(f\"  Seeds: {len(RANDOM_SEEDS)}\")\n",
    "\n",
    "print(f\"\\nConfigurations:\")\n",
    "for config in MAIN_CONFIGS:\n",
    "    if config['type'] == 'baseline':\n",
    "        print(f\"  â€¢ {config['name']}: No GA\")\n",
    "    else:\n",
    "        ga_calls = FIXED_PARAMS['num_iterations'] // config['g_n']\n",
    "        print(f\"  â€¢ {config['name']}: {ga_calls} GA calls\")\n",
    "\n",
    "total_exp = len(MAIN_CONFIGS) * len(RANDOM_SEEDS)\n",
    "print(f\"\\nTotal Experiments: {total_exp}\")\n",
    "print(f\"Estimated time (2-parallel): ~{(total_exp / 2) * 13.5 / 60:.1f} hours\")\n",
    "print(f\"Expected completion: ~12:15pm\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9b88e5",
   "metadata": {},
   "source": [
    "## Helper Functions (same as ant_omni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b724de51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded!\n"
     ]
    }
   ],
   "source": [
    "def calculate_ga_overhead_evals(g_n, num_iterations, population_size, num_ga_children, num_ga_generations):\n",
    "    \"\"\"Calculate total evaluations performed by Competition-GA.\"\"\"\n",
    "    if g_n is None or g_n >= num_iterations:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    num_ga_calls = num_iterations // g_n\n",
    "    if num_ga_children == 1:\n",
    "        offspring_per_call = population_size * num_ga_generations\n",
    "    else:\n",
    "        offspring_per_call = population_size * num_ga_children * (num_ga_children**num_ga_generations - 1) // (num_ga_children - 1)\n",
    "    evals_per_ga_call = offspring_per_call\n",
    "    total_ga_evals = num_ga_calls * evals_per_ga_call\n",
    "    return total_ga_evals, num_ga_calls, evals_per_ga_call\n",
    "\n",
    "\n",
    "def setup_environment(env_name, episode_length, policy_hidden_layer_sizes, batch_size, seed):\n",
    "    \"\"\"Initialize environment and policy network.\"\"\"\n",
    "    env = environments.create(env_name, episode_length=episode_length)\n",
    "    reset_fn = jax.jit(env.reset)\n",
    "    key = jax.random.key(seed)\n",
    "    \n",
    "    policy_layer_sizes = policy_hidden_layer_sizes + (env.action_size,)\n",
    "    policy_network = MLP(\n",
    "        layer_sizes=policy_layer_sizes,\n",
    "        kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "        final_activation=jnp.tanh,\n",
    "    )\n",
    "    \n",
    "    key, subkey = jax.random.split(key)\n",
    "    keys = jax.random.split(subkey, num=batch_size)\n",
    "    fake_batch = jnp.zeros(shape=(batch_size, env.observation_size))\n",
    "    init_variables = jax.vmap(policy_network.init)(keys, fake_batch)\n",
    "    \n",
    "    return env, policy_network, reset_fn, init_variables, key\n",
    "\n",
    "\n",
    "def create_scoring_function(env, policy_network, reset_fn, episode_length, env_name):\n",
    "    \"\"\"Create scoring function for fitness evaluation.\"\"\"\n",
    "    def play_step_fn(env_state, policy_params, key):\n",
    "        actions = policy_network.apply(policy_params, env_state.obs)\n",
    "        state_desc = env_state.info[\"state_descriptor\"]\n",
    "        next_state = env.step(env_state, actions)\n",
    "        \n",
    "        transition = QDTransition(\n",
    "            obs=env_state.obs,\n",
    "            next_obs=next_state.obs,\n",
    "            rewards=next_state.reward,\n",
    "            dones=next_state.done,\n",
    "            actions=actions,\n",
    "            truncations=next_state.info[\"truncation\"],\n",
    "            state_desc=state_desc,\n",
    "            next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "        )\n",
    "        return next_state, policy_params, key, transition\n",
    "    \n",
    "    descriptor_extraction_fn = environments.descriptor_extractor[env_name]\n",
    "    scoring_fn = functools.partial(\n",
    "        scoring_function,\n",
    "        episode_length=episode_length,\n",
    "        play_reset_fn=reset_fn,\n",
    "        play_step_fn=play_step_fn,\n",
    "        descriptor_extractor=descriptor_extraction_fn,\n",
    "    )\n",
    "    \n",
    "    return scoring_fn\n",
    "\n",
    "\n",
    "def create_mutation_function(iso_sigma):\n",
    "    \"\"\"Create mutation function for Competition-GA.\"\"\"\n",
    "    def competition_ga_mutation_fn(genotype, key):\n",
    "        genotype_flat, tree_def = jax.tree_util.tree_flatten(genotype)\n",
    "        num_leaves = len(genotype_flat)\n",
    "        keys = jax.random.split(key, num_leaves)\n",
    "        keys_tree = jax.tree_util.tree_unflatten(tree_def, keys)\n",
    "        \n",
    "        def add_noise(x, k):\n",
    "            return x + jax.random.normal(k, shape=x.shape) * iso_sigma\n",
    "        \n",
    "        mutated = jax.tree_util.tree_map(add_noise, genotype, keys_tree)\n",
    "        return mutated\n",
    "    \n",
    "    return competition_ga_mutation_fn\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e952b3",
   "metadata": {},
   "source": [
    "## Single Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b1a1771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment runner ready!\n"
     ]
    }
   ],
   "source": [
    "def run_single_experiment(config, seed, fixed_params):\n",
    "    \"\"\"Run a single experiment with given config and seed.\"\"\"\n",
    "    exp_name = f\"{config['name']}_seed{seed}\"\n",
    "    \n",
    "    env, policy_network, reset_fn, init_variables, key = setup_environment(\n",
    "        fixed_params['env_name'],\n",
    "        fixed_params['episode_length'],\n",
    "        fixed_params['policy_hidden_layer_sizes'],\n",
    "        fixed_params['batch_size'],\n",
    "        seed\n",
    "    )\n",
    "    \n",
    "    scoring_fn = create_scoring_function(env, policy_network, reset_fn, \n",
    "                                        fixed_params['episode_length'],\n",
    "                                        fixed_params['env_name'])\n",
    "    \n",
    "    reward_offset = environments.reward_offset[fixed_params['env_name']]\n",
    "    metrics_function = functools.partial(\n",
    "        default_qd_metrics,\n",
    "        qd_offset=reward_offset * fixed_params['episode_length'],\n",
    "    )\n",
    "    \n",
    "    variation_fn = functools.partial(\n",
    "        isoline_variation,\n",
    "        iso_sigma=fixed_params['iso_sigma'],\n",
    "        line_sigma=fixed_params['line_sigma']\n",
    "    )\n",
    "    \n",
    "    mixing_emitter = MixingEmitter(\n",
    "        mutation_fn=None,\n",
    "        variation_fn=variation_fn,\n",
    "        variation_percentage=1.0,\n",
    "        batch_size=fixed_params['batch_size']\n",
    "    )\n",
    "    \n",
    "    if config['type'] == 'baseline':\n",
    "        algorithm = DominatedNoveltySearch(\n",
    "            scoring_function=scoring_fn,\n",
    "            emitter=mixing_emitter,\n",
    "            metrics_function=metrics_function,\n",
    "            population_size=fixed_params['population_size'],\n",
    "            k=fixed_params['k'],\n",
    "        )\n",
    "    else:\n",
    "        mutation_fn = create_mutation_function(fixed_params['iso_sigma'])\n",
    "        algorithm = DominatedNoveltySearchGA(\n",
    "            scoring_function=scoring_fn,\n",
    "            emitter=mixing_emitter,\n",
    "            metrics_function=metrics_function,\n",
    "            population_size=fixed_params['population_size'],\n",
    "            k=fixed_params['k'],\n",
    "            g_n=config['g_n'],\n",
    "            num_ga_children=config['num_ga_children'],\n",
    "            num_ga_generations=config['num_ga_generations'],\n",
    "            mutation_fn=mutation_fn,\n",
    "        )\n",
    "    \n",
    "    key, subkey = jax.random.split(key)\n",
    "    repertoire, emitter_state, init_metrics = algorithm.init(init_variables, subkey)\n",
    "    \n",
    "    log_period = 100\n",
    "    num_loops = fixed_params['num_iterations'] // log_period\n",
    "    \n",
    "    # Log initial metrics - maintain consistent ordering\n",
    "    log_filename = os.path.join(\"seed_variability_logs_humanoid_omni\", f\"{exp_name}_logs.csv\")\n",
    "    csv_logger = CSVLogger(log_filename, header=[\"coverage\", \"iteration\", \"max_fitness\", \"qd_score\", \"time\"])\n",
    "    \n",
    "    init_metrics_formatted = jax.tree.map(lambda x: jnp.array([x]) if x.shape == () else x, init_metrics)\n",
    "    init_metrics_formatted[\"iteration\"] = jnp.array([0], dtype=jnp.int32)\n",
    "    init_metrics_formatted[\"time\"] = jnp.array([0.0])\n",
    "    csv_logger.log(jax.tree.map(lambda x: x[-1] if len(x.shape) > 0 else x, init_metrics_formatted))\n",
    "    \n",
    "    if config['type'] == 'baseline':\n",
    "        scan_state = (repertoire, emitter_state, key)\n",
    "    else:\n",
    "        scan_state = (repertoire, emitter_state, key, 1)\n",
    "    \n",
    "    start_time_total = time.time()\n",
    "    \n",
    "    for i in range(num_loops):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scan_state, current_metrics = jax.lax.scan(\n",
    "            algorithm.scan_update,\n",
    "            scan_state,\n",
    "            (),\n",
    "            length=log_period,\n",
    "        )\n",
    "        \n",
    "        timelapse = time.time() - start_time\n",
    "        \n",
    "        current_metrics[\"iteration\"] = jnp.arange(\n",
    "            1 + log_period * i, 1 + log_period * (i + 1), dtype=jnp.int32\n",
    "        )\n",
    "        current_metrics[\"time\"] = jnp.repeat(timelapse, log_period)\n",
    "        \n",
    "        # Only log the last value, don't accumulate full history\n",
    "        csv_logger.log(jax.tree.map(lambda x: x[-1], current_metrics))\n",
    "    \n",
    "    total_time = time.time() - start_time_total\n",
    "    \n",
    "    ga_total_evals, ga_num_calls, ga_evals_per_call = calculate_ga_overhead_evals(\n",
    "        config.get('g_n'), fixed_params['num_iterations'], fixed_params['population_size'],\n",
    "        config.get('num_ga_children'), config.get('num_ga_generations')\n",
    "    )\n",
    "    \n",
    "    # Save final repertoire for behavior space visualization\n",
    "    # Extract final repertoire state from scan_state\n",
    "    if config['type'] == 'baseline':\n",
    "        final_repertoire = scan_state[0]  # (repertoire, emitter_state, key)\n",
    "    else:\n",
    "        final_repertoire = scan_state[0]  # (repertoire, emitter_state, key, generation_counter)\n",
    "    \n",
    "    repertoire_file = os.path.join(\"seed_variability_logs_humanoid_omni\", f\"{exp_name}_repertoire.npz\")\n",
    "    jnp.savez(repertoire_file,\n",
    "        descriptors=final_repertoire.descriptors,\n",
    "        fitnesses=final_repertoire.fitnesses\n",
    "    )\n",
    "    \n",
    "    result = {\n",
    "        'config_name': config['name'],\n",
    "        'config_type': config['type'],\n",
    "        'seed': seed,\n",
    "        'g_n': config.get('g_n'),\n",
    "        'num_ga_generations': config.get('num_ga_generations'),\n",
    "        'final_qd_score': float(current_metrics['qd_score'][-1]),\n",
    "        'final_max_fitness': float(current_metrics['max_fitness'][-1]),\n",
    "        'final_coverage': float(current_metrics['coverage'][-1]),\n",
    "        'total_time': total_time,\n",
    "        'ga_overhead_evals': ga_total_evals,\n",
    "        'log_file': log_filename,\n",
    "        'repertoire_file': repertoire_file,\n",
    "    }\n",
    "    \n",
    "    # Aggressive cleanup to free device memory\n",
    "    #del repertoire, emitter_state, final_repertoire, scan_state, current_metrics\n",
    "    #del algorithm, mixing_emitter, scoring_fn, init_variables, env\n",
    "    \n",
    "    # Force JAX to clear device memory\n",
    "    #import gc\n",
    "    #gc.collect()\n",
    "    \n",
    "    # Clear JAX compilation cache periodically\n",
    "    #jax.clear_caches()\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Experiment runner ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38c786",
   "metadata": {},
   "source": [
    "## Run Experiments in 5 Groups (Restart Kernel Between Groups)\n",
    "\n",
    "**Instructions**: \n",
    "1. Run GROUP 1 below (19 experiments, ~20 min)\n",
    "2. **Restart kernel** (Kernel â†’ Restart Kernel)\n",
    "3. Re-run cells 1-8 to reload setup and functions\n",
    "4. Run GROUP 2 (19 experiments, ~20 min)\n",
    "5. Repeat: Restart kernel â†’ Re-run setup â†’ Run next group\n",
    "\n",
    "**Why**: Prevents memory accumulation across 93 experiments\n",
    "\n",
    "**Groups**:\n",
    "- Group 1: Seeds 0-6, all 3 configs (21 experiments)\n",
    "- Group 2: Seeds 7-13, all 3 configs (21 experiments)\n",
    "- Group 3: Seeds 14-20, all 3 configs (21 experiments)\n",
    "- Group 4: Seeds 21-27, all 3 configs (21 experiments)\n",
    "- Group 5: Seeds 28-30, all 3 configs (9 experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845b1d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GROUP 1: Seeds 0 to 6\n",
      "================================================================================\n",
      "Total experiments in this group: 21\n",
      "Estimated time: ~21.0 minutes\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GROUP 1: Seeds 0-6 (indices 0-6) - 21 experiments\n",
    "# ============================================================================\n",
    "\n",
    "GROUP_NUM = 1\n",
    "SEED_START_IDX = 0\n",
    "SEED_END_IDX = 7  # Exclusive, so seeds 0-6\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"GROUP {GROUP_NUM}: Seeds {SEED_START_IDX} to {SEED_END_IDX-1}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_queue = []\n",
    "exp_num = 0\n",
    "\n",
    "for config in MAIN_CONFIGS:\n",
    "    for seed_idx in range(SEED_START_IDX, SEED_END_IDX):\n",
    "        seed = RANDOM_SEEDS[seed_idx]\n",
    "        exp_num += 1\n",
    "        experiment_queue.append((exp_num, exp_num, config, seed))\n",
    "\n",
    "print(f\"Total experiments in this group: {len(experiment_queue)}\")\n",
    "print(f\"Estimated time: ~{len(experiment_queue) * 1:.1f} minutes\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1188b210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING GROUP 1 EXPERIMENTS\n",
      "================================================================================\n",
      "Start time: 2025-11-16 07:35:20\n",
      "================================================================================\n",
      "\n",
      "Running 21 experiments sequentially...\n",
      "\n",
      "  âœ“ Completed: DNS_baseline, seed=7817, QD=125642.4\n",
      "ðŸ“Š Progress: 1/21 (4.8%) | Elapsed: 10.9m | Remaining: ~218.5m\n",
      "  âœ“ Completed: DNS_baseline, seed=7817, QD=125642.4\n",
      "ðŸ“Š Progress: 1/21 (4.8%) | Elapsed: 10.9m | Remaining: ~218.5m\n",
      "  âœ“ Completed: DNS_baseline, seed=52731, QD=125633.7\n",
      "ðŸ“Š Progress: 2/21 (9.5%) | Elapsed: 21.8m | Remaining: ~206.9m\n",
      "  âœ“ Completed: DNS_baseline, seed=52731, QD=125633.7\n",
      "ðŸ“Š Progress: 2/21 (9.5%) | Elapsed: 21.8m | Remaining: ~206.9m\n",
      "  âœ“ Completed: DNS_baseline, seed=51809, QD=125657.5\n",
      "ðŸ“Š Progress: 3/21 (14.3%) | Elapsed: 32.5m | Remaining: ~195.3m\n",
      "  âœ“ Completed: DNS_baseline, seed=51809, QD=125657.5\n",
      "ðŸ“Š Progress: 3/21 (14.3%) | Elapsed: 32.5m | Remaining: ~195.3m\n",
      "  âœ“ Completed: DNS_baseline, seed=35457, QD=125637.9\n",
      "ðŸ“Š Progress: 4/21 (19.0%) | Elapsed: 43.5m | Remaining: ~184.7m\n",
      "  âœ“ Completed: DNS_baseline, seed=35457, QD=125637.9\n",
      "ðŸ“Š Progress: 4/21 (19.0%) | Elapsed: 43.5m | Remaining: ~184.7m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"RUNNING GROUP {GROUP_NUM} EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time_all = time.time()\n",
    "\n",
    "all_results = []\n",
    "errors = []\n",
    "last_update = time.time()\n",
    "\n",
    "print(f\"\\nRunning {len(experiment_queue)} experiments sequentially...\\n\")\n",
    "\n",
    "for exp_num, (exp_idx, total_exp, config, seed) in enumerate(experiment_queue, 1):\n",
    "    try:\n",
    "        result = run_single_experiment(config, seed, FIXED_PARAMS)\n",
    "        result['exp_num'] = exp_num\n",
    "        result['group_num'] = GROUP_NUM\n",
    "        all_results.append(result)\n",
    "        print(f\"  âœ“ Completed: {result['config_name']}, seed={result['seed']}, QD={result['final_qd_score']:.1f}\")\n",
    "    except Exception as e:\n",
    "        errors.append({'config_name': config['name'], 'seed': seed, 'error': str(e)})\n",
    "        print(f\"  âœ— Failed: {config['name']}, seed={seed}, error: {str(e)}\")\n",
    "    \n",
    "    # Progress update every 10 seconds\n",
    "    if time.time() - last_update > 10:\n",
    "        elapsed = time.time() - start_time_all\n",
    "        pct = exp_num / len(experiment_queue) * 100\n",
    "        avg_time = elapsed / exp_num\n",
    "        remaining_time = (len(experiment_queue) - exp_num) * avg_time / 60\n",
    "        print(f\"ðŸ“Š Progress: {exp_num}/{len(experiment_queue)} ({pct:.1f}%) | Elapsed: {elapsed/60:.1f}m | Remaining: ~{remaining_time:.1f}m\")\n",
    "        last_update = time.time()\n",
    "\n",
    "total_time = time.time() - start_time_all\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"GROUP {GROUP_NUM} COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total time: {total_time / 60:.1f} minutes\")\n",
    "print(f\"Successful: {len(all_results)}/{len(experiment_queue)}\")\n",
    "print(f\"Failed: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nErrors:\")\n",
    "    for error in errors:\n",
    "        print(f\"  â€¢ {error['config_name']}, seed={error['seed']}\")\n",
    "\n",
    "results_file = f\"seed_variability_logs_humanoid_omni/group{GROUP_NUM}_results_{timestamp}.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'results': all_results,\n",
    "        'errors': errors,\n",
    "        'total_time': total_time,\n",
    "        'timestamp': timestamp,\n",
    "        'group_num': GROUP_NUM,\n",
    "        'environment': 'humanoid_omni',\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_file}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âš ï¸  NEXT STEP: Restart kernel, re-run setup cells, then run next group\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c98af",
   "metadata": {},
   "source": [
    "---\n",
    "## GROUP 3: Seeds 14-20 (21 experiments)\n",
    "**After completing Group 2**: Restart kernel â†’ Re-run cells 1-8 â†’ Run cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GROUP 3: Seeds 14-20 (indices 14-20) - 21 experiments\n",
    "# ============================================================================\n",
    "\n",
    "GROUP_NUM = 3\n",
    "SEED_START_IDX = 14\n",
    "SEED_END_IDX = 21  # Exclusive, so seeds 14-20\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"GROUP {GROUP_NUM}: Seeds {SEED_START_IDX} to {SEED_END_IDX-1}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_queue = []\n",
    "exp_num = 0\n",
    "\n",
    "for config in MAIN_CONFIGS:\n",
    "    for seed_idx in range(SEED_START_IDX, SEED_END_IDX):\n",
    "        seed = RANDOM_SEEDS[seed_idx]\n",
    "        exp_num += 1\n",
    "        experiment_queue.append((exp_num, exp_num, config, seed))\n",
    "\n",
    "print(f\"Total experiments in this group: {len(experiment_queue)}\")\n",
    "print(f\"Estimated time: ~{len(experiment_queue) * 1:.1f} minutes\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"RUNNING GROUP {GROUP_NUM} EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time_all = time.time()\n",
    "\n",
    "all_results = []\n",
    "errors = []\n",
    "last_update = time.time()\n",
    "\n",
    "print(f\"\\nRunning {len(experiment_queue)} experiments sequentially...\\n\")\n",
    "\n",
    "for exp_num, (exp_idx, total_exp, config, seed) in enumerate(experiment_queue, 1):\n",
    "    try:\n",
    "        result = run_single_experiment(config, seed, FIXED_PARAMS)\n",
    "        result['exp_num'] = exp_num\n",
    "        result['group_num'] = GROUP_NUM\n",
    "        all_results.append(result)\n",
    "        print(f\"  âœ“ Completed: {result['config_name']}, seed={result['seed']}, QD={result['final_qd_score']:.1f}\")\n",
    "    except Exception as e:\n",
    "        errors.append({'config_name': config['name'], 'seed': seed, 'error': str(e)})\n",
    "        print(f\"  âœ— Failed: {config['name']}, seed={seed}, error: {str(e)}\")\n",
    "    \n",
    "    # Progress update every 10 seconds\n",
    "    if time.time() - last_update > 10:\n",
    "        elapsed = time.time() - start_time_all\n",
    "        pct = exp_num / len(experiment_queue) * 100\n",
    "        avg_time = elapsed / exp_num\n",
    "        remaining_time = (len(experiment_queue) - exp_num) * avg_time / 60\n",
    "        print(f\"ðŸ“Š Progress: {exp_num}/{len(experiment_queue)} ({pct:.1f}%) | Elapsed: {elapsed/60:.1f}m | Remaining: ~{remaining_time:.1f}m\")\n",
    "        last_update = time.time()\n",
    "\n",
    "total_time = time.time() - start_time_all\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"GROUP {GROUP_NUM} COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total time: {total_time / 60:.1f} minutes\")\n",
    "print(f\"Successful: {len(all_results)}/{len(experiment_queue)}\")\n",
    "print(f\"Failed: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nErrors:\")\n",
    "    for error in errors:\n",
    "        print(f\"  â€¢ {error['config_name']}, seed={error['seed']}\")\n",
    "\n",
    "results_file = f\"seed_variability_logs_humanoid_omni/group{GROUP_NUM}_results_{timestamp}.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'results': all_results,\n",
    "        'errors': errors,\n",
    "        'total_time': total_time,\n",
    "        'timestamp': timestamp,\n",
    "        'group_num': GROUP_NUM,\n",
    "        'environment': 'humanoid_omni',\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_file}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âš ï¸  NEXT STEP: Restart kernel, re-run setup cells, then run next group\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3964c59c",
   "metadata": {},
   "source": [
    "---\n",
    "## GROUP 4: Seeds 21-27 (21 experiments)\n",
    "**After completing Group 3**: Restart kernel â†’ Re-run cells 1-8 â†’ Run cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GROUP 4: Seeds 21-27 (indices 21-27) - 21 experiments\n",
    "# ============================================================================\n",
    "\n",
    "GROUP_NUM = 4\n",
    "SEED_START_IDX = 21\n",
    "SEED_END_IDX = 28  # Exclusive, so seeds 21-27\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"GROUP {GROUP_NUM}: Seeds {SEED_START_IDX} to {SEED_END_IDX-1}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_queue = []\n",
    "exp_num = 0\n",
    "\n",
    "for config in MAIN_CONFIGS:\n",
    "    for seed_idx in range(SEED_START_IDX, SEED_END_IDX):\n",
    "        seed = RANDOM_SEEDS[seed_idx]\n",
    "        exp_num += 1\n",
    "        experiment_queue.append((exp_num, exp_num, config, seed))\n",
    "\n",
    "print(f\"Total experiments in this group: {len(experiment_queue)}\")\n",
    "print(f\"Estimated time: ~{len(experiment_queue) * 1:.1f} minutes\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"RUNNING GROUP {GROUP_NUM} EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time_all = time.time()\n",
    "\n",
    "all_results = []\n",
    "errors = []\n",
    "last_update = time.time()\n",
    "\n",
    "print(f\"\\nRunning {len(experiment_queue)} experiments sequentially...\\n\")\n",
    "\n",
    "for exp_num, (exp_idx, total_exp, config, seed) in enumerate(experiment_queue, 1):\n",
    "    try:\n",
    "        result = run_single_experiment(config, seed, FIXED_PARAMS)\n",
    "        result['exp_num'] = exp_num\n",
    "        result['group_num'] = GROUP_NUM\n",
    "        all_results.append(result)\n",
    "        print(f\"  âœ“ Completed: {result['config_name']}, seed={result['seed']}, QD={result['final_qd_score']:.1f}\")\n",
    "    except Exception as e:\n",
    "        errors.append({'config_name': config['name'], 'seed': seed, 'error': str(e)})\n",
    "        print(f\"  âœ— Failed: {config['name']}, seed={seed}, error: {str(e)}\")\n",
    "    \n",
    "    # Progress update every 10 seconds\n",
    "    if time.time() - last_update > 10:\n",
    "        elapsed = time.time() - start_time_all\n",
    "        pct = exp_num / len(experiment_queue) * 100\n",
    "        avg_time = elapsed / exp_num\n",
    "        remaining_time = (len(experiment_queue) - exp_num) * avg_time / 60\n",
    "        print(f\"ðŸ“Š Progress: {exp_num}/{len(experiment_queue)} ({pct:.1f}%) | Elapsed: {elapsed/60:.1f}m | Remaining: ~{remaining_time:.1f}m\")\n",
    "        last_update = time.time()\n",
    "\n",
    "total_time = time.time() - start_time_all\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"GROUP {GROUP_NUM} COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total time: {total_time / 60:.1f} minutes\")\n",
    "print(f\"Successful: {len(all_results)}/{len(experiment_queue)}\")\n",
    "print(f\"Failed: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nErrors:\")\n",
    "    for error in errors:\n",
    "        print(f\"  â€¢ {error['config_name']}, seed={error['seed']}\")\n",
    "\n",
    "results_file = f\"seed_variability_logs_humanoid_omni/group{GROUP_NUM}_results_{timestamp}.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'results': all_results,\n",
    "        'errors': errors,\n",
    "        'total_time': total_time,\n",
    "        'timestamp': timestamp,\n",
    "        'group_num': GROUP_NUM,\n",
    "        'environment': 'humanoid_omni',\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_file}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âš ï¸  NEXT STEP: Restart kernel, re-run setup cells, then run GROUP 5 (FINAL)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbda39a",
   "metadata": {},
   "source": [
    "---\n",
    "## GROUP 5: Seeds 28-30 (9 experiments) - FINAL GROUP\n",
    "**After completing Group 4**: Restart kernel â†’ Re-run cells 1-8 â†’ Run cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GROUP 5: Seeds 28-30 (indices 28-30) - 9 experiments (FINAL)\n",
    "# ============================================================================\n",
    "\n",
    "GROUP_NUM = 5\n",
    "SEED_START_IDX = 28\n",
    "SEED_END_IDX = 31  # Exclusive, so seeds 28-30\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"GROUP {GROUP_NUM} (FINAL): Seeds {SEED_START_IDX} to {SEED_END_IDX-1}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_queue = []\n",
    "exp_num = 0\n",
    "\n",
    "for config in MAIN_CONFIGS:\n",
    "    for seed_idx in range(SEED_START_IDX, SEED_END_IDX):\n",
    "        seed = RANDOM_SEEDS[seed_idx]\n",
    "        exp_num += 1\n",
    "        experiment_queue.append((exp_num, exp_num, config, seed))\n",
    "\n",
    "print(f\"Total experiments in this group: {len(experiment_queue)}\")\n",
    "print(f\"Estimated time: ~{len(experiment_queue) * 1:.1f} minutes\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea0fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"RUNNING GROUP {GROUP_NUM} EXPERIMENTS (FINAL)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time_all = time.time()\n",
    "\n",
    "all_results = []\n",
    "errors = []\n",
    "last_update = time.time()\n",
    "\n",
    "print(f\"\\nRunning {len(experiment_queue)} experiments sequentially...\\n\")\n",
    "\n",
    "for exp_num, (exp_idx, total_exp, config, seed) in enumerate(experiment_queue, 1):\n",
    "    try:\n",
    "        result = run_single_experiment(config, seed, FIXED_PARAMS)\n",
    "        result['exp_num'] = exp_num\n",
    "        result['group_num'] = GROUP_NUM\n",
    "        all_results.append(result)\n",
    "        print(f\"  âœ“ Completed: {result['config_name']}, seed={result['seed']}, QD={result['final_qd_score']:.1f}\")\n",
    "    except Exception as e:\n",
    "        errors.append({'config_name': config['name'], 'seed': seed, 'error': str(e)})\n",
    "        print(f\"  âœ— Failed: {config['name']}, seed={seed}, error: {str(e)}\")\n",
    "    \n",
    "    # Progress update every 10 seconds\n",
    "    if time.time() - last_update > 10:\n",
    "        elapsed = time.time() - start_time_all\n",
    "        pct = exp_num / len(experiment_queue) * 100\n",
    "        avg_time = elapsed / exp_num\n",
    "        remaining_time = (len(experiment_queue) - exp_num) * avg_time / 60\n",
    "        print(f\"ðŸ“Š Progress: {exp_num}/{len(experiment_queue)} ({pct:.1f}%) | Elapsed: {elapsed/60:.1f}m | Remaining: ~{remaining_time:.1f}m\")\n",
    "        last_update = time.time()\n",
    "\n",
    "total_time = time.time() - start_time_all\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"GROUP {GROUP_NUM} COMPLETE! ðŸŽ‰\")\n",
    "print(\"=\"*80)\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total time: {total_time / 60:.1f} minutes\")\n",
    "print(f\"Successful: {len(all_results)}/{len(experiment_queue)}\")\n",
    "print(f\"Failed: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nErrors:\")\n",
    "    for error in errors:\n",
    "        print(f\"  â€¢ {error['config_name']}, seed={error['seed']}\")\n",
    "\n",
    "results_file = f\"seed_variability_logs_humanoid_omni/group{GROUP_NUM}_results_{timestamp}.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'results': all_results,\n",
    "        'errors': errors,\n",
    "        'total_time': total_time,\n",
    "        'timestamp': timestamp,\n",
    "        'group_num': GROUP_NUM,\n",
    "        'environment': 'humanoid_omni',\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_file}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ALL 5 GROUPS COMPLETE! All humanoid_omni experiments finished.\")\n",
    "print(\"ðŸ“ Results in: seed_variability_logs_humanoid_omni/group1-5_results_*.json\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ced0d",
   "metadata": {},
   "source": [
    "## Quick Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0225f989",
   "metadata": {},
   "source": [
    "---\n",
    "## GROUP 2: Seeds 7-13 (21 experiments)\n",
    "**After completing Group 1**: Restart kernel â†’ Re-run cells 1-8 â†’ Run cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GROUP 2: Seeds 7-13 (indices 7-13) - 21 experiments\n",
    "# ============================================================================\n",
    "\n",
    "GROUP_NUM = 2\n",
    "SEED_START_IDX = 7\n",
    "SEED_END_IDX = 14  # Exclusive, so seeds 7-13\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"GROUP {GROUP_NUM}: Seeds {SEED_START_IDX} to {SEED_END_IDX-1}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_queue = []\n",
    "exp_num = 0\n",
    "\n",
    "for config in MAIN_CONFIGS:\n",
    "    for seed_idx in range(SEED_START_IDX, SEED_END_IDX):\n",
    "        seed = RANDOM_SEEDS[seed_idx]\n",
    "        exp_num += 1\n",
    "        experiment_queue.append((exp_num, exp_num, config, seed))\n",
    "\n",
    "print(f\"Total experiments in this group: {len(experiment_queue)}\")\n",
    "print(f\"Estimated time: ~{len(experiment_queue) * 1:.1f} minutes\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b89cead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"RUNNING GROUP {GROUP_NUM} EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time_all = time.time()\n",
    "\n",
    "all_results = []\n",
    "errors = []\n",
    "last_update = time.time()\n",
    "\n",
    "print(f\"\\nRunning {len(experiment_queue)} experiments sequentially...\\n\")\n",
    "\n",
    "for exp_num, (exp_idx, total_exp, config, seed) in enumerate(experiment_queue, 1):\n",
    "    try:\n",
    "        result = run_single_experiment(config, seed, FIXED_PARAMS)\n",
    "        result['exp_num'] = exp_num\n",
    "        result['group_num'] = GROUP_NUM\n",
    "        all_results.append(result)\n",
    "        print(f\"  âœ“ Completed: {result['config_name']}, seed={result['seed']}, QD={result['final_qd_score']:.1f}\")\n",
    "    except Exception as e:\n",
    "        errors.append({'config_name': config['name'], 'seed': seed, 'error': str(e)})\n",
    "        print(f\"  âœ— Failed: {config['name']}, seed={seed}, error: {str(e)}\")\n",
    "    \n",
    "    # Progress update every 10 seconds\n",
    "    if time.time() - last_update > 10:\n",
    "        elapsed = time.time() - start_time_all\n",
    "        pct = exp_num / len(experiment_queue) * 100\n",
    "        avg_time = elapsed / exp_num\n",
    "        remaining_time = (len(experiment_queue) - exp_num) * avg_time / 60\n",
    "        print(f\"ðŸ“Š Progress: {exp_num}/{len(experiment_queue)} ({pct:.1f}%) | Elapsed: {elapsed/60:.1f}m | Remaining: ~{remaining_time:.1f}m\")\n",
    "        last_update = time.time()\n",
    "\n",
    "total_time = time.time() - start_time_all\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"GROUP {GROUP_NUM} COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total time: {total_time / 60:.1f} minutes\")\n",
    "print(f\"Successful: {len(all_results)}/{len(experiment_queue)}\")\n",
    "print(f\"Failed: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nErrors:\")\n",
    "    for error in errors:\n",
    "        print(f\"  â€¢ {error['config_name']}, seed={error['seed']}\")\n",
    "\n",
    "results_file = f\"seed_variability_logs_humanoid_omni/group{GROUP_NUM}_results_{timestamp}.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'results': all_results,\n",
    "        'errors': errors,\n",
    "        'total_time': total_time,\n",
    "        'timestamp': timestamp,\n",
    "        'group_num': GROUP_NUM,\n",
    "        'environment': 'humanoid_omni',\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_file}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âš ï¸  NEXT STEP: Restart kernel, re-run setup cells, then run next group\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84693a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_results) > 0:\n",
    "    df = pd.DataFrame(all_results)\n",
    "    print(\"=\"*80)\n",
    "    print(\"HUMANOID_OMNI RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal experiments: {len(df)}\")\n",
    "    print(\"\\nFinal QD Scores by Configuration:\")\n",
    "    print(df.groupby('config_name')['final_qd_score'].agg(['mean', 'std', 'min', 'max']).round(2))\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ“ Results ready for integration with ant_omni and walker2d data\")\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
