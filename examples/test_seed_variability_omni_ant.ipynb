{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33399b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"jax[cuda]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fcbdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"git+https://github.com/briancf1/QDax.git#egg=qdax[examples]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413df8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository to get experiment scripts\n",
    "!git clone https://github.com/briancf1/QDax.git\n",
    "%cd QDax/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04508ef4",
   "metadata": {},
   "source": [
    "## STEP 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "acef4b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n",
      "Current directory: /Users/briancf/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/examples\n",
      "JAX devices: [CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import functools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from qdax.core.dns_ga import DominatedNoveltySearchGA\n",
    "from qdax.core.dns import DominatedNoveltySearch\n",
    "import qdax.tasks.brax as environments\n",
    "from qdax.tasks.brax.env_creators import scoring_function_brax_envs as scoring_function\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.core.emitters.standard_emitters import MixingEmitter\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Create experiment logs directory\n",
    "os.makedirs(\"seed_variability_logs\", exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e6083",
   "metadata": {},
   "source": [
    "## Generate Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff749d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATED 31 RANDOM SEEDS\n",
      "================================================================================\n",
      "Seeds: [7817, 52731, 51809, 35457, 47644, 95781, 68031, 49336, 7978, 61378]... (showing first 10)\n",
      "Range: 317 to 99314\n",
      "Total: 31 seeds\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate 31 random seeds for robust statistical analysis\n",
    "np.random.seed(2024)  # Fixed seed for reproducibility of seed generation\n",
    "RANDOM_SEEDS = np.random.randint(1, 100000, size=31).tolist()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATED 31 RANDOM SEEDS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Seeds: {RANDOM_SEEDS[:10]}... (showing first 10)\")\n",
    "print(f\"Range: {min(RANDOM_SEEDS)} to {max(RANDOM_SEEDS)}\")\n",
    "print(f\"Total: {len(RANDOM_SEEDS)} seeds\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save seeds for reproducibility\n",
    "with open('seed_variability_logs/random_seeds.json', 'w') as f:\n",
    "    json.dump({'seeds': RANDOM_SEEDS, 'generation_seed': 2024}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ed96c",
   "metadata": {},
   "source": [
    "## Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1db35f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPERIMENT CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "Fixed Parameters:\n",
      "  Environment: ant_omni\n",
      "  Iterations: 3000\n",
      "  Batch size: 100\n",
      "  ISO_SIGMA: 0.01\n",
      "  Population: 1024\n",
      "\n",
      "Main Configurations (31 seeds each):\n",
      "  â€¢ DNS_baseline: No GA\n",
      "  â€¢ DNS-GA_g300_gen2: 10 GA calls (every 300 iters, 2 gens)\n",
      "  â€¢ DNS-GA_g1000_gen4: 3 GA calls (every 1000 iters, 4 gens)\n",
      "\n",
      "Sanity Check Configuration (seed 42 only):\n",
      "  â€¢ DNS-GA_sanity_no_ga: g_n=99999 (no GA triggers)\n",
      "    Purpose: Verify DNS-GA without GA = baseline\n",
      "\n",
      "Total Experiments:\n",
      "  Main: 3 configs Ã— 31 seeds = 93\n",
      "  Sanity: 1\n",
      "  Total: 94\n",
      "  Estimated time (2 parallel): ~2.6 hours\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "FIXED_PARAMS = {\n",
    "    'batch_size': 100,\n",
    "    'env_name': 'ant_omni',  # Ant Blocks from DNS paper: obstacles + final xy position\n",
    "    'episode_length': 100,\n",
    "    'num_iterations': 3000,\n",
    "    'policy_hidden_layer_sizes': (64, 64),\n",
    "    'population_size': 1024,\n",
    "    'k': 3,\n",
    "    'line_sigma': 0.05,\n",
    "    'iso_sigma': 0.01,  # Best performer from previous experiments\n",
    "}\n",
    "\n",
    "# Main experimental configurations (run with all 31 seeds)\n",
    "MAIN_CONFIGS = [\n",
    "    # Baseline (no GA)\n",
    "    {\n",
    "        'type': 'baseline',\n",
    "        'name': 'DNS_baseline',\n",
    "        'g_n': None,\n",
    "        'num_ga_children': None,\n",
    "        'num_ga_generations': None,\n",
    "    },\n",
    "    # Frequent GA calls (10 times during 3000 iterations)\n",
    "    {\n",
    "        'type': 'dns-ga',\n",
    "        'name': 'DNS-GA_g300_gen2',\n",
    "        'g_n': 300,\n",
    "        'num_ga_children': 2,\n",
    "        'num_ga_generations': 2,\n",
    "    },\n",
    "    # Rare but deep GA calls (3 times during 3000 iterations, seed 42's winner)\n",
    "    {\n",
    "        'type': 'dns-ga',\n",
    "        'name': 'DNS-GA_g1000_gen4',\n",
    "        'g_n': 1000,\n",
    "        'num_ga_children': 2,\n",
    "        'num_ga_generations': 4,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Sanity check: DNS-GA with g_n so large it never triggers (should match baseline)\n",
    "SANITY_CONFIG = {\n",
    "    'type': 'dns-ga',\n",
    "    'name': 'DNS-GA_sanity_no_ga',\n",
    "    'g_n': 99999,  # Never triggers within 3000 iterations\n",
    "    'num_ga_children': 2,\n",
    "    'num_ga_generations': 2,\n",
    "}\n",
    "\n",
    "# Sanity check runs with just seed 42\n",
    "SANITY_SEED = 42\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFixed Parameters:\")\n",
    "print(f\"  Environment: {FIXED_PARAMS['env_name']}\")\n",
    "print(f\"  Iterations: {FIXED_PARAMS['num_iterations']}\")\n",
    "print(f\"  Batch size: {FIXED_PARAMS['batch_size']}\")\n",
    "print(f\"  ISO_SIGMA: {FIXED_PARAMS['iso_sigma']}\")\n",
    "print(f\"  Population: {FIXED_PARAMS['population_size']}\")\n",
    "\n",
    "print(f\"\\nMain Configurations (31 seeds each):\")\n",
    "for config in MAIN_CONFIGS:\n",
    "    if config['type'] == 'baseline':\n",
    "        print(f\"  â€¢ {config['name']}: No GA\")\n",
    "    else:\n",
    "        ga_calls = FIXED_PARAMS['num_iterations'] // config['g_n']\n",
    "        print(f\"  â€¢ {config['name']}: {ga_calls} GA calls (every {config['g_n']} iters, {config['num_ga_generations']} gens)\")\n",
    "\n",
    "print(f\"\\nSanity Check Configuration (seed {SANITY_SEED} only):\")\n",
    "print(f\"  â€¢ {SANITY_CONFIG['name']}: g_n={SANITY_CONFIG['g_n']} (no GA triggers)\")\n",
    "print(f\"    Purpose: Verify DNS-GA without GA = baseline\")\n",
    "\n",
    "print(f\"\\nTotal Experiments:\")\n",
    "main_exp = len(MAIN_CONFIGS) * len(RANDOM_SEEDS)\n",
    "sanity_exp = 1\n",
    "total_exp = main_exp + sanity_exp\n",
    "print(f\"  Main: {len(MAIN_CONFIGS)} configs Ã— {len(RANDOM_SEEDS)} seeds = {main_exp}\")\n",
    "print(f\"  Sanity: {sanity_exp}\")\n",
    "print(f\"  Total: {total_exp}\")\n",
    "print(f\"  Estimated time (2 parallel): ~{(total_exp / 2) * 3.3 / 60:.1f} hours\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5390bd",
   "metadata": {},
   "source": [
    "## STEP 2: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e67d76f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded!\n"
     ]
    }
   ],
   "source": [
    "def calculate_ga_overhead_evals(g_n, num_iterations, population_size, num_ga_children, num_ga_generations):\n",
    "    \"\"\"Calculate total evaluations performed by Competition-GA.\"\"\"\n",
    "    if g_n is None or g_n >= num_iterations:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    num_ga_calls = num_iterations // g_n\n",
    "    if num_ga_children == 1:\n",
    "        offspring_per_call = population_size * num_ga_generations\n",
    "    else:\n",
    "        offspring_per_call = population_size * num_ga_children * (num_ga_children**num_ga_generations - 1) // (num_ga_children - 1)\n",
    "    evals_per_ga_call = offspring_per_call\n",
    "    total_ga_evals = num_ga_calls * evals_per_ga_call\n",
    "    return total_ga_evals, num_ga_calls, evals_per_ga_call\n",
    "\n",
    "\n",
    "def setup_environment(env_name, episode_length, policy_hidden_layer_sizes, batch_size, seed):\n",
    "    \"\"\"Initialize environment and policy network.\"\"\"\n",
    "    env = environments.create(env_name, episode_length=episode_length)\n",
    "    reset_fn = jax.jit(env.reset)\n",
    "    key = jax.random.key(seed)\n",
    "    \n",
    "    policy_layer_sizes = policy_hidden_layer_sizes + (env.action_size,)\n",
    "    policy_network = MLP(\n",
    "        layer_sizes=policy_layer_sizes,\n",
    "        kernel_init=jax.nn.initializers.lecun_uniform(),\n",
    "        final_activation=jnp.tanh,\n",
    "    )\n",
    "    \n",
    "    key, subkey = jax.random.split(key)\n",
    "    keys = jax.random.split(subkey, num=batch_size)\n",
    "    fake_batch = jnp.zeros(shape=(batch_size, env.observation_size))\n",
    "    init_variables = jax.vmap(policy_network.init)(keys, fake_batch)\n",
    "    \n",
    "    return env, policy_network, reset_fn, init_variables, key\n",
    "\n",
    "\n",
    "def create_scoring_function(env, policy_network, reset_fn, episode_length, env_name):\n",
    "    \"\"\"Create scoring function for fitness evaluation.\"\"\"\n",
    "    def play_step_fn(env_state, policy_params, key):\n",
    "        actions = policy_network.apply(policy_params, env_state.obs)\n",
    "        state_desc = env_state.info[\"state_descriptor\"]\n",
    "        next_state = env.step(env_state, actions)\n",
    "        \n",
    "        transition = QDTransition(\n",
    "            obs=env_state.obs,\n",
    "            next_obs=next_state.obs,\n",
    "            rewards=next_state.reward,\n",
    "            dones=next_state.done,\n",
    "            actions=actions,\n",
    "            truncations=next_state.info[\"truncation\"],\n",
    "            state_desc=state_desc,\n",
    "            next_state_desc=next_state.info[\"state_descriptor\"],\n",
    "        )\n",
    "        return next_state, policy_params, key, transition\n",
    "    \n",
    "    descriptor_extraction_fn = environments.descriptor_extractor[env_name]\n",
    "    scoring_fn = functools.partial(\n",
    "        scoring_function,\n",
    "        episode_length=episode_length,\n",
    "        play_reset_fn=reset_fn,\n",
    "        play_step_fn=play_step_fn,\n",
    "        descriptor_extractor=descriptor_extraction_fn,\n",
    "    )\n",
    "    \n",
    "    return scoring_fn\n",
    "\n",
    "\n",
    "def create_mutation_function(iso_sigma):\n",
    "    \"\"\"Create mutation function for Competition-GA.\"\"\"\n",
    "    def competition_ga_mutation_fn(genotype, key):\n",
    "        genotype_flat, tree_def = jax.tree_util.tree_flatten(genotype)\n",
    "        num_leaves = len(genotype_flat)\n",
    "        keys = jax.random.split(key, num_leaves)\n",
    "        keys_tree = jax.tree_util.tree_unflatten(tree_def, keys)\n",
    "        \n",
    "        def add_noise(x, k):\n",
    "            return x + jax.random.normal(k, shape=x.shape) * iso_sigma\n",
    "        \n",
    "        mutated = jax.tree_util.tree_map(add_noise, genotype, keys_tree)\n",
    "        return mutated\n",
    "    \n",
    "    return competition_ga_mutation_fn\n",
    "\n",
    "\n",
    "def calculate_cumulative_evals_for_log(config, log_df, batch_size, population_size):\n",
    "    \"\"\"Calculate cumulative evaluations at each logged iteration.\"\"\"\n",
    "    iterations = log_df['iteration'].values\n",
    "    evals = np.zeros(len(iterations))\n",
    "    \n",
    "    if config['type'] == 'baseline':\n",
    "        # Baseline: constant batch_size per iteration\n",
    "        evals = iterations * batch_size\n",
    "    else:\n",
    "        # DNS-GA: batch_size + periodic GA overhead\n",
    "        g_n = config['g_n']\n",
    "        num_ga_children = config['num_ga_children']\n",
    "        num_ga_generations = config['num_ga_generations']\n",
    "        \n",
    "        # Calculate GA overhead per call\n",
    "        if g_n >= len(iterations) * 100:  # Sanity check: g_n so large it never triggers\n",
    "            evals = iterations * batch_size\n",
    "        elif num_ga_children == 1:\n",
    "            ga_evals_per_call = population_size * num_ga_generations\n",
    "            for idx, iter_num in enumerate(iterations):\n",
    "                cumulative = iter_num * batch_size\n",
    "                num_ga_calls = iter_num // g_n\n",
    "                cumulative += num_ga_calls * ga_evals_per_call\n",
    "                evals[idx] = cumulative\n",
    "        else:\n",
    "            ga_evals_per_call = (population_size * num_ga_children * \n",
    "                                (num_ga_children**num_ga_generations - 1) // (num_ga_children - 1))\n",
    "            for idx, iter_num in enumerate(iterations):\n",
    "                cumulative = iter_num * batch_size\n",
    "                num_ga_calls = iter_num // g_n\n",
    "                cumulative += num_ga_calls * ga_evals_per_call\n",
    "                evals[idx] = cumulative\n",
    "    \n",
    "    return evals\n",
    "\n",
    "\n",
    "def interpolate_evals_to_milestone(qd_scores, evals, milestone):\n",
    "    \"\"\"Interpolate evaluations needed to reach a QD score milestone.\"\"\"\n",
    "    idx = np.searchsorted(qd_scores, milestone)\n",
    "    \n",
    "    if idx == 0:\n",
    "        return evals[0]\n",
    "    elif idx >= len(qd_scores):\n",
    "        return None  # Milestone not reached\n",
    "    else:\n",
    "        # Linear interpolation between two points\n",
    "        qd_low, qd_high = qd_scores[idx-1], qd_scores[idx]\n",
    "        eval_low, eval_high = evals[idx-1], evals[idx]\n",
    "        ratio = (milestone - qd_low) / (qd_high - qd_low)\n",
    "        return eval_low + ratio * (eval_high - eval_low)\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77ea26",
   "metadata": {},
   "source": [
    "## STEP 3: Parallel Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "73f8e8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions ready!\n"
     ]
    }
   ],
   "source": [
    "def run_single_experiment(config, seed, fixed_params):\n",
    "    \"\"\"Run a single experiment with given config and seed.\"\"\"\n",
    "    exp_name = f\"{config['name']}_seed{seed}\"\n",
    "    \n",
    "    # Setup environment\n",
    "    env, policy_network, reset_fn, init_variables, key = setup_environment(\n",
    "        fixed_params['env_name'],\n",
    "        fixed_params['episode_length'],\n",
    "        fixed_params['policy_hidden_layer_sizes'],\n",
    "        fixed_params['batch_size'],\n",
    "        seed\n",
    "    )\n",
    "    \n",
    "    scoring_fn = create_scoring_function(env, policy_network, reset_fn, \n",
    "                                        fixed_params['episode_length'],\n",
    "                                        fixed_params['env_name'])\n",
    "    \n",
    "    reward_offset = environments.reward_offset[fixed_params['env_name']]\n",
    "    metrics_function = functools.partial(\n",
    "        default_qd_metrics,\n",
    "        qd_offset=reward_offset * fixed_params['episode_length'],\n",
    "    )\n",
    "    \n",
    "    # Create emitter\n",
    "    variation_fn = functools.partial(\n",
    "        isoline_variation,\n",
    "        iso_sigma=fixed_params['iso_sigma'],\n",
    "        line_sigma=fixed_params['line_sigma']\n",
    "    )\n",
    "    \n",
    "    mixing_emitter = MixingEmitter(\n",
    "        mutation_fn=None,\n",
    "        variation_fn=variation_fn,\n",
    "        variation_percentage=1.0,\n",
    "        batch_size=fixed_params['batch_size']\n",
    "    )\n",
    "    \n",
    "    # Create algorithm (DNS or DNS-GA)\n",
    "    if config['type'] == 'baseline':\n",
    "        algorithm = DominatedNoveltySearch(\n",
    "            scoring_function=scoring_fn,\n",
    "            emitter=mixing_emitter,\n",
    "            metrics_function=metrics_function,\n",
    "            population_size=fixed_params['population_size'],\n",
    "            k=fixed_params['k'],\n",
    "        )\n",
    "    else:\n",
    "        mutation_fn = create_mutation_function(fixed_params['iso_sigma'])\n",
    "        algorithm = DominatedNoveltySearchGA(\n",
    "            scoring_function=scoring_fn,\n",
    "            emitter=mixing_emitter,\n",
    "            metrics_function=metrics_function,\n",
    "            population_size=fixed_params['population_size'],\n",
    "            k=fixed_params['k'],\n",
    "            g_n=config['g_n'],\n",
    "            num_ga_children=config['num_ga_children'],\n",
    "            num_ga_generations=config['num_ga_generations'],\n",
    "            mutation_fn=mutation_fn,\n",
    "        )\n",
    "    \n",
    "    # Initialize\n",
    "    key, subkey = jax.random.split(key)\n",
    "    repertoire, emitter_state, init_metrics = algorithm.init(init_variables, subkey)\n",
    "    \n",
    "    # Setup logging\n",
    "    log_period = 100\n",
    "    num_loops = fixed_params['num_iterations'] // log_period\n",
    "    \n",
    "    metrics = {key: jnp.array([]) for key in [\"iteration\", \"qd_score\", \"coverage\", \"max_fitness\", \"time\"]}\n",
    "    init_metrics = jax.tree.map(lambda x: jnp.array([x]) if x.shape == () else x, init_metrics)\n",
    "    init_metrics[\"iteration\"] = jnp.array([0], dtype=jnp.int32)\n",
    "    init_metrics[\"time\"] = jnp.array([0.0])\n",
    "    metrics = jax.tree.map(\n",
    "        lambda metric, init_metric: jnp.concatenate([metric, init_metric], axis=0),\n",
    "        metrics, init_metrics\n",
    "    )\n",
    "    \n",
    "    log_filename = os.path.join(\"seed_variability_logs\", f\"{exp_name}_logs.csv\")\n",
    "    csv_logger = CSVLogger(log_filename, header=list(metrics.keys()))\n",
    "    csv_logger.log(jax.tree.map(lambda x: x[-1], metrics))\n",
    "    \n",
    "    # Main training loop\n",
    "    if config['type'] == 'baseline':\n",
    "        algorithm_scan_update = algorithm.scan_update\n",
    "        scan_state = (repertoire, emitter_state, key)\n",
    "    else:\n",
    "        algorithm_scan_update = algorithm.scan_update\n",
    "        scan_state = (repertoire, emitter_state, key, 1)  # generation_counter\n",
    "    \n",
    "    start_time_total = time.time()\n",
    "    \n",
    "    for i in range(num_loops):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scan_state, current_metrics = jax.lax.scan(\n",
    "            algorithm_scan_update,\n",
    "            scan_state,\n",
    "            (),\n",
    "            length=log_period,\n",
    "        )\n",
    "        \n",
    "        timelapse = time.time() - start_time\n",
    "        \n",
    "        current_metrics[\"iteration\"] = jnp.arange(\n",
    "            1 + log_period * i, 1 + log_period * (i + 1), dtype=jnp.int32\n",
    "        )\n",
    "        current_metrics[\"time\"] = jnp.repeat(timelapse, log_period)\n",
    "        metrics = jax.tree.map(\n",
    "            lambda metric, current_metric: jnp.concatenate([metric, current_metric], axis=0),\n",
    "            metrics, current_metrics\n",
    "        )\n",
    "        \n",
    "        csv_logger.log(jax.tree.map(lambda x: x[-1], metrics))\n",
    "    \n",
    "    total_time = time.time() - start_time_total\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ga_total_evals, ga_num_calls, ga_evals_per_call = calculate_ga_overhead_evals(\n",
    "        config.get('g_n'), fixed_params['num_iterations'], fixed_params['population_size'],\n",
    "        config.get('num_ga_children'), config.get('num_ga_generations')\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'config_name': config['name'],\n",
    "        'config_type': config['type'],\n",
    "        'seed': seed,\n",
    "        'g_n': config.get('g_n'),\n",
    "        'num_ga_generations': config.get('num_ga_generations'),\n",
    "        'final_qd_score': float(metrics['qd_score'][-1]),\n",
    "        'final_max_fitness': float(metrics['max_fitness'][-1]),\n",
    "        'final_coverage': float(metrics['coverage'][-1]),\n",
    "        'total_time': total_time,\n",
    "        'ga_overhead_evals': ga_total_evals,\n",
    "        'log_file': log_filename,\n",
    "    }\n",
    "\n",
    "print(\"Helper functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f11a8",
   "metadata": {},
   "source": [
    "## STEP 4: Build Experiment Queue and Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92f08cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BUILDING EXPERIMENT QUEUE - 20251115_203700\n",
      "================================================================================\n",
      "\n",
      "Experiment Queue Summary:\n",
      "  SANITY CHECK FIRST: 2 experiments (DNS-GA g_n=99999 + baseline, both seed 42)\n",
      "  Main experiments: 3 configs Ã— 31 seeds = 93\n",
      "  Total: 95\n",
      "  Execution: Sequential (Jupyter can't use multiprocessing)\n",
      "  Estimated time: ~5.2 hours (~5.2 hours)\n",
      "================================================================================\n",
      "\n",
      "ðŸ” FIRST 2 EXPERIMENTS (SANITY CHECK):\n",
      "  1. DNS_baseline, seed=42\n",
      "  2. DNS-GA_sanity_no_ga, seed=42\n",
      "  â†’ These should have IDENTICAL final QD scores\n",
      "  â†’ If they differ by >0.5%, stop the run and investigate\n",
      "\n",
      "Next 5 experiments:\n",
      "  3. DNS_baseline, seed=7817\n",
      "  4. DNS_baseline, seed=52731\n",
      "  5. DNS_baseline, seed=51809\n",
      "  6. DNS_baseline, seed=35457\n",
      "  7. DNS_baseline, seed=47644\n",
      "\n",
      "Last 3 experiments:\n",
      "  93. DNS-GA_g1000_gen4, seed=53095\n",
      "  94. DNS-GA_g1000_gen4, seed=51931\n",
      "  95. DNS-GA_g1000_gen4, seed=35573\n",
      "\n",
      "================================================================================\n",
      "Ready to run experiments!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Build experiment queue\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"BUILDING EXPERIMENT QUEUE - {timestamp}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "experiment_queue = []\n",
    "exp_num = 0\n",
    "\n",
    "# SANITY CHECK: Run baseline seed 42 FIRST to establish reference\n",
    "baseline_config = [c for c in MAIN_CONFIGS if c['type'] == 'baseline'][0]\n",
    "exp_num += 1\n",
    "experiment_queue.append((exp_num, exp_num, baseline_config, SANITY_SEED))\n",
    "\n",
    "# Then run DNS-GA with g_n=99999 (should be slightly different but comparable)\n",
    "exp_num += 1\n",
    "experiment_queue.append((exp_num, exp_num, SANITY_CONFIG, SANITY_SEED))\n",
    "\n",
    "# Add main experiments (all configs Ã— all seeds)\n",
    "for config in MAIN_CONFIGS:\n",
    "    for seed in RANDOM_SEEDS:\n",
    "        exp_num += 1\n",
    "        experiment_queue.append((exp_num, exp_num, config, seed))\n",
    "\n",
    "total_experiments = len(experiment_queue)\n",
    "\n",
    "print(f\"\\nExperiment Queue Summary:\")\n",
    "print(f\"  SANITY CHECK FIRST: 2 experiments (DNS-GA g_n=99999 + baseline, both seed 42)\")\n",
    "print(f\"  Main experiments: {len(MAIN_CONFIGS)} configs Ã— {len(RANDOM_SEEDS)} seeds = {len(MAIN_CONFIGS) * len(RANDOM_SEEDS)}\")\n",
    "print(f\"  Total: {total_experiments}\")\n",
    "print(f\"  Execution: Sequential (Jupyter can't use multiprocessing)\")\n",
    "print(f\"  Estimated time: ~{total_experiments * 3.3 / 60:.1f} hours (~5.2 hours)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ” FIRST 2 EXPERIMENTS (SANITY CHECK):\")\n",
    "for i in range(min(2, len(experiment_queue))):\n",
    "    exp_num, _, config, seed = experiment_queue[i]\n",
    "    print(f\"  {exp_num}. {config['name']}, seed={seed}\")\n",
    "print(f\"  â†’ These should have IDENTICAL final QD scores\")\n",
    "print(f\"  â†’ If they differ by >0.5%, stop the run and investigate\")\n",
    "\n",
    "print(f\"\\nNext 5 experiments:\")\n",
    "for i in range(2, min(7, len(experiment_queue))):\n",
    "    exp_num, _, config, seed = experiment_queue[i]\n",
    "    print(f\"  {exp_num}. {config['name']}, seed={seed}\")\n",
    "\n",
    "print(\"\\nLast 3 experiments:\")\n",
    "for i in range(max(0, len(experiment_queue) - 3), len(experiment_queue)):\n",
    "    exp_num, _, config, seed = experiment_queue[i]\n",
    "    print(f\"  {exp_num}. {config['name']}, seed={seed}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Ready to run experiments!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db024292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING ALL EXPERIMENTS WITH ipyparallel\n",
      "================================================================================\n",
      "Start time: 2025-11-15 20:37:00\n",
      "Total experiments: 95\n",
      "Setting up parallel cluster with 2 engines...\n",
      "================================================================================\n",
      "Starting 2 engines with <class 'ipyparallel.cluster.launcher.LocalEngineSetLauncher'>\n",
      "Starting 2 engines with <class 'ipyparallel.cluster.launcher.LocalEngineSetLauncher'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe01756687d942a88f85fa539dbeab61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?engine/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Cluster started with 2 engines\n",
      "  Estimated time: ~2.6 hours\n",
      "âœ“ Engines initialized with functions and parameters\n",
      "Submitting all experiments to cluster...\n",
      "âœ“ Submitted 95 experiments\n",
      "\n",
      "Monitoring progress (updates every 10 seconds)...\n",
      "Note: Each experiment takes ~3-4 minutes\n",
      "âœ“ Engines initialized with functions and parameters\n",
      "Submitting all experiments to cluster...\n",
      "âœ“ Submitted 95 experiments\n",
      "\n",
      "Monitoring progress (updates every 10 seconds)...\n",
      "Note: Each experiment takes ~3-4 minutes\n",
      "ðŸ“Š Waiting for first experiment to complete... (19s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (19s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (29s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (29s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (39s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (39s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (49s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (49s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (59s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (59s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (69s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (69s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (79s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (79s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (89s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (89s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (99s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (99s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (109s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (109s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (119s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (119s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (129s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (129s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (139s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (139s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (149s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (149s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (159s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (159s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (169s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (169s elapsed)\n",
      "Output for 1:\n",
      "2025-11-15 20:29:19.041 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_4\n",
      "2025-11-15 20:29:19.045 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_6\n",
      "2025-11-15 20:29:19.046 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_8\n",
      "2025-11-15 20:29:19.048 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_10\n",
      "2025-11-15 20:29:19.049 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_12\n",
      "2025-11-15 20:29:19.049 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_14\n",
      "2025-11-15 20:29:19.051 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_16\n",
      "2025-11-15 20:29:19.052 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_18\n",
      "2025-11-15 20:29:19.053 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_20\n",
      "2025-11-15 20:29:19.055 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_22\n",
      "2025-11-15 20:37:01.889 [IPEngine.1] Exception in apply request:\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/576920416.py:59\u001b[39m, in \u001b[36mrun_experiment_wrapper\u001b[39m\u001b[34m(exp_tuple)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/645012194.py:113\u001b[39m, in \u001b[36mrun_single_experiment\u001b[39m\u001b[34m(config, seed, fixed_params)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/qdax/utils/metrics.py:49\u001b[39m, in \u001b[36mCSVLogger.log\u001b[39m\u001b[34m(self, metrics)\u001b[39m\n",
      "\u001b[32m     47\u001b[39m writer = csv.DictWriter(file, fieldnames=\u001b[38;5;28mself\u001b[39m._header)\n",
      "\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# write new metrics in a raw\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/csv.py:164\u001b[39m, in \u001b[36mDictWriter.writerow\u001b[39m\u001b[34m(self, rowdict)\u001b[39m\n",
      "\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwriterow\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowdict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:288\u001b[39m, in \u001b[36mArrayImpl.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    356\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n",
      "\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n",
      "\u001b[32m    358\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:635\u001b[39m, in \u001b[36mArrayImpl._value\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    633\u001b[39m   \u001b[38;5;66;03m# addressable_device_list can be empty. If it's empty, we will error below\u001b[39;00m\n",
      "\u001b[32m    634\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fully_replicated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sharding.has_addressable_devices:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     npy_value, did_copy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_device_array_to_np_array_did_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    636\u001b[39m     npy_value.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m did_copy:\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: \n",
      "2025-11-15 20:37:04.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (1 time(s) in a row).\n",
      "2025-11-15 20:37:08.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (2 time(s) in a row).\n",
      "2025-11-15 20:37:11.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (3 time(s) in a row).\n",
      "2025-11-15 20:37:15.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (4 time(s) in a row).\n",
      "2025-11-15 20:37:18.619 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (5 time(s) in a row).\n",
      "2025-11-15 20:37:22.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (6 time(s) in a row).\n",
      "2025-11-15 20:37:25.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (7 time(s) in a row).\n",
      "2025-11-15 20:37:29.119 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (8 time(s) in a row).\n",
      "2025-11-15 20:37:32.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (9 time(s) in a row).\n",
      "2025-11-15 20:37:36.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (10 time(s) in a row).\n",
      "2025-11-15 20:37:39.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (11 time(s) in a row).\n",
      "2025-11-15 20:37:43.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (12 time(s) in a row).\n",
      "2025-11-15 20:37:46.618 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (13 time(s) in a row).\n",
      "2025-11-15 20:37:50.119 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (14 time(s) in a row).\n",
      "2025-11-15 20:37:53.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (15 time(s) in a row).\n",
      "2025-11-15 20:37:57.116 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (16 time(s) in a row).\n",
      "2025-11-15 20:38:00.616 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (17 time(s) in a row).\n",
      "2025-11-15 20:38:04.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (18 time(s) in a row).\n",
      "2025-11-15 20:38:07.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (19 time(s) in a row).\n",
      "2025-11-15 20:38:11.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (20 time(s) in a row).\n",
      "2025-11-15 20:38:14.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (21 time(s) in a row).\n",
      "2025-11-15 20:38:18.118 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (22 time(s) in a row).\n",
      "2025-11-15 20:38:21.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (23 time(s) in a row).\n",
      "2025-11-15 20:38:25.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (24 time(s) in a row).\n",
      "2025-11-15 20:38:28.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (25 time(s) in a row).\n",
      "2025-11-15 20:38:32.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (26 time(s) in a row).\n",
      "2025-11-15 20:38:35.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (27 time(s) in a row).\n",
      "2025-11-15 20:38:39.118 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (28 time(s) in a row).\n",
      "2025-11-15 20:38:42.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (29 time(s) in a row).\n",
      "2025-11-15 20:38:46.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (30 time(s) in a row).\n",
      "2025-11-15 20:38:49.616 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (31 time(s) in a row).\n",
      "2025-11-15 20:38:53.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (32 time(s) in a row).\n",
      "2025-11-15 20:38:56.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (33 time(s) in a row).\n",
      "2025-11-15 20:39:00.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (34 time(s) in a row).\n",
      "2025-11-15 20:39:03.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (35 time(s) in a row).\n",
      "2025-11-15 20:39:07.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (36 time(s) in a row).\n",
      "2025-11-15 20:39:10.618 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (37 time(s) in a row).\n",
      "2025-11-15 20:39:14.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (38 time(s) in a row).\n",
      "2025-11-15 20:39:17.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (39 time(s) in a row).\n",
      "2025-11-15 20:39:21.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (40 time(s) in a row).\n",
      "2025-11-15 20:39:24.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (41 time(s) in a row).\n",
      "2025-11-15 20:39:28.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (42 time(s) in a row).\n",
      "2025-11-15 20:39:31.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (43 time(s) in a row).\n",
      "2025-11-15 20:39:35.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (44 time(s) in a row).\n",
      "2025-11-15 20:39:38.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (45 time(s) in a row).\n",
      "2025-11-15 20:39:42.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (46 time(s) in a row).\n",
      "2025-11-15 20:39:45.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (47 time(s) in a row).\n",
      "2025-11-15 20:39:49.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (48 time(s) in a row).\n",
      "2025-11-15 20:39:52.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (49 time(s) in a row).\n",
      "2025-11-15 20:39:56.116 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (50 time(s) in a row).\n",
      "2025-11-15 20:39:56.116 [IPEngine.1] CRITICAL | Maximum number of heartbeats misses reached (50 times 3500 ms), shutting down.\n",
      "2025-11-15 20:39:56.975 [KernelNanny.1] Pipe closed, parent 163 has status: zombie\n",
      "2025-11-15 20:39:56.978 [KernelNanny.1] Notifying Hub that our parent has shut down\n",
      "\n",
      "Output for 1:\n",
      "2025-11-15 20:29:19.041 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_4\n",
      "2025-11-15 20:29:19.045 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_6\n",
      "2025-11-15 20:29:19.046 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_8\n",
      "2025-11-15 20:29:19.048 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_10\n",
      "2025-11-15 20:29:19.049 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_12\n",
      "2025-11-15 20:29:19.049 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_14\n",
      "2025-11-15 20:29:19.051 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_16\n",
      "2025-11-15 20:29:19.052 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_18\n",
      "2025-11-15 20:29:19.053 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_20\n",
      "2025-11-15 20:29:19.055 [IPEngine.1] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_22\n",
      "2025-11-15 20:37:01.889 [IPEngine.1] Exception in apply request:\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/576920416.py:59\u001b[39m, in \u001b[36mrun_experiment_wrapper\u001b[39m\u001b[34m(exp_tuple)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/645012194.py:113\u001b[39m, in \u001b[36mrun_single_experiment\u001b[39m\u001b[34m(config, seed, fixed_params)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/qdax/utils/metrics.py:49\u001b[39m, in \u001b[36mCSVLogger.log\u001b[39m\u001b[34m(self, metrics)\u001b[39m\n",
      "\u001b[32m     47\u001b[39m writer = csv.DictWriter(file, fieldnames=\u001b[38;5;28mself\u001b[39m._header)\n",
      "\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# write new metrics in a raw\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/csv.py:164\u001b[39m, in \u001b[36mDictWriter.writerow\u001b[39m\u001b[34m(self, rowdict)\u001b[39m\n",
      "\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwriterow\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowdict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:288\u001b[39m, in \u001b[36mArrayImpl.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    356\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n",
      "\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n",
      "\u001b[32m    358\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:635\u001b[39m, in \u001b[36mArrayImpl._value\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    633\u001b[39m   \u001b[38;5;66;03m# addressable_device_list can be empty. If it's empty, we will error below\u001b[39;00m\n",
      "\u001b[32m    634\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fully_replicated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sharding.has_addressable_devices:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     npy_value, did_copy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_device_array_to_np_array_did_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    636\u001b[39m     npy_value.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m did_copy:\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: \n",
      "2025-11-15 20:37:04.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (1 time(s) in a row).\n",
      "2025-11-15 20:37:08.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (2 time(s) in a row).\n",
      "2025-11-15 20:37:11.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (3 time(s) in a row).\n",
      "2025-11-15 20:37:15.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (4 time(s) in a row).\n",
      "2025-11-15 20:37:18.619 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (5 time(s) in a row).\n",
      "2025-11-15 20:37:22.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (6 time(s) in a row).\n",
      "2025-11-15 20:37:25.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (7 time(s) in a row).\n",
      "2025-11-15 20:37:29.119 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (8 time(s) in a row).\n",
      "2025-11-15 20:37:32.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (9 time(s) in a row).\n",
      "2025-11-15 20:37:36.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (10 time(s) in a row).\n",
      "2025-11-15 20:37:39.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (11 time(s) in a row).\n",
      "2025-11-15 20:37:43.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (12 time(s) in a row).\n",
      "2025-11-15 20:37:46.618 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (13 time(s) in a row).\n",
      "2025-11-15 20:37:50.119 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (14 time(s) in a row).\n",
      "2025-11-15 20:37:53.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (15 time(s) in a row).\n",
      "2025-11-15 20:37:57.116 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (16 time(s) in a row).\n",
      "2025-11-15 20:38:00.616 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (17 time(s) in a row).\n",
      "2025-11-15 20:38:04.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (18 time(s) in a row).\n",
      "2025-11-15 20:38:07.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (19 time(s) in a row).\n",
      "2025-11-15 20:38:11.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (20 time(s) in a row).\n",
      "2025-11-15 20:38:14.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (21 time(s) in a row).\n",
      "2025-11-15 20:38:18.118 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (22 time(s) in a row).\n",
      "2025-11-15 20:38:21.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (23 time(s) in a row).\n",
      "2025-11-15 20:38:25.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (24 time(s) in a row).\n",
      "2025-11-15 20:38:28.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (25 time(s) in a row).\n",
      "2025-11-15 20:38:32.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (26 time(s) in a row).\n",
      "2025-11-15 20:38:35.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (27 time(s) in a row).\n",
      "2025-11-15 20:38:39.118 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (28 time(s) in a row).\n",
      "2025-11-15 20:38:42.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (29 time(s) in a row).\n",
      "2025-11-15 20:38:46.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (30 time(s) in a row).\n",
      "2025-11-15 20:38:49.616 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (31 time(s) in a row).\n",
      "2025-11-15 20:38:53.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (32 time(s) in a row).\n",
      "2025-11-15 20:38:56.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (33 time(s) in a row).\n",
      "2025-11-15 20:39:00.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (34 time(s) in a row).\n",
      "2025-11-15 20:39:03.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (35 time(s) in a row).\n",
      "2025-11-15 20:39:07.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (36 time(s) in a row).\n",
      "2025-11-15 20:39:10.618 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (37 time(s) in a row).\n",
      "2025-11-15 20:39:14.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (38 time(s) in a row).\n",
      "2025-11-15 20:39:17.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (39 time(s) in a row).\n",
      "2025-11-15 20:39:21.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (40 time(s) in a row).\n",
      "2025-11-15 20:39:24.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (41 time(s) in a row).\n",
      "2025-11-15 20:39:28.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (42 time(s) in a row).\n",
      "2025-11-15 20:39:31.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (43 time(s) in a row).\n",
      "2025-11-15 20:39:35.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (44 time(s) in a row).\n",
      "2025-11-15 20:39:38.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (45 time(s) in a row).\n",
      "2025-11-15 20:39:42.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (46 time(s) in a row).\n",
      "2025-11-15 20:39:45.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (47 time(s) in a row).\n",
      "2025-11-15 20:39:49.117 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (48 time(s) in a row).\n",
      "2025-11-15 20:39:52.617 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (49 time(s) in a row).\n",
      "2025-11-15 20:39:56.116 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (50 time(s) in a row).\n",
      "2025-11-15 20:39:56.116 [IPEngine.1] CRITICAL | Maximum number of heartbeats misses reached (50 times 3500 ms), shutting down.\n",
      "2025-11-15 20:39:56.975 [KernelNanny.1] Pipe closed, parent 163 has status: zombie\n",
      "2025-11-15 20:39:56.978 [KernelNanny.1] Notifying Hub that our parent has shut down\n",
      "\n",
      "ðŸ“Š Waiting for first experiment to complete... (179s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (179s elapsed)\n",
      "Output for 0:\n",
      "2025-11-15 20:29:12.045 [IPEngine.0] Completed registration with id 0\n",
      "2025-11-15 20:29:17.434 [IPEngine.0] Handling execute_request: b31093b3-9b4190362a6590ba459dc554_94312_1\n",
      "2025-11-15 20:29:19.041 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_3\n",
      "2025-11-15 20:29:19.045 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_5\n",
      "2025-11-15 20:29:19.046 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_7\n",
      "2025-11-15 20:29:19.048 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_9\n",
      "2025-11-15 20:29:19.049 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_11\n",
      "2025-11-15 20:29:19.050 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_13\n",
      "2025-11-15 20:29:19.051 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_15\n",
      "2025-11-15 20:29:19.052 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_17\n",
      "2025-11-15 20:29:19.053 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_21\n",
      "2025-11-15 20:37:06.872 [IPEngine.0] Exception in apply request:\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/576920416.py:59\u001b[39m, in \u001b[36mrun_experiment_wrapper\u001b[39m\u001b[34m(exp_tuple)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/645012194.py:113\u001b[39m, in \u001b[36mrun_single_experiment\u001b[39m\u001b[34m(config, seed, fixed_params)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/qdax/utils/metrics.py:49\u001b[39m, in \u001b[36mCSVLogger.log\u001b[39m\u001b[34m(self, metrics)\u001b[39m\n",
      "\u001b[32m     47\u001b[39m writer = csv.DictWriter(file, fieldnames=\u001b[38;5;28mself\u001b[39m._header)\n",
      "\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# write new metrics in a raw\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/csv.py:164\u001b[39m, in \u001b[36mDictWriter.writerow\u001b[39m\u001b[34m(self, rowdict)\u001b[39m\n",
      "\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwriterow\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowdict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:288\u001b[39m, in \u001b[36mArrayImpl.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    356\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n",
      "\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n",
      "\u001b[32m    358\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:635\u001b[39m, in \u001b[36mArrayImpl._value\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    633\u001b[39m   \u001b[38;5;66;03m# addressable_device_list can be empty. If it's empty, we will error below\u001b[39;00m\n",
      "\u001b[32m    634\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fully_replicated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sharding.has_addressable_devices:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     npy_value, did_copy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_device_array_to_np_array_did_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    636\u001b[39m     npy_value.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m did_copy:\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: \n",
      "2025-11-15 20:37:06.879 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_19\n",
      "2025-11-15 20:37:08.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (1 time(s) in a row).\n",
      "2025-11-15 20:37:11.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (2 time(s) in a row).\n",
      "2025-11-15 20:37:15.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (3 time(s) in a row).\n",
      "2025-11-15 20:37:18.548 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (4 time(s) in a row).\n",
      "2025-11-15 20:37:22.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (5 time(s) in a row).\n",
      "2025-11-15 20:37:25.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (6 time(s) in a row).\n",
      "2025-11-15 20:37:29.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (7 time(s) in a row).\n",
      "2025-11-15 20:37:32.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (8 time(s) in a row).\n",
      "2025-11-15 20:37:36.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (9 time(s) in a row).\n",
      "2025-11-15 20:37:39.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (10 time(s) in a row).\n",
      "2025-11-15 20:37:43.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (11 time(s) in a row).\n",
      "2025-11-15 20:37:46.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (12 time(s) in a row).\n",
      "2025-11-15 20:37:50.090 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (13 time(s) in a row).\n",
      "2025-11-15 20:37:53.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (14 time(s) in a row).\n",
      "2025-11-15 20:37:57.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (15 time(s) in a row).\n",
      "2025-11-15 20:38:00.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (16 time(s) in a row).\n",
      "2025-11-15 20:38:04.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (17 time(s) in a row).\n",
      "2025-11-15 20:38:07.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (18 time(s) in a row).\n",
      "2025-11-15 20:38:11.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (19 time(s) in a row).\n",
      "2025-11-15 20:38:14.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (20 time(s) in a row).\n",
      "2025-11-15 20:38:18.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (21 time(s) in a row).\n",
      "2025-11-15 20:38:21.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (22 time(s) in a row).\n",
      "2025-11-15 20:38:25.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (23 time(s) in a row).\n",
      "2025-11-15 20:38:28.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (24 time(s) in a row).\n",
      "2025-11-15 20:38:32.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (25 time(s) in a row).\n",
      "2025-11-15 20:38:35.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (26 time(s) in a row).\n",
      "2025-11-15 20:38:39.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (27 time(s) in a row).\n",
      "2025-11-15 20:38:42.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (28 time(s) in a row).\n",
      "2025-11-15 20:38:46.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (29 time(s) in a row).\n",
      "2025-11-15 20:38:49.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (30 time(s) in a row).\n",
      "2025-11-15 20:38:53.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (31 time(s) in a row).\n",
      "2025-11-15 20:38:56.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (32 time(s) in a row).\n",
      "2025-11-15 20:39:00.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (33 time(s) in a row).\n",
      "2025-11-15 20:39:03.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (34 time(s) in a row).\n",
      "2025-11-15 20:39:07.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (35 time(s) in a row).\n",
      "2025-11-15 20:39:10.551 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (36 time(s) in a row).\n",
      "2025-11-15 20:39:14.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (37 time(s) in a row).\n",
      "2025-11-15 20:39:17.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (38 time(s) in a row).\n",
      "2025-11-15 20:39:21.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (39 time(s) in a row).\n",
      "2025-11-15 20:39:24.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (40 time(s) in a row).\n",
      "2025-11-15 20:39:28.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (41 time(s) in a row).\n",
      "2025-11-15 20:39:31.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (42 time(s) in a row).\n",
      "2025-11-15 20:39:35.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (43 time(s) in a row).\n",
      "2025-11-15 20:39:38.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (44 time(s) in a row).\n",
      "2025-11-15 20:39:42.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (45 time(s) in a row).\n",
      "2025-11-15 20:39:45.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (46 time(s) in a row).\n",
      "2025-11-15 20:39:49.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (47 time(s) in a row).\n",
      "2025-11-15 20:39:52.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (48 time(s) in a row).\n",
      "2025-11-15 20:39:56.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (49 time(s) in a row).\n",
      "2025-11-15 20:39:59.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (50 time(s) in a row).\n",
      "2025-11-15 20:39:59.549 [IPEngine.0] CRITICAL | Maximum number of heartbeats misses reached (50 times 3500 ms), shutting down.\n",
      "\n",
      "engine set stopped 1763260151: {'engines': {'1': {'exit_code': <Negsignal.SIGINT: -2>, 'pid': 163, 'identifier': '1'}, '0': {'exit_code': 1, 'pid': 162, 'identifier': '0'}}, 'exit_code': <Negsignal.SIGINT: -2>}\n",
      "Output for 0:\n",
      "2025-11-15 20:29:12.045 [IPEngine.0] Completed registration with id 0\n",
      "2025-11-15 20:29:17.434 [IPEngine.0] Handling execute_request: b31093b3-9b4190362a6590ba459dc554_94312_1\n",
      "2025-11-15 20:29:19.041 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_3\n",
      "2025-11-15 20:29:19.045 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_5\n",
      "2025-11-15 20:29:19.046 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_7\n",
      "2025-11-15 20:29:19.048 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_9\n",
      "2025-11-15 20:29:19.049 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_11\n",
      "2025-11-15 20:29:19.050 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_13\n",
      "2025-11-15 20:29:19.051 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_15\n",
      "2025-11-15 20:29:19.052 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_17\n",
      "2025-11-15 20:29:19.053 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_21\n",
      "2025-11-15 20:37:06.872 [IPEngine.0] Exception in apply request:\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/576920416.py:59\u001b[39m, in \u001b[36mrun_experiment_wrapper\u001b[39m\u001b[34m(exp_tuple)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/645012194.py:113\u001b[39m, in \u001b[36mrun_single_experiment\u001b[39m\u001b[34m(config, seed, fixed_params)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/qdax/utils/metrics.py:49\u001b[39m, in \u001b[36mCSVLogger.log\u001b[39m\u001b[34m(self, metrics)\u001b[39m\n",
      "\u001b[32m     47\u001b[39m writer = csv.DictWriter(file, fieldnames=\u001b[38;5;28mself\u001b[39m._header)\n",
      "\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# write new metrics in a raw\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/csv.py:164\u001b[39m, in \u001b[36mDictWriter.writerow\u001b[39m\u001b[34m(self, rowdict)\u001b[39m\n",
      "\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwriterow\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowdict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:288\u001b[39m, in \u001b[36mArrayImpl.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    356\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n",
      "\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n",
      "\u001b[32m    358\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:635\u001b[39m, in \u001b[36mArrayImpl._value\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    633\u001b[39m   \u001b[38;5;66;03m# addressable_device_list can be empty. If it's empty, we will error below\u001b[39;00m\n",
      "\u001b[32m    634\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fully_replicated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sharding.has_addressable_devices:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     npy_value, did_copy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_device_array_to_np_array_did_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    636\u001b[39m     npy_value.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m did_copy:\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: \n",
      "2025-11-15 20:37:06.879 [IPEngine.0] Handling apply_request: b31093b3-9b4190362a6590ba459dc554_94312_19\n",
      "2025-11-15 20:37:08.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (1 time(s) in a row).\n",
      "2025-11-15 20:37:11.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (2 time(s) in a row).\n",
      "2025-11-15 20:37:15.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (3 time(s) in a row).\n",
      "2025-11-15 20:37:18.548 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (4 time(s) in a row).\n",
      "2025-11-15 20:37:22.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (5 time(s) in a row).\n",
      "2025-11-15 20:37:25.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (6 time(s) in a row).\n",
      "2025-11-15 20:37:29.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (7 time(s) in a row).\n",
      "2025-11-15 20:37:32.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (8 time(s) in a row).\n",
      "2025-11-15 20:37:36.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (9 time(s) in a row).\n",
      "2025-11-15 20:37:39.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (10 time(s) in a row).\n",
      "2025-11-15 20:37:43.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (11 time(s) in a row).\n",
      "2025-11-15 20:37:46.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (12 time(s) in a row).\n",
      "2025-11-15 20:37:50.090 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (13 time(s) in a row).\n",
      "2025-11-15 20:37:53.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (14 time(s) in a row).\n",
      "2025-11-15 20:37:57.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (15 time(s) in a row).\n",
      "2025-11-15 20:38:00.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (16 time(s) in a row).\n",
      "2025-11-15 20:38:04.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (17 time(s) in a row).\n",
      "2025-11-15 20:38:07.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (18 time(s) in a row).\n",
      "2025-11-15 20:38:11.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (19 time(s) in a row).\n",
      "2025-11-15 20:38:14.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (20 time(s) in a row).\n",
      "2025-11-15 20:38:18.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (21 time(s) in a row).\n",
      "2025-11-15 20:38:21.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (22 time(s) in a row).\n",
      "2025-11-15 20:38:25.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (23 time(s) in a row).\n",
      "2025-11-15 20:38:28.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (24 time(s) in a row).\n",
      "2025-11-15 20:38:32.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (25 time(s) in a row).\n",
      "2025-11-15 20:38:35.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (26 time(s) in a row).\n",
      "2025-11-15 20:38:39.047 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (27 time(s) in a row).\n",
      "2025-11-15 20:38:42.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (28 time(s) in a row).\n",
      "2025-11-15 20:38:46.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (29 time(s) in a row).\n",
      "2025-11-15 20:38:49.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (30 time(s) in a row).\n",
      "2025-11-15 20:38:53.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (31 time(s) in a row).\n",
      "2025-11-15 20:38:56.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (32 time(s) in a row).\n",
      "2025-11-15 20:39:00.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (33 time(s) in a row).\n",
      "2025-11-15 20:39:03.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (34 time(s) in a row).\n",
      "2025-11-15 20:39:07.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (35 time(s) in a row).\n",
      "2025-11-15 20:39:10.551 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (36 time(s) in a row).\n",
      "2025-11-15 20:39:14.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (37 time(s) in a row).\n",
      "2025-11-15 20:39:17.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (38 time(s) in a row).\n",
      "2025-11-15 20:39:21.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (39 time(s) in a row).\n",
      "2025-11-15 20:39:24.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (40 time(s) in a row).\n",
      "2025-11-15 20:39:28.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (41 time(s) in a row).\n",
      "2025-11-15 20:39:31.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (42 time(s) in a row).\n",
      "2025-11-15 20:39:35.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (43 time(s) in a row).\n",
      "2025-11-15 20:39:38.547 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (44 time(s) in a row).\n",
      "2025-11-15 20:39:42.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (45 time(s) in a row).\n",
      "2025-11-15 20:39:45.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (46 time(s) in a row).\n",
      "2025-11-15 20:39:49.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (47 time(s) in a row).\n",
      "2025-11-15 20:39:52.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (48 time(s) in a row).\n",
      "2025-11-15 20:39:56.046 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (49 time(s) in a row).\n",
      "2025-11-15 20:39:59.546 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (50 time(s) in a row).\n",
      "2025-11-15 20:39:59.549 [IPEngine.0] CRITICAL | Maximum number of heartbeats misses reached (50 times 3500 ms), shutting down.\n",
      "\n",
      "engine set stopped 1763260151: {'engines': {'1': {'exit_code': <Negsignal.SIGINT: -2>, 'pid': 163, 'identifier': '1'}, '0': {'exit_code': 1, 'pid': 162, 'identifier': '0'}}, 'exit_code': <Negsignal.SIGINT: -2>}\n",
      "ðŸ“Š Waiting for first experiment to complete... (189s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (189s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (199s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (199s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (209s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (209s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (219s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (219s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (229s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (229s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (239s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (239s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (249s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (249s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (259s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (259s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (269s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (269s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (279s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (279s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (289s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (289s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (299s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (299s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (309s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (309s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (319s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (319s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (329s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (329s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (339s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (339s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (349s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (349s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (359s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (359s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (369s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (369s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (379s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (379s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (389s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (389s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (399s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (399s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (409s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (409s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (419s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (419s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (429s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (429s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (439s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (439s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (449s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (449s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (459s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (459s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (469s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (469s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (479s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (479s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (489s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (489s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (499s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (499s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (510s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (510s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (520s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (520s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (530s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (530s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (540s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (540s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (550s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (550s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (560s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (560s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (570s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (570s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (580s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (580s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (590s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (590s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (600s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (600s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (610s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (610s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (620s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (620s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (630s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (630s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (640s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (640s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (650s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (650s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (660s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (660s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (670s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (670s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (680s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (680s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (690s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (690s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (700s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (700s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (710s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (710s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (720s elapsed)\n",
      "ðŸ“Š Waiting for first experiment to complete... (720s elapsed)\n",
      "  âœ“ Completed: DNS_baseline, seed=42, QD=306862.8\n",
      "  âœ“ Completed: DNS_baseline, seed=42, QD=306862.8\n",
      "ðŸ“Š Progress: 1/95 (1.1%) | Elapsed: 12.2m | Remaining: ~19.06h\n",
      "ðŸ“Š Progress: 1/95 (1.1%) | Elapsed: 12.2m | Remaining: ~19.06h\n",
      "ðŸ“Š Progress: 1/95 (1.1%) | Elapsed: 12.3m | Remaining: ~19.32h\n",
      "ðŸ“Š Progress: 1/95 (1.1%) | Elapsed: 12.3m | Remaining: ~19.32h\n",
      "  âœ“ Completed: DNS-GA_sanity_no_ga, seed=42, QD=325715.7\n",
      "  âœ“ Completed: DNS-GA_sanity_no_ga, seed=42, QD=325715.7\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 12.5m | Remaining: ~9.69h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 12.5m | Remaining: ~9.69h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 12.7m | Remaining: ~9.82h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 12.7m | Remaining: ~9.82h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 12.8m | Remaining: ~9.95h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 12.8m | Remaining: ~9.95h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 13.0m | Remaining: ~10.08h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 13.0m | Remaining: ~10.08h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 13.2m | Remaining: ~10.20h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 13.2m | Remaining: ~10.20h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 13.3m | Remaining: ~10.33h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 13.3m | Remaining: ~10.33h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 13.5m | Remaining: ~10.46h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 13.5m | Remaining: ~10.46h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 13.7m | Remaining: ~10.59h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 13.7m | Remaining: ~10.59h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 13.8m | Remaining: ~10.72h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 13.8m | Remaining: ~10.72h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 14.0m | Remaining: ~10.85h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 14.0m | Remaining: ~10.85h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 14.2m | Remaining: ~10.98h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 14.2m | Remaining: ~10.98h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 14.3m | Remaining: ~11.11h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 14.3m | Remaining: ~11.11h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 14.5m | Remaining: ~11.24h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 14.5m | Remaining: ~11.24h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 14.7m | Remaining: ~11.37h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 14.7m | Remaining: ~11.37h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 14.8m | Remaining: ~11.50h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 14.8m | Remaining: ~11.50h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 15.0m | Remaining: ~11.63h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 15.0m | Remaining: ~11.63h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 15.2m | Remaining: ~11.76h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 15.2m | Remaining: ~11.76h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 15.3m | Remaining: ~11.89h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 15.3m | Remaining: ~11.89h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 15.5m | Remaining: ~12.02h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 15.5m | Remaining: ~12.02h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 15.7m | Remaining: ~12.15h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 15.7m | Remaining: ~12.15h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 15.8m | Remaining: ~12.28h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 15.8m | Remaining: ~12.28h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 16.0m | Remaining: ~12.40h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 16.0m | Remaining: ~12.40h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 16.2m | Remaining: ~12.53h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 16.2m | Remaining: ~12.53h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 16.3m | Remaining: ~12.66h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 16.3m | Remaining: ~12.66h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 16.5m | Remaining: ~12.79h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 16.5m | Remaining: ~12.79h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 16.7m | Remaining: ~12.92h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 16.7m | Remaining: ~12.92h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 16.8m | Remaining: ~13.05h\n",
      "ðŸ“Š Progress: 2/95 (2.1%) | Elapsed: 16.8m | Remaining: ~13.05h\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 118\u001b[39m\n\u001b[32m    115\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“Š Waiting for first experiment to complete... (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms elapsed)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    116\u001b[39m         last_update = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Check every 2 seconds\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[32m    121\u001b[39m cluster.stop_cluster_sync()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controller stopped: {'exit_code': 0, 'pid': 516, 'identifier': 'ipcontroller-1763260620-dk82-94312'}\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Run All Experiments in Parallel with ipyparallel\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING ALL EXPERIMENTS WITH ipyparallel\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total experiments: {len(experiment_queue)}\")\n",
    "print(f\"Setting up parallel cluster with 2 engines...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time_all = time.time()\n",
    "\n",
    "import ipyparallel as ipp\n",
    "\n",
    "# Start a local cluster with 2 engines\n",
    "cluster = ipp.Cluster(n=2)\n",
    "rc = cluster.start_and_connect_sync()\n",
    "\n",
    "print(f\"âœ“ Cluster started with {len(rc)} engines\")\n",
    "print(f\"  Estimated time: ~{len(experiment_queue) / 2 * 3.3 / 60:.1f} hours\")\n",
    "\n",
    "# Push necessary imports to engines using execute (more reliable than sync_imports)\n",
    "rc[:].execute(\"\"\"\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import functools\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from qdax.core.dns_ga import DominatedNoveltySearchGA\n",
    "from qdax.core.dns import DominatedNoveltySearch\n",
    "import qdax.tasks.brax as environments\n",
    "from qdax.tasks.brax.env_creators import scoring_function_brax_envs as scoring_function\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.core.emitters.standard_emitters import MixingEmitter\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "\"\"\").wait()\n",
    "\n",
    "# Push FIXED_PARAMS and helper functions to engines using .push()\n",
    "rc[:].push({\n",
    "    'FIXED_PARAMS': FIXED_PARAMS,\n",
    "    'setup_environment': setup_environment,\n",
    "    'create_scoring_function': create_scoring_function,\n",
    "    'create_mutation_function': create_mutation_function,\n",
    "    'calculate_ga_overhead_evals': calculate_ga_overhead_evals,\n",
    "    'calculate_cumulative_evals_for_log': calculate_cumulative_evals_for_log,\n",
    "    'interpolate_evals_to_milestone': interpolate_evals_to_milestone,\n",
    "    'run_single_experiment': run_single_experiment\n",
    "}).wait()\n",
    "\n",
    "print(\"âœ“ Engines initialized with functions and parameters\")\n",
    "\n",
    "# Create load-balanced view\n",
    "lview = rc.load_balanced_view()\n",
    "\n",
    "# Define wrapper function for ipyparallel\n",
    "def run_experiment_wrapper(exp_tuple):\n",
    "    exp_num, total_exp, config, seed = exp_tuple\n",
    "    try:\n",
    "        result = run_single_experiment(config, seed, FIXED_PARAMS)\n",
    "        result['exp_num'] = exp_num\n",
    "        return ('success', result)\n",
    "    except Exception as e:\n",
    "        return ('error', {'config_name': config['name'], 'seed': seed, 'error': str(e)})\n",
    "\n",
    "# Push wrapper to engines using .push()\n",
    "rc[:].push({'run_experiment_wrapper': run_experiment_wrapper}).wait()\n",
    "\n",
    "# Submit ALL experiments (including sanity check)\n",
    "print(\"Submitting all experiments to cluster...\")\n",
    "async_results = []\n",
    "for exp_tuple in experiment_queue:\n",
    "    ar = lview.apply_async(run_experiment_wrapper, exp_tuple)\n",
    "    async_results.append(ar)\n",
    "\n",
    "print(f\"âœ“ Submitted {len(async_results)} experiments\")\n",
    "print(\"\\nMonitoring progress (updates every 10 seconds)...\")\n",
    "print(\"Note: Each experiment takes ~3-4 minutes\")\n",
    "\n",
    "# Monitor progress with non-blocking checks\n",
    "all_results = []\n",
    "errors = []\n",
    "completed_count = 0\n",
    "last_update = time.time()\n",
    "\n",
    "while completed_count < len(async_results):\n",
    "    # Check all results non-blocking\n",
    "    for idx, ar in enumerate(async_results):\n",
    "        if ar.ready() and not hasattr(ar, '_collected'):\n",
    "            ar._collected = True  # Mark as collected\n",
    "            status, result = ar.result()\n",
    "            \n",
    "            if status == 'success':\n",
    "                all_results.append(result)\n",
    "                config_name = result['config_name']\n",
    "                seed = result['seed']\n",
    "                qd = result['final_qd_score']\n",
    "                print(f\"  âœ“ Completed: {config_name}, seed={seed}, QD={qd:.1f}\")\n",
    "            else:\n",
    "                errors.append(result)\n",
    "                print(f\"  âœ— Failed: {result['config_name']}, seed={result['seed']}\")\n",
    "            \n",
    "            completed_count += 1\n",
    "    \n",
    "    # Progress update every 10 seconds\n",
    "    if time.time() - last_update > 10:\n",
    "        elapsed = time.time() - start_time_all\n",
    "        pct = completed_count / len(experiment_queue) * 100\n",
    "        if completed_count > 0:\n",
    "            avg_time = elapsed / completed_count\n",
    "            remaining_time = (len(experiment_queue) - completed_count) * avg_time / 3600\n",
    "            print(f\"ðŸ“Š Progress: {completed_count}/{len(experiment_queue)} ({pct:.1f}%) | Elapsed: {elapsed/60:.1f}m | Remaining: ~{remaining_time:.2f}h\")\n",
    "        else:\n",
    "            print(f\"ðŸ“Š Waiting for first experiment to complete... ({elapsed:.0f}s elapsed)\")\n",
    "        last_update = time.time()\n",
    "    \n",
    "    time.sleep(2)  # Check every 2 seconds\n",
    "\n",
    "# Cleanup\n",
    "cluster.stop_cluster_sync()\n",
    "print(\"\\nâœ“ Cluster stopped\")\n",
    "\n",
    "total_time = time.time() - start_time_all\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total time: {total_time / 60:.1f} minutes ({total_time / 3600:.2f} hours)\")\n",
    "print(f\"Successful experiments: {len(all_results)}\")\n",
    "print(f\"Failed experiments: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nErrors encountered:\")\n",
    "    for error in errors:\n",
    "        print(f\"  â€¢ {error['config_name']}, seed={error['seed']}: {error['error']}\")\n",
    "\n",
    "# Save all results\n",
    "results_file = f\"seed_variability_logs/all_results_{timestamp}.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'results': all_results,\n",
    "        'errors': errors,\n",
    "        'total_time': total_time,\n",
    "        'timestamp': timestamp,\n",
    "        'num_seeds': len(RANDOM_SEEDS),\n",
    "        'seeds': RANDOM_SEEDS,\n",
    "    }, f, indent=2)\n",
    "\n",
    "\n",
    "print(f\"\\nResults saved to: {results_file}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c7473",
   "metadata": {},
   "source": [
    "## STEP 4 (ALTERNATIVE): Sequential Execution\n",
    "\n",
    "**Use this instead of parallel execution to ensure reproducibility.**\n",
    "\n",
    "The parallel version has 6.14% variation due to JAX JIT/RNG differences between engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da91876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 SEQUENTIAL: Run All Experiments Sequentially\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING ALL EXPERIMENTS SEQUENTIALLY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total experiments: {len(experiment_queue)}\")\n",
    "print(f\"Estimated time: ~{len(experiment_queue) * 3.3 / 60:.1f} hours (~5.2 hours)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time_all = time.time()\n",
    "\n",
    "all_results = []\n",
    "errors = []\n",
    "\n",
    "for exp_num, (_, _, config, seed) in enumerate(experiment_queue, 1):\n",
    "    exp_start = time.time()\n",
    "    config_name = config['name']\n",
    "    \n",
    "    print(f\"\\n[{exp_num}/{len(experiment_queue)}] Starting: {config_name}, seed={seed}\")\n",
    "    \n",
    "    try:\n",
    "        result = run_single_experiment(config, seed, FIXED_PARAMS)\n",
    "        result['exp_num'] = exp_num\n",
    "        all_results.append(result)\n",
    "        \n",
    "        exp_time = time.time() - exp_start\n",
    "        qd = result['final_qd_score']\n",
    "        print(f\"  âœ“ Completed in {exp_time/60:.1f}m: QD={qd:,.1f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors.append({'config_name': config_name, 'seed': seed, 'error': str(e)})\n",
    "        print(f\"  âœ— Failed: {str(e)}\")\n",
    "    \n",
    "    # Progress update every 5 experiments\n",
    "    if exp_num % 5 == 0:\n",
    "        elapsed = time.time() - start_time_all\n",
    "        avg_time = elapsed / exp_num\n",
    "        remaining = (len(experiment_queue) - exp_num) * avg_time\n",
    "        print(f\"\\nðŸ“Š Progress: {exp_num}/{len(experiment_queue)} ({exp_num/len(experiment_queue)*100:.1f}%) | \"\n",
    "              f\"Elapsed: {elapsed/3600:.1f}h | Remaining: ~{remaining/3600:.1f}h\")\n",
    "\n",
    "total_time = time.time() - start_time_all\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXECUTION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total time: {total_time/3600:.2f} hours\")\n",
    "print(f\"Successful: {len(all_results)}/{len(experiment_queue)}\")\n",
    "print(f\"Failed: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nErrors encountered:\")\n",
    "    for error in errors:\n",
    "        print(f\"  â€¢ {error['config_name']}, seed={error['seed']}: {error['error']}\")\n",
    "\n",
    "# Save results to JSON\n",
    "results_file = \"seed_variability_results.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({'results': all_results, 'errors': errors}, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_file}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d750c70",
   "metadata": {},
   "source": [
    "## STEP 4B: Detailed Error Diagnostics\n",
    "\n",
    "If experiments failed, run this cell to see full error details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cbcc9e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ No errors - all experiments completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Print detailed error information\n",
    "if errors:\n",
    "    print(\"=\"*80)\n",
    "    print(\"DETAILED ERROR REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    for i, error in enumerate(errors, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Error {i}/{len(errors)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Config: {error['config_name']}\")\n",
    "        print(f\"Seed: {error['seed']}\")\n",
    "        print(f\"\\nError message:\")\n",
    "        print(error['error'])\n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "    # Try to get traceback from engine logs if available\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"CHECKING ENGINE LOGS FOR MORE DETAILS\")\n",
    "    print(\"=\"*80)\n",
    "    try:\n",
    "        # Get engine stdout/stderr\n",
    "        for engine_id in rc.ids:\n",
    "            print(f\"\\nEngine {engine_id} stdout:\")\n",
    "            stdout = rc[engine_id].apply_sync(lambda: None)\n",
    "            print(\"(Check Colab logs or engine output files)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not retrieve engine logs: {e}\")\n",
    "else:\n",
    "    print(\"âœ“ No errors - all experiments completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7ddee",
   "metadata": {},
   "source": [
    "## STEP 5: Convergence Efficiency Analysis\n",
    "\n",
    "**Goal**: Calculate how many evaluations each DNS-GA config needs to reach baseline's convergence score\n",
    "\n",
    "**Key Metric**: Evaluations-to-convergence (not performance at same iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7df77b",
   "metadata": {},
   "source": [
    "## STEP 4C: Sanity Check Validation\n",
    "\n",
    "**CRITICAL**: Check if baseline and g_n=99999 differ by more than 2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24437db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SANITY CHECK: Baseline vs g_n=99999 (both seed 42)\n",
      "================================================================================\n",
      "\n",
      "DNS_baseline              : QD = 306,862.8\n",
      "DNS-GA_sanity_no_ga       : QD = 325,715.7\n",
      "\n",
      "Absolute difference: 18,853.0\n",
      "Percentage difference: 6.14%\n",
      "\n",
      "âœ— FAIL: Expected â‰¤2%, got 6.14%\n",
      "\n",
      "================================================================================\n",
      "âš ï¸  WARNING: SANITY CHECK FAILED\n",
      "================================================================================\n",
      "\n",
      "Possible causes:\n",
      "  1. Parallel engines have different JAX random states\n",
      "  2. JIT compilation timing differences\n",
      "  3. Bug in DNS-GA g_n=99999 implementation\n",
      "  4. Engine state contamination\n",
      "\n",
      "RECOMMENDATION:\n",
      "  â€¢ Stop experiments and investigate\n",
      "  â€¢ May need to run experiments sequentially instead of parallel\n",
      "  â€¢ Or accept higher stochastic variation (but results less reliable)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for 0:\n",
      "2025-11-15 20:37:02.413 [KernelNanny.0] Nanny watching parent pid 536.\n",
      "2025-11-15 20:37:02.481 [IPEngine.0] Loading IPython extension: storemagic\n",
      "2025-11-15 20:37:02.482 [IPEngine.0] WARNING | debugpy_stream undefined, debugging will not be enabled\n",
      "2025-11-15 20:37:02.483 [IPEngine.0] Starting to monitor the heartbeat signal from the hub every 3500 ms.\n",
      "2025-11-15 20:37:02.483 [IPEngine.0] Completed registration with id 0\n",
      "2025-11-15 20:37:07.913 [IPEngine.0] Handling execute_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_1\n",
      "2025-11-15 20:37:09.448 [IPEngine.0] Handling apply_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_3\n",
      "2025-11-15 20:37:09.451 [IPEngine.0] Handling apply_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_5\n",
      "2025-11-15 20:37:09.464 [IPEngine.0] Handling apply_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_7\n",
      "2025-11-15 20:49:00.965 [IPEngine.0] Handling apply_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_9\n",
      "2025-11-15 20:54:04.299 [IPEngine.0] Exception in apply request:\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/169890922.py:61\u001b[39m, in \u001b[36mrun_experiment_wrapper\u001b[39m\u001b[34m(exp_tuple)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/645012194.py:113\u001b[39m, in \u001b[36mrun_single_experiment\u001b[39m\u001b[34m(config, seed, fixed_params)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/qdax/utils/metrics.py:49\u001b[39m, in \u001b[36mCSVLogger.log\u001b[39m\u001b[34m(self, metrics)\u001b[39m\n",
      "\u001b[32m     47\u001b[39m writer = csv.DictWriter(file, fieldnames=\u001b[38;5;28mself\u001b[39m._header)\n",
      "\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# write new metrics in a raw\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/csv.py:164\u001b[39m, in \u001b[36mDictWriter.writerow\u001b[39m\u001b[34m(self, rowdict)\u001b[39m\n",
      "\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwriterow\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowdict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:288\u001b[39m, in \u001b[36mArrayImpl.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    356\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n",
      "\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n",
      "\u001b[32m    358\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:635\u001b[39m, in \u001b[36mArrayImpl._value\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    633\u001b[39m   \u001b[38;5;66;03m# addressable_device_list can be empty. If it's empty, we will error below\u001b[39;00m\n",
      "\u001b[32m    634\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fully_replicated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sharding.has_addressable_devices:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     npy_value, did_copy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_device_array_to_np_array_did_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    636\u001b[39m     npy_value.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m did_copy:\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: \n",
      "2025-11-15 20:54:04.484 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (1 time(s) in a row).\n",
      "2025-11-15 20:54:07.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (2 time(s) in a row).\n",
      "2025-11-15 20:54:11.485 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (3 time(s) in a row).\n",
      "2025-11-15 20:54:14.984 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (4 time(s) in a row).\n",
      "2025-11-15 20:54:18.484 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (5 time(s) in a row).\n",
      "2025-11-15 20:54:21.984 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (6 time(s) in a row).\n",
      "2025-11-15 20:54:25.484 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (7 time(s) in a row).\n",
      "2025-11-15 20:54:28.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (8 time(s) in a row).\n",
      "2025-11-15 20:54:32.485 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (9 time(s) in a row).\n",
      "2025-11-15 20:54:35.984 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (10 time(s) in a row).\n",
      "2025-11-15 20:54:39.485 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (11 time(s) in a row).\n",
      "2025-11-15 20:54:42.984 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (12 time(s) in a row).\n",
      "2025-11-15 20:54:46.485 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (13 time(s) in a row).\n",
      "2025-11-15 20:54:49.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (14 time(s) in a row).\n",
      "2025-11-15 20:54:53.484 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (15 time(s) in a row).\n",
      "2025-11-15 20:54:56.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (16 time(s) in a row).\n",
      "2025-11-15 20:55:00.485 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (17 time(s) in a row).\n",
      "2025-11-15 20:55:03.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (18 time(s) in a row).\n",
      "2025-11-15 20:55:07.485 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (19 time(s) in a row).\n",
      "2025-11-15 20:55:10.984 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (20 time(s) in a row).\n",
      "2025-11-15 20:55:14.485 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (21 time(s) in a row).\n",
      "2025-11-15 20:55:17.984 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (22 time(s) in a row).\n",
      "2025-11-15 20:55:21.484 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (23 time(s) in a row).\n",
      "2025-11-15 20:55:24.984 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (24 time(s) in a row).\n",
      "2025-11-15 20:55:28.484 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (25 time(s) in a row).\n",
      "2025-11-15 20:55:31.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (26 time(s) in a row).\n",
      "2025-11-15 20:55:35.484 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (27 time(s) in a row).\n",
      "2025-11-15 20:55:38.984 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (28 time(s) in a row).\n",
      "2025-11-15 20:55:42.485 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (29 time(s) in a row).\n",
      "2025-11-15 20:55:45.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (30 time(s) in a row).\n",
      "2025-11-15 20:55:49.484 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (31 time(s) in a row).\n",
      "2025-11-15 20:55:52.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (32 time(s) in a row).\n",
      "2025-11-15 20:55:56.485 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (33 time(s) in a row).\n",
      "2025-11-15 20:55:59.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (34 time(s) in a row).\n",
      "2025-11-15 20:56:03.484 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (35 time(s) in a row).\n",
      "2025-11-15 20:56:06.984 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (36 time(s) in a row).\n",
      "2025-11-15 20:56:10.485 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (37 time(s) in a row).\n",
      "2025-11-15 20:56:13.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (38 time(s) in a row).\n",
      "2025-11-15 20:56:17.485 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (39 time(s) in a row).\n",
      "2025-11-15 20:56:20.984 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (40 time(s) in a row).\n",
      "2025-11-15 20:56:24.484 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (41 time(s) in a row).\n",
      "2025-11-15 20:56:27.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (42 time(s) in a row).\n",
      "2025-11-15 20:56:31.484 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (43 time(s) in a row).\n",
      "2025-11-15 20:56:34.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (44 time(s) in a row).\n",
      "2025-11-15 20:56:38.484 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (45 time(s) in a row).\n",
      "2025-11-15 20:56:41.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (46 time(s) in a row).\n",
      "2025-11-15 20:56:45.484 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (47 time(s) in a row).\n",
      "2025-11-15 20:56:48.984 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (48 time(s) in a row).\n",
      "2025-11-15 20:56:52.485 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (49 time(s) in a row).\n",
      "2025-11-15 20:56:55.985 [IPEngine.0] WARNING | No heartbeat in the last 3500 ms (50 time(s) in a row).\n",
      "2025-11-15 20:56:55.985 [IPEngine.0] CRITICAL | Maximum number of heartbeats misses reached (50 times 3500 ms), shutting down.\n",
      "2025-11-15 20:56:56.936 [KernelNanny.0] Pipe closed, parent 536 has status: zombie\n",
      "2025-11-15 20:56:56.937 [KernelNanny.0] Notifying Hub that our parent has shut down\n",
      "\n",
      "Output for 1:\n",
      "2025-11-15 20:37:02.527 [KernelNanny.1] Nanny watching parent pid 537.\n",
      "2025-11-15 20:37:02.565 [IPEngine.1] Loading IPython extension: storemagic\n",
      "2025-11-15 20:37:02.566 [IPEngine.1] WARNING | debugpy_stream undefined, debugging will not be enabled\n",
      "2025-11-15 20:37:02.567 [IPEngine.1] Starting to monitor the heartbeat signal from the hub every 3500 ms.\n",
      "2025-11-15 20:37:02.567 [IPEngine.1] Completed registration with id 1\n",
      "2025-11-15 20:37:07.913 [IPEngine.1] Handling execute_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_2\n",
      "2025-11-15 20:37:09.448 [IPEngine.1] Handling apply_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_4\n",
      "2025-11-15 20:37:09.451 [IPEngine.1] Handling apply_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_6\n",
      "2025-11-15 20:37:09.465 [IPEngine.1] Handling apply_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_8\n",
      "2025-11-15 20:49:26.364 [IPEngine.1] Handling apply_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_10\n",
      "2025-11-15 20:54:04.400 [IPEngine.1] Exception in apply request:\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/169890922.py:61\u001b[39m, in \u001b[36mrun_experiment_wrapper\u001b[39m\u001b[34m(exp_tuple)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/645012194.py:113\u001b[39m, in \u001b[36mrun_single_experiment\u001b[39m\u001b[34m(config, seed, fixed_params)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/qdax/utils/metrics.py:49\u001b[39m, in \u001b[36mCSVLogger.log\u001b[39m\u001b[34m(self, metrics)\u001b[39m\n",
      "\u001b[32m     47\u001b[39m writer = csv.DictWriter(file, fieldnames=\u001b[38;5;28mself\u001b[39m._header)\n",
      "\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# write new metrics in a raw\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/csv.py:164\u001b[39m, in \u001b[36mDictWriter.writerow\u001b[39m\u001b[34m(self, rowdict)\u001b[39m\n",
      "\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwriterow\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowdict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:288\u001b[39m, in \u001b[36mArrayImpl.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    356\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n",
      "\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n",
      "\u001b[32m    358\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:635\u001b[39m, in \u001b[36mArrayImpl._value\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    633\u001b[39m   \u001b[38;5;66;03m# addressable_device_list can be empty. If it's empty, we will error below\u001b[39;00m\n",
      "\u001b[32m    634\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fully_replicated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sharding.has_addressable_devices:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     npy_value, did_copy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_device_array_to_np_array_did_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    636\u001b[39m     npy_value.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m did_copy:\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: \n",
      "2025-11-15 20:54:04.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (1 time(s) in a row).\n",
      "2025-11-15 20:54:08.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (2 time(s) in a row).\n",
      "2025-11-15 20:54:11.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (3 time(s) in a row).\n",
      "2025-11-15 20:54:15.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (4 time(s) in a row).\n",
      "2025-11-15 20:54:18.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (5 time(s) in a row).\n",
      "2025-11-15 20:54:22.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (6 time(s) in a row).\n",
      "2025-11-15 20:54:25.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (7 time(s) in a row).\n",
      "2025-11-15 20:54:29.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (8 time(s) in a row).\n",
      "2025-11-15 20:54:32.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (9 time(s) in a row).\n",
      "2025-11-15 20:54:36.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (10 time(s) in a row).\n",
      "2025-11-15 20:54:39.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (11 time(s) in a row).\n",
      "2025-11-15 20:54:43.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (12 time(s) in a row).\n",
      "2025-11-15 20:54:46.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (13 time(s) in a row).\n",
      "2025-11-15 20:54:50.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (14 time(s) in a row).\n",
      "2025-11-15 20:54:53.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (15 time(s) in a row).\n",
      "2025-11-15 20:54:57.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (16 time(s) in a row).\n",
      "2025-11-15 20:55:00.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (17 time(s) in a row).\n",
      "2025-11-15 20:55:04.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (18 time(s) in a row).\n",
      "2025-11-15 20:55:07.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (19 time(s) in a row).\n",
      "2025-11-15 20:55:11.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (20 time(s) in a row).\n",
      "2025-11-15 20:55:14.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (21 time(s) in a row).\n",
      "2025-11-15 20:55:18.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (22 time(s) in a row).\n",
      "2025-11-15 20:55:21.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (23 time(s) in a row).\n",
      "2025-11-15 20:55:25.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (24 time(s) in a row).\n",
      "2025-11-15 20:55:28.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (25 time(s) in a row).\n",
      "2025-11-15 20:55:32.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (26 time(s) in a row).\n",
      "2025-11-15 20:55:35.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (27 time(s) in a row).\n",
      "2025-11-15 20:55:39.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (28 time(s) in a row).\n",
      "2025-11-15 20:55:42.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (29 time(s) in a row).\n",
      "2025-11-15 20:55:46.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (30 time(s) in a row).\n",
      "2025-11-15 20:55:49.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (31 time(s) in a row).\n",
      "2025-11-15 20:55:53.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (32 time(s) in a row).\n",
      "2025-11-15 20:55:56.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (33 time(s) in a row).\n",
      "2025-11-15 20:56:00.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (34 time(s) in a row).\n",
      "2025-11-15 20:56:03.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (35 time(s) in a row).\n",
      "2025-11-15 20:56:07.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (36 time(s) in a row).\n",
      "2025-11-15 20:56:10.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (37 time(s) in a row).\n",
      "2025-11-15 20:56:14.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (38 time(s) in a row).\n",
      "2025-11-15 20:56:17.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (39 time(s) in a row).\n",
      "2025-11-15 20:56:21.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (40 time(s) in a row).\n",
      "2025-11-15 20:56:24.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (41 time(s) in a row).\n",
      "2025-11-15 20:56:28.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (42 time(s) in a row).\n",
      "2025-11-15 20:56:31.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (43 time(s) in a row).\n",
      "2025-11-15 20:56:35.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (44 time(s) in a row).\n",
      "2025-11-15 20:56:38.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (45 time(s) in a row).\n",
      "2025-11-15 20:56:42.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (46 time(s) in a row).\n",
      "2025-11-15 20:56:45.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (47 time(s) in a row).\n",
      "2025-11-15 20:56:49.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (48 time(s) in a row).\n",
      "2025-11-15 20:56:52.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (49 time(s) in a row).\n",
      "2025-11-15 20:56:56.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (50 time(s) in a row).\n",
      "2025-11-15 20:56:56.068 [IPEngine.1] CRITICAL | Maximum number of heartbeats misses reached (50 times 3500 ms), shutting down.\n",
      "2025-11-15 20:56:57.070 [KernelNanny.1] Pipe closed, parent 537 has status: zombie\n",
      "2025-11-15 20:56:57.070 [KernelNanny.1] Notifying Hub that our parent has shut down\n",
      "\n",
      "Output for 1:\n",
      "2025-11-15 20:37:02.527 [KernelNanny.1] Nanny watching parent pid 537.\n",
      "2025-11-15 20:37:02.565 [IPEngine.1] Loading IPython extension: storemagic\n",
      "2025-11-15 20:37:02.566 [IPEngine.1] WARNING | debugpy_stream undefined, debugging will not be enabled\n",
      "2025-11-15 20:37:02.567 [IPEngine.1] Starting to monitor the heartbeat signal from the hub every 3500 ms.\n",
      "2025-11-15 20:37:02.567 [IPEngine.1] Completed registration with id 1\n",
      "2025-11-15 20:37:07.913 [IPEngine.1] Handling execute_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_2\n",
      "2025-11-15 20:37:09.448 [IPEngine.1] Handling apply_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_4\n",
      "2025-11-15 20:37:09.451 [IPEngine.1] Handling apply_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_6\n",
      "2025-11-15 20:37:09.465 [IPEngine.1] Handling apply_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_8\n",
      "2025-11-15 20:49:26.364 [IPEngine.1] Handling apply_request: f7230f5b-c235ae0dfe883bf46c82aaed_94312_10\n",
      "2025-11-15 20:54:04.400 [IPEngine.1] Exception in apply request:\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/169890922.py:61\u001b[39m, in \u001b[36mrun_experiment_wrapper\u001b[39m\u001b[34m(exp_tuple)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/mv/xknc10sn41z5twhgvk97qsqw0000gn/T/ipykernel_94312/645012194.py:113\u001b[39m, in \u001b[36mrun_single_experiment\u001b[39m\u001b[34m(config, seed, fixed_params)\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/qdax/utils/metrics.py:49\u001b[39m, in \u001b[36mCSVLogger.log\u001b[39m\u001b[34m(self, metrics)\u001b[39m\n",
      "\u001b[32m     47\u001b[39m writer = csv.DictWriter(file, fieldnames=\u001b[38;5;28mself\u001b[39m._header)\n",
      "\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# write new metrics in a raw\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/csv.py:164\u001b[39m, in \u001b[36mDictWriter.writerow\u001b[39m\u001b[34m(self, rowdict)\u001b[39m\n",
      "\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwriterow\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowdict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:288\u001b[39m, in \u001b[36mArrayImpl.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    356\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n",
      "\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n",
      "\u001b[32m    358\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/source/EvoAlgsAndSwarm/lib-qdax/QDax/.venv/lib/python3.12/site-packages/jax/_src/array.py:635\u001b[39m, in \u001b[36mArrayImpl._value\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    633\u001b[39m   \u001b[38;5;66;03m# addressable_device_list can be empty. If it's empty, we will error below\u001b[39;00m\n",
      "\u001b[32m    634\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fully_replicated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sharding.has_addressable_devices:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     npy_value, did_copy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_device_array_to_np_array_did_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    636\u001b[39m     npy_value.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m did_copy:\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: \n",
      "2025-11-15 20:54:04.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (1 time(s) in a row).\n",
      "2025-11-15 20:54:08.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (2 time(s) in a row).\n",
      "2025-11-15 20:54:11.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (3 time(s) in a row).\n",
      "2025-11-15 20:54:15.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (4 time(s) in a row).\n",
      "2025-11-15 20:54:18.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (5 time(s) in a row).\n",
      "2025-11-15 20:54:22.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (6 time(s) in a row).\n",
      "2025-11-15 20:54:25.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (7 time(s) in a row).\n",
      "2025-11-15 20:54:29.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (8 time(s) in a row).\n",
      "2025-11-15 20:54:32.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (9 time(s) in a row).\n",
      "2025-11-15 20:54:36.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (10 time(s) in a row).\n",
      "2025-11-15 20:54:39.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (11 time(s) in a row).\n",
      "2025-11-15 20:54:43.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (12 time(s) in a row).\n",
      "2025-11-15 20:54:46.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (13 time(s) in a row).\n",
      "2025-11-15 20:54:50.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (14 time(s) in a row).\n",
      "2025-11-15 20:54:53.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (15 time(s) in a row).\n",
      "2025-11-15 20:54:57.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (16 time(s) in a row).\n",
      "2025-11-15 20:55:00.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (17 time(s) in a row).\n",
      "2025-11-15 20:55:04.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (18 time(s) in a row).\n",
      "2025-11-15 20:55:07.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (19 time(s) in a row).\n",
      "2025-11-15 20:55:11.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (20 time(s) in a row).\n",
      "2025-11-15 20:55:14.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (21 time(s) in a row).\n",
      "2025-11-15 20:55:18.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (22 time(s) in a row).\n",
      "2025-11-15 20:55:21.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (23 time(s) in a row).\n",
      "2025-11-15 20:55:25.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (24 time(s) in a row).\n",
      "2025-11-15 20:55:28.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (25 time(s) in a row).\n",
      "2025-11-15 20:55:32.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (26 time(s) in a row).\n",
      "2025-11-15 20:55:35.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (27 time(s) in a row).\n",
      "2025-11-15 20:55:39.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (28 time(s) in a row).\n",
      "2025-11-15 20:55:42.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (29 time(s) in a row).\n",
      "2025-11-15 20:55:46.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (30 time(s) in a row).\n",
      "2025-11-15 20:55:49.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (31 time(s) in a row).\n",
      "2025-11-15 20:55:53.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (32 time(s) in a row).\n",
      "2025-11-15 20:55:56.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (33 time(s) in a row).\n",
      "2025-11-15 20:56:00.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (34 time(s) in a row).\n",
      "2025-11-15 20:56:03.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (35 time(s) in a row).\n",
      "2025-11-15 20:56:07.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (36 time(s) in a row).\n",
      "2025-11-15 20:56:10.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (37 time(s) in a row).\n",
      "2025-11-15 20:56:14.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (38 time(s) in a row).\n",
      "2025-11-15 20:56:17.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (39 time(s) in a row).\n",
      "2025-11-15 20:56:21.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (40 time(s) in a row).\n",
      "2025-11-15 20:56:24.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (41 time(s) in a row).\n",
      "2025-11-15 20:56:28.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (42 time(s) in a row).\n",
      "2025-11-15 20:56:31.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (43 time(s) in a row).\n",
      "2025-11-15 20:56:35.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (44 time(s) in a row).\n",
      "2025-11-15 20:56:38.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (45 time(s) in a row).\n",
      "2025-11-15 20:56:42.069 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (46 time(s) in a row).\n",
      "2025-11-15 20:56:45.569 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (47 time(s) in a row).\n",
      "2025-11-15 20:56:49.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (48 time(s) in a row).\n",
      "2025-11-15 20:56:52.568 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (49 time(s) in a row).\n",
      "2025-11-15 20:56:56.068 [IPEngine.1] WARNING | No heartbeat in the last 3500 ms (50 time(s) in a row).\n",
      "2025-11-15 20:56:56.068 [IPEngine.1] CRITICAL | Maximum number of heartbeats misses reached (50 times 3500 ms), shutting down.\n",
      "2025-11-15 20:56:57.070 [KernelNanny.1] Pipe closed, parent 537 has status: zombie\n",
      "2025-11-15 20:56:57.070 [KernelNanny.1] Notifying Hub that our parent has shut down\n",
      "\n",
      "engine set stopped 1763260621: {'engines': {'0': {'exit_code': <Negsignal.SIGINT: -2>, 'pid': 536, 'identifier': '0'}, '1': {'exit_code': <Negsignal.SIGINT: -2>, 'pid': 537, 'identifier': '1'}}, 'exit_code': <Negsignal.SIGINT: -2>}\n",
      "engine set stopped 1763260621: {'engines': {'0': {'exit_code': <Negsignal.SIGINT: -2>, 'pid': 536, 'identifier': '0'}, '1': {'exit_code': <Negsignal.SIGINT: -2>, 'pid': 537, 'identifier': '1'}}, 'exit_code': <Negsignal.SIGINT: -2>}\n"
     ]
    }
   ],
   "source": [
    "# Read final QD scores from log files\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "log_dir = \"seed_variability_logs\"\n",
    "baseline_log = os.path.join(log_dir, \"DNS_baseline_seed42_logs.csv\")\n",
    "sanity_log = os.path.join(log_dir, \"DNS-GA_sanity_no_ga_seed42_logs.csv\")\n",
    "\n",
    "if os.path.exists(baseline_log) and os.path.exists(sanity_log):\n",
    "    # Read final QD scores (last row of each log)\n",
    "    df_baseline = pd.read_csv(baseline_log)\n",
    "    df_sanity = pd.read_csv(sanity_log)\n",
    "    \n",
    "    qd_baseline = df_baseline['qd_score'].iloc[-1]\n",
    "    qd_sanity = df_sanity['qd_score'].iloc[-1]\n",
    "    \n",
    "    diff_abs = abs(qd_baseline - qd_sanity)\n",
    "    diff_pct = diff_abs / qd_baseline * 100\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SANITY CHECK: Baseline vs g_n=99999 (both seed 42)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDNS_baseline              : QD = {qd_baseline:,.1f}\")\n",
    "    print(f\"DNS-GA_sanity_no_ga       : QD = {qd_sanity:,.1f}\")\n",
    "    print(f\"\\nAbsolute difference: {diff_abs:,.1f}\")\n",
    "    print(f\"Percentage difference: {diff_pct:.2f}%\")\n",
    "    print(f\"\\n{'âœ“ PASS' if diff_pct <= 2.0 else 'âœ— FAIL'}: Expected â‰¤2%, got {diff_pct:.2f}%\")\n",
    "    \n",
    "    if diff_pct > 2.0:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"âš ï¸  WARNING: SANITY CHECK FAILED\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"  1. Parallel engines have different JAX random states\")\n",
    "        print(\"  2. JIT compilation timing differences\")\n",
    "        print(\"  3. Bug in DNS-GA g_n=99999 implementation\")\n",
    "        print(\"  4. Engine state contamination\")\n",
    "        print(\"\\nRECOMMENDATION:\")\n",
    "        print(\"  â€¢ Stop experiments and investigate\")\n",
    "        print(\"  â€¢ May need to run experiments sequentially instead of parallel\")\n",
    "        print(\"  â€¢ Or accept higher stochastic variation (but results less reliable)\")\n",
    "        print(\"=\"*80)\n",
    "else:\n",
    "    print(f\"â³ Waiting for log files:\")\n",
    "    print(f\"  {baseline_log}: {'âœ“' if os.path.exists(baseline_log) else 'âœ—'}\")\n",
    "    print(f\"  {sanity_log}: {'âœ“' if os.path.exists(sanity_log) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e5815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results into DataFrame\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal experiments: {len(df)}\")\n",
    "print(f\"Configurations: {df['config_name'].unique()}\")\n",
    "print(f\"Seeds per config: {df.groupby('config_name')['seed'].count().to_dict()}\")\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.groupby('config_name')['final_qd_score'].agg(['mean', 'std', 'min', 'max']).round(2))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6041f",
   "metadata": {},
   "source": [
    "### Sanity Check: Verify g_n=99999 Matches Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SANITY CHECK: DNS-GA with g_n=99999 should match baseline\")\n",
    "print(\"=\"*80)\n",
    "print(\"This validates that DNS-GA without GA triggers = baseline DNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sanity_result = df[df['config_name'] == 'DNS-GA_sanity_no_ga']\n",
    "baseline_seed42 = df[(df['config_name'] == 'DNS_baseline') & (df['seed'] == 42)]\n",
    "\n",
    "if len(sanity_result) > 0 and len(baseline_seed42) > 0:\n",
    "    sanity_qd = sanity_result['final_qd_score'].values[0]\n",
    "    baseline_qd = baseline_seed42['final_qd_score'].values[0]\n",
    "    diff = abs(sanity_qd - baseline_qd)\n",
    "    pct_diff = (diff / baseline_qd) * 100\n",
    "    \n",
    "    print(f\"\\nSeed 42 comparison:\")\n",
    "    print(f\"  Baseline (DNS):        {baseline_qd:.2f}\")\n",
    "    print(f\"  DNS-GA (g_n=99999):    {sanity_qd:.2f}\")\n",
    "    print(f\"  Absolute difference:   {diff:.2f}\")\n",
    "    print(f\"  Percentage difference: {pct_diff:.3f}%\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ SANITY CHECK LOGIC:\")\n",
    "    print(f\"   DNS-GA now uses: (gen > 0) AND (gen % g_n == 0)\")\n",
    "    print(f\"   With g_n=99999 and 3000 iterations, GA never triggers\")\n",
    "    print(f\"   Both use same DominatedNoveltyRepertoire.add() logic\")\n",
    "    \n",
    "    if pct_diff < 0.5:\n",
    "        print(f\"\\nâœ… SANITY CHECK PASSED: Difference < 0.5%\")\n",
    "        print(f\"   DNS-GA without GA calls behaves identically to baseline\")\n",
    "        print(f\"   Implementation is correct - safe to trust main results\")\n",
    "    elif pct_diff < 2.0:\n",
    "        print(f\"\\nâš ï¸  SANITY CHECK: Small difference ({pct_diff:.3f}%)\")\n",
    "        print(f\"   Likely due to JAX random key differences or numerical precision\")\n",
    "        print(f\"   Acceptable for stochastic algorithm\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ SANITY CHECK FAILED: Difference = {pct_diff:.3f}%\")\n",
    "        print(f\"   Expected < 2% for stochastic algorithms\")\n",
    "        print(f\"   âš ï¸  WARNING: May indicate implementation bug!\")\n",
    "        print(f\"   Review results carefully before drawing conclusions\")\n",
    "else:\n",
    "    print(\"\\nâœ— SANITY CHECK DATA MISSING\")\n",
    "    print(\"  Cannot validate implementation correctness\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f07a8",
   "metadata": {},
   "source": [
    "### Calculate Convergence Efficiency for Each Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91518b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CONVERGENCE EFFICIENCY CALCULATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFor each seed, calculate:\")\n",
    "print(\"  1. Baseline's final convergence score (QD @ 3000 iters)\")\n",
    "print(\"  2. Evaluations needed for DNS-GA to reach that score\")\n",
    "print(\"  3. Evaluation savings = Baseline evals - DNS-GA evals\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store convergence efficiency results\n",
    "convergence_results = []\n",
    "\n",
    "# Get unique configs (excluding sanity check)\n",
    "dns_ga_configs = [c for c in MAIN_CONFIGS if c['type'] == 'dns-ga']\n",
    "\n",
    "for seed in RANDOM_SEEDS:\n",
    "    # Get baseline convergence score for this seed\n",
    "    baseline_row = df[(df['config_name'] == 'DNS_baseline') & (df['seed'] == seed)]\n",
    "    \n",
    "    if len(baseline_row) == 0:\n",
    "        print(f\"Warning: No baseline data for seed {seed}\")\n",
    "        continue\n",
    "    \n",
    "    baseline_convergence_qd = baseline_row['final_qd_score'].values[0]\n",
    "    baseline_convergence_evals = FIXED_PARAMS['num_iterations'] * FIXED_PARAMS['batch_size']\n",
    "    baseline_log_file = baseline_row['log_file'].values[0]\n",
    "    \n",
    "    # Load baseline trajectory\n",
    "    if not os.path.exists(baseline_log_file):\n",
    "        print(f\"Warning: Log file missing for baseline seed {seed}\")\n",
    "        continue\n",
    "    \n",
    "    baseline_log_df = pd.read_csv(baseline_log_file)\n",
    "    \n",
    "    # For each DNS-GA config, find when it reaches baseline's convergence\n",
    "    for config in dns_ga_configs:\n",
    "        ga_row = df[(df['config_name'] == config['name']) & (df['seed'] == seed)]\n",
    "        \n",
    "        if len(ga_row) == 0:\n",
    "            continue\n",
    "        \n",
    "        ga_log_file = ga_row['log_file'].values[0]\n",
    "        \n",
    "        if not os.path.exists(ga_log_file):\n",
    "            continue\n",
    "        \n",
    "        ga_log_df = pd.read_csv(ga_log_file)\n",
    "        \n",
    "        # Calculate cumulative evaluations\n",
    "        ga_evals = calculate_cumulative_evals_for_log(config, ga_log_df, \n",
    "                                                      FIXED_PARAMS['batch_size'],\n",
    "                                                      FIXED_PARAMS['population_size'])\n",
    "        \n",
    "        # Find when DNS-GA reaches baseline's convergence score\n",
    "        evals_to_convergence = interpolate_evals_to_milestone(\n",
    "            ga_log_df['qd_score'].values,\n",
    "            ga_evals,\n",
    "            baseline_convergence_qd\n",
    "        )\n",
    "        \n",
    "        if evals_to_convergence is not None:\n",
    "            eval_savings = baseline_convergence_evals - evals_to_convergence\n",
    "            pct_savings = (eval_savings / baseline_convergence_evals) * 100\n",
    "            \n",
    "            convergence_results.append({\n",
    "                'seed': seed,\n",
    "                'config_name': config['name'],\n",
    "                'baseline_convergence_qd': baseline_convergence_qd,\n",
    "                'baseline_convergence_evals': baseline_convergence_evals,\n",
    "                'ga_evals_to_convergence': evals_to_convergence,\n",
    "                'eval_savings': eval_savings,\n",
    "                'pct_savings': pct_savings,\n",
    "                'converged': True,\n",
    "            })\n",
    "        else:\n",
    "            # DNS-GA didn't reach baseline's convergence score\n",
    "            convergence_results.append({\n",
    "                'seed': seed,\n",
    "                'config_name': config['name'],\n",
    "                'baseline_convergence_qd': baseline_convergence_qd,\n",
    "                'baseline_convergence_evals': baseline_convergence_evals,\n",
    "                'ga_evals_to_convergence': None,\n",
    "                'eval_savings': None,\n",
    "                'pct_savings': None,\n",
    "                'converged': False,\n",
    "            })\n",
    "\n",
    "convergence_df = pd.DataFrame(convergence_results)\n",
    "\n",
    "print(f\"\\nProcessed {len(convergence_results)} seed-config pairs\")\n",
    "print(f\"Converged: {convergence_df['converged'].sum()}\")\n",
    "print(f\"Did not converge: {(~convergence_df['converged']).sum()}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e3fee",
   "metadata": {},
   "source": [
    "### Statistical Summary: Convergence Efficiency Across All Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONVERGENCE EFFICIENCY STATISTICS (31 SEEDS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  {'='*60}\")\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        print(f\"  âœ— No seeds reached baseline convergence\")\n",
    "        continue\n",
    "    \n",
    "    # Success rate\n",
    "    success_rate = (len(converged_data) / len(config_data)) * 100\n",
    "    print(f\"  Success rate: {len(converged_data)}/{len(config_data)} seeds ({success_rate:.1f}%)\")\n",
    "    \n",
    "    # Evaluation savings statistics\n",
    "    eval_savings = converged_data['eval_savings'].values\n",
    "    pct_savings = converged_data['pct_savings'].values\n",
    "    \n",
    "    print(f\"\\n  Evaluation Savings:\")\n",
    "    print(f\"    Mean:   {np.mean(eval_savings):>10,.0f} evals ({np.mean(pct_savings):>6.2f}%)\")\n",
    "    print(f\"    Median: {np.median(eval_savings):>10,.0f} evals ({np.median(pct_savings):>6.2f}%)\")\n",
    "    print(f\"    Std:    {np.std(eval_savings):>10,.0f} evals ({np.std(pct_savings):>6.2f}%)\")\n",
    "    print(f\"    Min:    {np.min(eval_savings):>10,.0f} evals ({np.min(pct_savings):>6.2f}%)\")\n",
    "    print(f\"    Max:    {np.max(eval_savings):>10,.0f} evals ({np.max(pct_savings):>6.2f}%)\")\n",
    "    \n",
    "    # How many show positive savings?\n",
    "    positive_savings = np.sum(eval_savings > 0)\n",
    "    print(f\"\\n  Seeds with positive savings: {positive_savings}/{len(converged_data)} ({positive_savings/len(converged_data)*100:.1f}%)\")\n",
    "    \n",
    "    # Baseline comparison\n",
    "    baseline_evals = converged_data['baseline_convergence_evals'].values[0]\n",
    "    mean_ga_evals = np.mean(converged_data['ga_evals_to_convergence'].values)\n",
    "    print(f\"\\n  Mean evaluations to convergence:\")\n",
    "    print(f\"    Baseline:       {baseline_evals:>10,.0f} evals (always)\")\n",
    "    print(f\"    DNS-GA (mean):  {mean_ga_evals:>10,.0f} evals\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6397eca",
   "metadata": {},
   "source": [
    "### Key Question: Is Seed 42 an Outlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SEED 42 ANALYSIS: Outlier or Representative?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        print(f\"  No convergence data available\")\n",
    "        continue\n",
    "    \n",
    "    # Find seed 42's performance\n",
    "    seed42_data = converged_data[converged_data['seed'] == 42]\n",
    "    \n",
    "    if len(seed42_data) == 0:\n",
    "        print(f\"  Seed 42 did not converge for this config\")\n",
    "        continue\n",
    "    \n",
    "    seed42_savings_pct = seed42_data['pct_savings'].values[0]\n",
    "    \n",
    "    # Calculate percentile rank\n",
    "    all_savings = converged_data['pct_savings'].values\n",
    "    percentile = stats.percentileofscore(all_savings, seed42_savings_pct)\n",
    "    \n",
    "    # Is it an outlier? (>2 std from mean)\n",
    "    mean_savings = np.mean(all_savings)\n",
    "    std_savings = np.std(all_savings)\n",
    "    z_score = (seed42_savings_pct - mean_savings) / std_savings if std_savings > 0 else 0\n",
    "    \n",
    "    print(f\"  Seed 42 savings: {seed42_savings_pct:.2f}%\")\n",
    "    print(f\"  Mean savings:    {mean_savings:.2f}% Â± {std_savings:.2f}%\")\n",
    "    print(f\"  Percentile rank: {percentile:.1f}th\")\n",
    "    print(f\"  Z-score:         {z_score:.2f}\")\n",
    "    \n",
    "    if abs(z_score) > 2:\n",
    "        print(f\"  âš  OUTLIER: |Z-score| > 2 (unusual performance)\")\n",
    "    elif percentile > 75:\n",
    "        print(f\"  âœ“ ABOVE AVERAGE: Top {100-percentile:.0f}% performer\")\n",
    "    elif percentile < 25:\n",
    "        print(f\"  âœ— BELOW AVERAGE: Bottom {percentile:.0f}%\")\n",
    "    else:\n",
    "        print(f\"  ~ TYPICAL: Middle 50% of distribution\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5187f01",
   "metadata": {},
   "source": [
    "### Save Convergence Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1103804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save convergence analysis\n",
    "convergence_file = f\"seed_variability_logs/convergence_analysis_{timestamp}.csv\"\n",
    "convergence_df.to_csv(convergence_file, index=False)\n",
    "print(f\"Convergence analysis saved to: {convergence_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3d7ab",
   "metadata": {},
   "source": [
    "## STEP 6: Statistical Tests and Significance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec76a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PAIRED T-TESTS: Convergence Efficiency\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTesting whether DNS-GA reaches convergence with significantly fewer evaluations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  {'='*60}\")\n",
    "    \n",
    "    if len(converged_data) < 3:\n",
    "        print(f\"  âœ— Insufficient data for statistical test (n={len(converged_data)})\")\n",
    "        continue\n",
    "    \n",
    "    # Get matched pairs (same seeds)\n",
    "    baseline_evals = converged_data['baseline_convergence_evals'].values\n",
    "    ga_evals = converged_data['ga_evals_to_convergence'].values\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_value = stats.ttest_rel(ga_evals, baseline_evals)\n",
    "    \n",
    "    # Effect size (Cohen's d for paired samples)\n",
    "    differences = ga_evals - baseline_evals\n",
    "    mean_diff = np.mean(differences)\n",
    "    std_diff = np.std(differences, ddof=1)\n",
    "    cohens_d = mean_diff / std_diff if std_diff > 0 else 0\n",
    "    \n",
    "    # Mean evaluation savings\n",
    "    mean_savings = np.mean(converged_data['eval_savings'].values)\n",
    "    mean_savings_pct = np.mean(converged_data['pct_savings'].values)\n",
    "    \n",
    "    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "    \n",
    "    print(f\"\\n  Sample size: {len(converged_data)} seeds\")\n",
    "    print(f\"  Mean baseline evals:  {np.mean(baseline_evals):>10,.0f}\")\n",
    "    print(f\"  Mean DNS-GA evals:    {np.mean(ga_evals):>10,.0f}\")\n",
    "    print(f\"  Mean difference:      {mean_diff:>10,.0f} ({mean_savings_pct:+.2f}%)\")\n",
    "    print(f\"\\n  t-statistic: {t_stat:>7.3f}\")\n",
    "    print(f\"  p-value:     {p_value:>7.4f} {significance}\")\n",
    "    print(f\"  Cohen's d:   {cohens_d:>7.3f}\")\n",
    "    \n",
    "    # Interpret effect size\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect = \"medium\"\n",
    "    else:\n",
    "        effect = \"large\"\n",
    "    \n",
    "    print(f\"  Effect size: {effect}\")\n",
    "    \n",
    "    # Interpret results\n",
    "    if p_value < 0.05:\n",
    "        if mean_diff < 0:\n",
    "            print(f\"\\n  âœ“ SIGNIFICANT: DNS-GA reaches convergence with FEWER evaluations\")\n",
    "        else:\n",
    "            print(f\"\\n  âœ— SIGNIFICANT: DNS-GA requires MORE evaluations\")\n",
    "    else:\n",
    "        print(f\"\\n  ~ NOT SIGNIFICANT: No reliable difference in convergence speed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34024c",
   "metadata": {},
   "source": [
    "### Config Comparison: g300_gen2 vs g1000_gen4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CONFIG COMPARISON: Frequent (g300) vs Rare (g1000) GA Calls\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nQuestion: Does calling GA more frequently reduce seed dependency?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get data for both configs\n",
    "g300_data = convergence_df[convergence_df['config_name'] == 'DNS-GA_g300_gen2']\n",
    "g1000_data = convergence_df[convergence_df['config_name'] == 'DNS-GA_g1000_gen4']\n",
    "\n",
    "g300_converged = g300_data[g300_data['converged']]\n",
    "g1000_converged = g1000_data[g1000_data['converged']]\n",
    "\n",
    "print(f\"\\nSuccess Rates:\")\n",
    "print(f\"  g300_gen2 (10 GA calls):  {len(g300_converged)}/{len(g300_data)} seeds ({len(g300_converged)/len(g300_data)*100:.1f}%)\")\n",
    "print(f\"  g1000_gen4 (3 GA calls):   {len(g1000_converged)}/{len(g1000_data)} seeds ({len(g1000_converged)/len(g1000_data)*100:.1f}%)\")\n",
    "\n",
    "if len(g300_converged) > 0 and len(g1000_converged) > 0:\n",
    "    print(f\"\\nEvaluation Savings (converged seeds only):\")\n",
    "    print(f\"  g300_gen2:  {np.mean(g300_converged['pct_savings'].values):>6.2f}% Â± {np.std(g300_converged['pct_savings'].values):.2f}%\")\n",
    "    print(f\"  g1000_gen4: {np.mean(g1000_converged['pct_savings'].values):>6.2f}% Â± {np.std(g1000_converged['pct_savings'].values):.2f}%\")\n",
    "    \n",
    "    # Compare on common seeds (seeds that converged in both configs)\n",
    "    common_seeds = set(g300_converged['seed'].values) & set(g1000_converged['seed'].values)\n",
    "    \n",
    "    if len(common_seeds) >= 3:\n",
    "        g300_common = g300_converged[g300_converged['seed'].isin(common_seeds)]\n",
    "        g1000_common = g1000_converged[g1000_converged['seed'].isin(common_seeds)]\n",
    "        \n",
    "        # Sort by seed to ensure matching\n",
    "        g300_common = g300_common.sort_values('seed')\n",
    "        g1000_common = g1000_common.sort_values('seed')\n",
    "        \n",
    "        g300_evals = g300_common['ga_evals_to_convergence'].values\n",
    "        g1000_evals = g1000_common['ga_evals_to_convergence'].values\n",
    "        \n",
    "        # Paired t-test\n",
    "        t_stat, p_value = stats.ttest_rel(g300_evals, g1000_evals)\n",
    "        \n",
    "        mean_diff = np.mean(g300_evals) - np.mean(g1000_evals)\n",
    "        \n",
    "        print(f\"\\nDirect Comparison (n={len(common_seeds)} common seeds):\")\n",
    "        print(f\"  g300_gen2 mean:  {np.mean(g300_evals):>10,.0f} evals\")\n",
    "        print(f\"  g1000_gen4 mean: {np.mean(g1000_evals):>10,.0f} evals\")\n",
    "        print(f\"  Difference:      {mean_diff:>10,.0f} evals\")\n",
    "        print(f\"  t-statistic:     {t_stat:>7.3f}\")\n",
    "        print(f\"  p-value:         {p_value:>7.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            if mean_diff < 0:\n",
    "                print(f\"\\n  âœ“ g300_gen2 significantly FASTER to convergence\")\n",
    "            else:\n",
    "                print(f\"\\n  âœ“ g1000_gen4 significantly FASTER to convergence\")\n",
    "        else:\n",
    "            print(f\"\\n  ~ No significant difference between configs\")\n",
    "    else:\n",
    "        print(f\"\\n  Insufficient common seeds for comparison (n={len(common_seeds)})\")\n",
    "    \n",
    "    # Variance comparison (seed dependency)\n",
    "    print(f\"\\nVariance (seed dependency):\")\n",
    "    print(f\"  g300_gen2 std:  {np.std(g300_converged['pct_savings'].values):.2f}%\")\n",
    "    print(f\"  g1000_gen4 std: {np.std(g1000_converged['pct_savings'].values):.2f}%\")\n",
    "    \n",
    "    var_ratio = np.var(g300_converged['pct_savings'].values) / np.var(g1000_converged['pct_savings'].values)\n",
    "    print(f\"  Variance ratio: {var_ratio:.2f}\")\n",
    "    \n",
    "    if var_ratio < 0.8:\n",
    "        print(f\"  â†’ g300_gen2 shows LESS seed dependency\")\n",
    "    elif var_ratio > 1.2:\n",
    "        print(f\"  â†’ g1000_gen4 shows LESS seed dependency\")\n",
    "    else:\n",
    "        print(f\"  â†’ Similar seed dependency\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42248324",
   "metadata": {},
   "source": [
    "### Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78890074",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DISTRIBUTION ANALYSIS: Evaluation Savings\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    savings_pct = converged_data['pct_savings'].values\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  {'='*60}\")\n",
    "    \n",
    "    # Percentiles\n",
    "    print(f\"\\n  Percentiles of evaluation savings:\")\n",
    "    print(f\"    10th: {np.percentile(savings_pct, 10):>6.2f}%\")\n",
    "    print(f\"    25th: {np.percentile(savings_pct, 25):>6.2f}%\")\n",
    "    print(f\"    50th: {np.percentile(savings_pct, 50):>6.2f}% (median)\")\n",
    "    print(f\"    75th: {np.percentile(savings_pct, 75):>6.2f}%\")\n",
    "    print(f\"    90th: {np.percentile(savings_pct, 90):>6.2f}%\")\n",
    "    \n",
    "    # Distribution shape\n",
    "    skewness = stats.skew(savings_pct)\n",
    "    kurtosis = stats.kurtosis(savings_pct)\n",
    "    \n",
    "    print(f\"\\n  Distribution shape:\")\n",
    "    print(f\"    Skewness: {skewness:>6.3f}\", end=\"\")\n",
    "    if abs(skewness) < 0.5:\n",
    "        print(\" (approximately symmetric)\")\n",
    "    elif skewness < 0:\n",
    "        print(\" (left-skewed, negative tail)\")\n",
    "    else:\n",
    "        print(\" (right-skewed, positive tail)\")\n",
    "    \n",
    "    print(f\"    Kurtosis: {kurtosis:>6.3f}\", end=\"\")\n",
    "    if abs(kurtosis) < 0.5:\n",
    "        print(\" (normal-like tails)\")\n",
    "    elif kurtosis > 0:\n",
    "        print(\" (heavy tails, more outliers)\")\n",
    "    else:\n",
    "        print(\" (light tails, fewer outliers)\")\n",
    "    \n",
    "    # Normality test\n",
    "    _, norm_p = stats.normaltest(savings_pct)\n",
    "    print(f\"\\n  Normality test p-value: {norm_p:.4f}\", end=\"\")\n",
    "    if norm_p > 0.05:\n",
    "        print(\" (approximately normal)\")\n",
    "    else:\n",
    "        print(\" (not normally distributed)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd09450",
   "metadata": {},
   "source": [
    "## STEP 7: Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55e77f",
   "metadata": {},
   "source": [
    "### Plot 1: Boxplots of Evaluation Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cac138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot: Evaluation Savings Distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Prepare data for plotting\n",
    "plot_data = []\n",
    "plot_labels = []\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    if len(converged_data) > 0:\n",
    "        plot_data.append(converged_data['pct_savings'].values)\n",
    "        plot_labels.append(config['name'].replace('DNS-GA_', ''))\n",
    "\n",
    "bp = ax.boxplot(plot_data, labels=plot_labels, patch_artist=True)\n",
    "\n",
    "# Color boxplots\n",
    "colors = ['#ff9999', '#66b3ff']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "# Add horizontal line at 0%\n",
    "ax.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='No savings')\n",
    "\n",
    "# Highlight seed 42\n",
    "for i, config in enumerate(dns_ga_configs):\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    seed42_data = config_data[config_data['seed'] == 42]\n",
    "    \n",
    "    if len(seed42_data) > 0 and seed42_data['converged'].values[0]:\n",
    "        seed42_savings = seed42_data['pct_savings'].values[0]\n",
    "        ax.plot(i+1, seed42_savings, 'r*', markersize=15, label='Seed 42' if i == 0 else '')\n",
    "\n",
    "ax.set_xlabel('Configuration', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Evaluation Savings (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Convergence Efficiency: Evaluation Savings Distribution\\n(31 seeds per config)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'seed_variability_logs/boxplot_savings_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Boxplot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beae0d8",
   "metadata": {},
   "source": [
    "### Plot 2: Histograms of Evaluation Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms: Distribution of Evaluation Savings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for idx, config in enumerate(dns_ga_configs):\n",
    "    ax = axes[idx]\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        ax.text(0.5, 0.5, 'No convergence data', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=14)\n",
    "        ax.set_title(config['name'].replace('DNS-GA_', ''))\n",
    "        continue\n",
    "    \n",
    "    savings = converged_data['pct_savings'].values\n",
    "    \n",
    "    # Histogram\n",
    "    n, bins, patches = ax.hist(savings, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Add vertical lines for statistics\n",
    "    mean_val = np.mean(savings)\n",
    "    median_val = np.median(savings)\n",
    "    \n",
    "    ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}%')\n",
    "    ax.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}%')\n",
    "    ax.axvline(0, color='gray', linestyle='-', linewidth=1, alpha=0.5, label='No savings')\n",
    "    \n",
    "    # Highlight seed 42\n",
    "    seed42_data = converged_data[converged_data['seed'] == 42]\n",
    "    if len(seed42_data) > 0:\n",
    "        seed42_val = seed42_data['pct_savings'].values[0]\n",
    "        ax.axvline(seed42_val, color='purple', linestyle=':', linewidth=2, label=f'Seed 42: {seed42_val:.2f}%')\n",
    "    \n",
    "    ax.set_xlabel('Evaluation Savings (%)', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Seeds', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(config['name'].replace('DNS-GA_', ''), fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='upper left', fontsize=9)\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Distribution of Evaluation Savings Across 31 Seeds', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'seed_variability_logs/histograms_savings_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Histograms saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c03d30",
   "metadata": {},
   "source": [
    "### Plot 3: Success Rate Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88041d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: Success rate and mean savings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "config_names = []\n",
    "success_rates = []\n",
    "mean_savings = []\n",
    "positive_savings_pct = []\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    config_names.append(config['name'].replace('DNS-GA_', ''))\n",
    "    success_rates.append((len(converged_data) / len(config_data)) * 100)\n",
    "    \n",
    "    if len(converged_data) > 0:\n",
    "        mean_savings.append(np.mean(converged_data['pct_savings'].values))\n",
    "        positive_count = np.sum(converged_data['pct_savings'].values > 0)\n",
    "        positive_savings_pct.append((positive_count / len(converged_data)) * 100)\n",
    "    else:\n",
    "        mean_savings.append(0)\n",
    "        positive_savings_pct.append(0)\n",
    "\n",
    "# Plot 1: Success rate\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(config_names, success_rates, color=['#ff9999', '#66b3ff'], alpha=0.7, edgecolor='black')\n",
    "ax1.axhline(y=100, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax1.set_ylabel('Success Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Configuration', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Convergence Success Rate\\n(% of seeds reaching baseline convergence faster)', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax1.set_ylim(0, 110)\n",
    "ax1.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars1, success_rates):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "            f'{val:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 2: Mean evaluation savings\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(config_names, mean_savings, color=['#ff9999', '#66b3ff'], alpha=0.7, edgecolor='black')\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax2.set_ylabel('Mean Evaluation Savings (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Configuration', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Mean Convergence Efficiency\\n(converged seeds only)', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax2.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars2, mean_savings):\n",
    "    height = bar.get_height()\n",
    "    va = 'bottom' if height >= 0 else 'top'\n",
    "    offset = 0.5 if height >= 0 else -0.5\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + offset,\n",
    "            f'{val:+.2f}%', ha='center', va=va, fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'seed_variability_logs/success_rates_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Success rate charts saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc7556d",
   "metadata": {},
   "source": [
    "### Plot 4: Scatter Plot - Baseline Convergence vs Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Baseline QD convergence score vs Evaluation savings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors_map = {'DNS-GA_g300_gen2': '#ff9999', 'DNS-GA_g1000_gen4': '#66b3ff'}\n",
    "\n",
    "for idx, config in enumerate(dns_ga_configs):\n",
    "    ax = axes[idx]\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        ax.text(0.5, 0.5, 'No convergence data', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=14)\n",
    "        ax.set_title(config['name'].replace('DNS-GA_', ''))\n",
    "        continue\n",
    "    \n",
    "    baseline_qd = converged_data['baseline_convergence_qd'].values\n",
    "    pct_savings = converged_data['pct_savings'].values\n",
    "    seeds = converged_data['seed'].values\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(baseline_qd, pct_savings, alpha=0.6, s=80, \n",
    "              color=colors_map[config['name']], edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Highlight seed 42\n",
    "    seed42_data = converged_data[converged_data['seed'] == 42]\n",
    "    if len(seed42_data) > 0:\n",
    "        seed42_qd = seed42_data['baseline_convergence_qd'].values[0]\n",
    "        seed42_savings = seed42_data['pct_savings'].values[0]\n",
    "        ax.scatter(seed42_qd, seed42_savings, color='red', s=200, marker='*', \n",
    "                  edgecolors='black', linewidth=1, label='Seed 42', zorder=5)\n",
    "    \n",
    "    # Add horizontal line at 0\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(baseline_qd, pct_savings, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(baseline_qd, p(baseline_qd), \"r--\", alpha=0.5, linewidth=1.5, \n",
    "           label=f'Trend: y={z[0]:.4f}x+{z[1]:.2f}')\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = np.corrcoef(baseline_qd, pct_savings)[0, 1]\n",
    "    \n",
    "    ax.set_xlabel('Baseline Convergence QD Score', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Evaluation Savings (%)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{config[\"name\"].replace(\"DNS-GA_\", \"\")}\\n(correlation: {corr:.3f})', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Does Baseline Convergence Score Predict DNS-GA Efficiency?', \n",
    "            fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'seed_variability_logs/scatter_baseline_vs_savings_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Scatter plots saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072d892",
   "metadata": {},
   "source": [
    "## STEP 8: Final Research Conclusions\n",
    "\n",
    "**Core Question**: Is seed 42's 53% evaluation savings representative, or did we get lucky?\n",
    "\n",
    "This section synthesizes findings from Steps 5-7 to provide publication-ready conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3dcb5",
   "metadata": {},
   "source": [
    "### Question 1: Is Seed 42 an Outlier or Representative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"QUESTION 1: Is Seed 42 an Outlier or Representative?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  {'='*60}\")\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        print(f\"  No convergence data\")\n",
    "        continue\n",
    "    \n",
    "    # Find seed 42\n",
    "    seed42_data = converged_data[converged_data['seed'] == 42]\n",
    "    \n",
    "    if len(seed42_data) == 0:\n",
    "        print(f\"  Seed 42 did not converge\")\n",
    "        continue\n",
    "    \n",
    "    seed42_savings = seed42_data['pct_savings'].values[0]\n",
    "    all_savings = converged_data['pct_savings'].values\n",
    "    \n",
    "    # Statistics\n",
    "    mean_savings = np.mean(all_savings)\n",
    "    std_savings = np.std(all_savings)\n",
    "    median_savings = np.median(all_savings)\n",
    "    percentile = stats.percentileofscore(all_savings, seed42_savings)\n",
    "    z_score = (seed42_savings - mean_savings) / std_savings if std_savings > 0 else 0\n",
    "    \n",
    "    print(f\"\\n  Seed 42 Performance:\")\n",
    "    print(f\"    Evaluation savings: {seed42_savings:.2f}%\")\n",
    "    print(f\"    Percentile rank:    {percentile:.1f}th\")\n",
    "    print(f\"    Z-score:            {z_score:.2f}\")\n",
    "    \n",
    "    print(f\"\\n  Distribution Statistics (31 seeds):\")\n",
    "    print(f\"    Mean:   {mean_savings:.2f}% Â± {std_savings:.2f}%\")\n",
    "    print(f\"    Median: {median_savings:.2f}%\")\n",
    "    print(f\"    Range:  [{np.min(all_savings):.2f}%, {np.max(all_savings):.2f}%]\")\n",
    "    \n",
    "    # Classification\n",
    "    print(f\"\\n  **VERDICT**:\")\n",
    "    if abs(z_score) > 2:\n",
    "        print(f\"    ðŸ”´ OUTLIER: Seed 42 is >2 standard deviations from mean\")\n",
    "        print(f\"       Its {seed42_savings:.2f}% savings is NOT representative of typical performance\")\n",
    "    elif percentile > 75:\n",
    "        print(f\"    ðŸŸ¡ ABOVE AVERAGE: Seed 42 is in top {100-percentile:.0f}%\")\n",
    "        print(f\"       Performance is better than typical, but not impossibly rare\")\n",
    "    elif percentile < 25:\n",
    "        print(f\"    ðŸ”µ BELOW AVERAGE: Seed 42 is in bottom {percentile:.0f}%\")\n",
    "        print(f\"       Performance is worse than typical\")\n",
    "    else:\n",
    "        print(f\"    ðŸŸ¢ TYPICAL: Seed 42 is in middle 50% of distribution\")\n",
    "        print(f\"       Performance is representative of average behavior\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION:\")\n",
    "print(\"=\"*80)\n",
    "print(\"Seed 42's performance should be classified based on z-score and percentile.\")\n",
    "print(\"If |z| > 2, it's a statistical outlier and not representative.\")\n",
    "print(\"If percentile > 75, it's above average but within realistic range.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea82613b",
   "metadata": {},
   "source": [
    "### Question 2: What is Competition-GA's True Success Rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"QUESTION 2: Competition-GA's True Success Rate\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nDefining 'success' as: DNS-GA reaches baseline convergence with fewer evaluations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "success_summary = []\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  {'='*60}\")\n",
    "    \n",
    "    # Success rate 1: Convergence rate\n",
    "    convergence_rate = (len(converged_data) / len(config_data)) * 100\n",
    "    print(f\"\\n  1. Convergence Rate:\")\n",
    "    print(f\"     {len(converged_data)}/{len(config_data)} seeds ({convergence_rate:.1f}%)\")\n",
    "    print(f\"     â†’ Reached baseline's final QD score within 3000 iterations\")\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        print(f\"\\n  âš  No seeds converged - unable to calculate efficiency metrics\")\n",
    "        continue\n",
    "    \n",
    "    # Success rate 2: Positive savings rate\n",
    "    positive_savings = converged_data['pct_savings'] > 0\n",
    "    positive_rate = (positive_savings.sum() / len(converged_data)) * 100\n",
    "    print(f\"\\n  2. Efficiency Success Rate (converged seeds only):\")\n",
    "    print(f\"     {positive_savings.sum()}/{len(converged_data)} seeds ({positive_rate:.1f}%)\")\n",
    "    print(f\"     â†’ Converged faster than baseline\")\n",
    "    \n",
    "    # Success rate 3: Combined\n",
    "    combined_success = positive_savings.sum()\n",
    "    combined_rate = (combined_success / len(config_data)) * 100\n",
    "    print(f\"\\n  3. Overall Success Rate (all 31 seeds):\")\n",
    "    print(f\"     {combined_success}/{len(config_data)} seeds ({combined_rate:.1f}%)\")\n",
    "    print(f\"     â†’ Both converged AND faster than baseline\")\n",
    "    \n",
    "    # Evaluation savings statistics\n",
    "    all_savings = converged_data['pct_savings'].values\n",
    "    print(f\"\\n  Evaluation Savings (converged seeds):\")\n",
    "    print(f\"     Mean:   {np.mean(all_savings):>6.2f}%\")\n",
    "    print(f\"     Median: {np.median(all_savings):>6.2f}%\")\n",
    "    print(f\"     Std:    {np.std(all_savings):>6.2f}%\")\n",
    "    \n",
    "    # Best and worst performers\n",
    "    best_idx = np.argmax(all_savings)\n",
    "    worst_idx = np.argmin(all_savings)\n",
    "    best_seed = converged_data.iloc[best_idx]['seed']\n",
    "    worst_seed = converged_data.iloc[worst_idx]['seed']\n",
    "    \n",
    "    print(f\"\\n  Best performer:  Seed {int(best_seed)} ({all_savings[best_idx]:+.2f}%)\")\n",
    "    print(f\"  Worst performer: Seed {int(worst_seed)} ({all_savings[worst_idx]:+.2f}%)\")\n",
    "    \n",
    "    success_summary.append({\n",
    "        'config': config['name'],\n",
    "        'convergence_rate': convergence_rate,\n",
    "        'efficiency_rate': positive_rate,\n",
    "        'overall_success': combined_rate,\n",
    "        'mean_savings': np.mean(all_savings),\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: Success Rate Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "success_df = pd.DataFrame(success_summary)\n",
    "print(\"\\n\" + success_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION:\")\n",
    "print(\"=\"*80)\n",
    "print(\"Report 'Overall Success Rate' as the true success rate:\")\n",
    "print(\"  â†’ Percentage of all 31 seeds that both converged AND saved evaluations\")\n",
    "print(\"This is the most honest metric for publication.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e813974",
   "metadata": {},
   "source": [
    "### Question 3: Which Config is Better? (g300_gen2 vs g1000_gen4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eeead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"QUESTION 3: Config Comparison - Frequent vs Rare GA Calls\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nHypothesis: More frequent GA calls reduce seed dependency\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get data\n",
    "g300_data = convergence_df[convergence_df['config_name'] == 'DNS-GA_g300_gen2']\n",
    "g1000_data = convergence_df[convergence_df['config_name'] == 'DNS-GA_g1000_gen4']\n",
    "\n",
    "g300_converged = g300_data[g300_data['converged']]\n",
    "g1000_converged = g1000_data[g1000_data['converged']]\n",
    "\n",
    "print(f\"\\nConfiguration Details:\")\n",
    "print(f\"  g300_gen2:  GA every 300 iters Ã— 2 generations = 10 GA calls\")\n",
    "print(f\"  g1000_gen4: GA every 1000 iters Ã— 4 generations = 3 GA calls\")\n",
    "\n",
    "# 1. Success Rate Comparison\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"1. SUCCESS RATE (Overall: converged + faster)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "g300_success = (g300_converged['pct_savings'] > 0).sum()\n",
    "g1000_success = (g1000_converged['pct_savings'] > 0).sum()\n",
    "\n",
    "g300_success_rate = (g300_success / len(g300_data)) * 100\n",
    "g1000_success_rate = (g1000_success / len(g1000_data)) * 100\n",
    "\n",
    "print(f\"  g300_gen2:  {g300_success}/{len(g300_data)} = {g300_success_rate:.1f}%\")\n",
    "print(f\"  g1000_gen4: {g1000_success}/{len(g1000_data)} = {g1000_success_rate:.1f}%\")\n",
    "\n",
    "if g300_success_rate > g1000_success_rate:\n",
    "    diff = g300_success_rate - g1000_success_rate\n",
    "    print(f\"\\n  âœ“ g300_gen2 has {diff:.1f}% higher success rate\")\n",
    "    print(f\"    More frequent GA calls improve reliability\")\n",
    "elif g1000_success_rate > g300_success_rate:\n",
    "    diff = g1000_success_rate - g300_success_rate\n",
    "    print(f\"\\n  âœ“ g1000_gen4 has {diff:.1f}% higher success rate\")\n",
    "    print(f\"    Deeper but rarer GA calls are more effective\")\n",
    "else:\n",
    "    print(f\"\\n  ~ Equal success rates\")\n",
    "\n",
    "# 2. Mean Savings Comparison (converged seeds only)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"2. MEAN EVALUATION SAVINGS (converged seeds)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if len(g300_converged) > 0 and len(g1000_converged) > 0:\n",
    "    g300_mean = np.mean(g300_converged['pct_savings'].values)\n",
    "    g1000_mean = np.mean(g1000_converged['pct_savings'].values)\n",
    "    \n",
    "    print(f\"  g300_gen2:  {g300_mean:>6.2f}%\")\n",
    "    print(f\"  g1000_gen4: {g1000_mean:>6.2f}%\")\n",
    "    \n",
    "    if g300_mean > g1000_mean:\n",
    "        diff = g300_mean - g1000_mean\n",
    "        print(f\"\\n  âœ“ g300_gen2 saves {diff:.2f}% more evaluations on average\")\n",
    "    else:\n",
    "        diff = g1000_mean - g300_mean\n",
    "        print(f\"\\n  âœ“ g1000_gen4 saves {diff:.2f}% more evaluations on average\")\n",
    "else:\n",
    "    print(\"  Insufficient data for comparison\")\n",
    "\n",
    "# 3. Seed Dependency (Variance)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"3. SEED DEPENDENCY (variance of savings)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if len(g300_converged) > 0 and len(g1000_converged) > 0:\n",
    "    g300_std = np.std(g300_converged['pct_savings'].values)\n",
    "    g1000_std = np.std(g1000_converged['pct_savings'].values)\n",
    "    var_ratio = (g300_std**2) / (g1000_std**2)\n",
    "    \n",
    "    print(f\"  g300_gen2 std:  {g300_std:.2f}%\")\n",
    "    print(f\"  g1000_gen4 std: {g1000_std:.2f}%\")\n",
    "    print(f\"  Variance ratio: {var_ratio:.2f}\")\n",
    "    \n",
    "    if var_ratio < 0.8:\n",
    "        print(f\"\\n  âœ“ g300_gen2 shows 20%+ LESS seed dependency\")\n",
    "        print(f\"    More frequent GA calls â†’ more consistent performance\")\n",
    "    elif var_ratio > 1.25:\n",
    "        print(f\"\\n  âœ“ g1000_gen4 shows 20%+ LESS seed dependency\")\n",
    "        print(f\"    Deeper but rarer GA calls â†’ more consistent performance\")\n",
    "    else:\n",
    "        print(f\"\\n  ~ Similar seed dependency\")\n",
    "\n",
    "# 4. Statistical Test (Common Seeds)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"4. STATISTICAL COMPARISON (paired t-test)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "common_seeds = set(g300_converged['seed'].values) & set(g1000_converged['seed'].values)\n",
    "\n",
    "if len(common_seeds) >= 3:\n",
    "    g300_common = g300_converged[g300_converged['seed'].isin(common_seeds)].sort_values('seed')\n",
    "    g1000_common = g1000_converged[g1000_converged['seed'].isin(common_seeds)].sort_values('seed')\n",
    "    \n",
    "    g300_evals = g300_common['ga_evals_to_convergence'].values\n",
    "    g1000_evals = g1000_common['ga_evals_to_convergence'].values\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_rel(g300_evals, g1000_evals)\n",
    "    mean_diff = np.mean(g300_evals) - np.mean(g1000_evals)\n",
    "    \n",
    "    print(f\"  Common seeds: {len(common_seeds)}\")\n",
    "    print(f\"  Mean g300_gen2:  {np.mean(g300_evals):>10,.0f} evals\")\n",
    "    print(f\"  Mean g1000_gen4: {np.mean(g1000_evals):>10,.0f} evals\")\n",
    "    print(f\"  Difference:      {mean_diff:>10,.0f} evals\")\n",
    "    print(f\"  t-statistic:     {t_stat:>7.3f}\")\n",
    "    print(f\"  p-value:         {p_value:>7.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        if mean_diff < 0:\n",
    "            print(f\"\\n  âœ“âœ“ g300_gen2 is SIGNIFICANTLY faster (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"\\n  âœ“âœ“ g1000_gen4 is SIGNIFICANTLY faster (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"\\n  ~ No significant difference (p â‰¥ 0.05)\")\n",
    "else:\n",
    "    print(f\"  Insufficient common seeds (n={len(common_seeds)})\")\n",
    "\n",
    "# 5. Recommendation\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RECOMMENDATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nScore each config on 4 criteria:\")\n",
    "print(\"  1. Success rate (higher is better)\")\n",
    "print(\"  2. Mean savings (higher is better)\")\n",
    "print(\"  3. Low seed dependency (lower variance is better)\")\n",
    "print(\"  4. Statistical significance (p < 0.05 is decisive)\")\n",
    "print(\"\\nRecommend the config that wins more criteria.\")\n",
    "print(\"If tied, recommend based on success rate (most important for reliability).\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95900a62",
   "metadata": {},
   "source": [
    "### Question 4: What Predicts DNS-GA Success?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0874fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"QUESTION 4: Predictive Factors for DNS-GA Success\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAnalyzing correlation between baseline characteristics and DNS-GA efficiency\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  {'='*60}\")\n",
    "    \n",
    "    if len(converged_data) < 3:\n",
    "        print(f\"  Insufficient data (n={len(converged_data)})\")\n",
    "        continue\n",
    "    \n",
    "    # Test 1: Baseline convergence QD score vs savings\n",
    "    baseline_qd = converged_data['baseline_convergence_qd'].values\n",
    "    pct_savings = converged_data['pct_savings'].values\n",
    "    \n",
    "    corr_qd = np.corrcoef(baseline_qd, pct_savings)[0, 1]\n",
    "    \n",
    "    print(f\"\\n  1. Baseline Convergence QD vs Evaluation Savings\")\n",
    "    print(f\"     Correlation: {corr_qd:.3f}\")\n",
    "    \n",
    "    if abs(corr_qd) > 0.5:\n",
    "        direction = \"higher\" if corr_qd > 0 else \"lower\"\n",
    "        print(f\"     âœ“ STRONG: Seeds with {direction} baseline QD save more evaluations\")\n",
    "    elif abs(corr_qd) > 0.3:\n",
    "        direction = \"higher\" if corr_qd > 0 else \"lower\"\n",
    "        print(f\"     ~ MODERATE: Seeds with {direction} baseline QD tend to save more\")\n",
    "    else:\n",
    "        print(f\"     âœ— WEAK: Baseline QD is not a strong predictor\")\n",
    "    \n",
    "    # Test 2: Is there a threshold effect?\n",
    "    median_qd = np.median(baseline_qd)\n",
    "    high_qd_mask = baseline_qd > median_qd\n",
    "    low_qd_mask = baseline_qd <= median_qd\n",
    "    \n",
    "    high_qd_savings = pct_savings[high_qd_mask]\n",
    "    low_qd_savings = pct_savings[low_qd_mask]\n",
    "    \n",
    "    print(f\"\\n  2. High vs Low Baseline QD (threshold: {median_qd:.2f})\")\n",
    "    print(f\"     High QD seeds (n={len(high_qd_savings)}): {np.mean(high_qd_savings):>6.2f}% savings\")\n",
    "    print(f\"     Low QD seeds  (n={len(low_qd_savings)}):  {np.mean(low_qd_savings):>6.2f}% savings\")\n",
    "    \n",
    "    if len(high_qd_savings) >= 2 and len(low_qd_savings) >= 2:\n",
    "        t_stat, p_value = stats.ttest_ind(high_qd_savings, low_qd_savings)\n",
    "        print(f\"     t-test p-value: {p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            if np.mean(high_qd_savings) > np.mean(low_qd_savings):\n",
    "                print(f\"     âœ“ High baseline QD â†’ significantly MORE savings\")\n",
    "            else:\n",
    "                print(f\"     âœ“ Low baseline QD â†’ significantly MORE savings\")\n",
    "        else:\n",
    "            print(f\"     ~ No significant difference by baseline QD\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nCorrelation analysis reveals:\")\n",
    "print(\"  â€¢ If |corr| > 0.5: Baseline QD is a STRONG predictor\")\n",
    "print(\"  â€¢ If |corr| > 0.3: Baseline QD has MODERATE predictive power\")\n",
    "print(\"  â€¢ If |corr| < 0.3: Baseline QD is NOT a reliable predictor\")\n",
    "print(\"\\nPositive correlation: Better baseline performance â†’ more DNS-GA savings\")\n",
    "print(\"Negative correlation: Worse baseline performance â†’ more DNS-GA savings\")\n",
    "print(\"\\nThis helps identify when Competition-GA is most useful.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6483e8fc",
   "metadata": {},
   "source": [
    "### Publication-Ready Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf31b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PUBLICATION-READY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ“Š EXPERIMENTAL DESIGN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  â€¢ 31 random seeds (robust statistical power)\")\n",
    "print(f\"  â€¢ 2 DNS-GA configurations:\")\n",
    "print(f\"    - g300_gen2: Frequent GA (10 calls)\")\n",
    "print(f\"    - g1000_gen4: Rare but deep GA (3 calls)\")\n",
    "print(f\"  â€¢ 1 baseline (DNS without GA)\")\n",
    "print(f\"  â€¢ 3000 iterations per experiment\")\n",
    "print(f\"  â€¢ iso_sigma=0.01 (aggressive mutation)\")\n",
    "print(f\"  â€¢ Total: 94 experiments (~2.6 hours parallel)\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    if len(converged_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    overall_success = (converged_data['pct_savings'] > 0).sum()\n",
    "    overall_rate = (overall_success / len(config_data)) * 100\n",
    "    mean_savings = np.mean(converged_data['pct_savings'].values)\n",
    "    median_savings = np.median(converged_data['pct_savings'].values)\n",
    "    std_savings = np.std(converged_data['pct_savings'].values)\n",
    "    \n",
    "    print(f\"\\n  {config['name']}:\")\n",
    "    print(f\"    â€¢ Overall success rate: {overall_rate:.1f}% ({overall_success}/{len(config_data)} seeds)\")\n",
    "    print(f\"    â€¢ Mean evaluation savings: {mean_savings:+.2f}% Â± {std_savings:.2f}%\")\n",
    "    print(f\"    â€¢ Median evaluation savings: {median_savings:+.2f}%\")\n",
    "    \n",
    "    # Seed 42 analysis\n",
    "    seed42_data = converged_data[converged_data['seed'] == 42]\n",
    "    if len(seed42_data) > 0:\n",
    "        seed42_savings = seed42_data['pct_savings'].values[0]\n",
    "        z_score = (seed42_savings - mean_savings) / std_savings if std_savings > 0 else 0\n",
    "        percentile = stats.percentileofscore(converged_data['pct_savings'].values, seed42_savings)\n",
    "        \n",
    "        print(f\"    â€¢ Seed 42: {seed42_savings:.2f}% (z={z_score:.2f}, {percentile:.0f}th percentile)\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ RESEARCH QUESTIONS ANSWERED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Automatically generate answers based on data\n",
    "print(\"\\n  Q1: Is seed 42 an outlier?\")\n",
    "for config in dns_ga_configs:\n",
    "    converged_data = convergence_df[(convergence_df['config_name'] == config['name']) & \n",
    "                                   (convergence_df['converged'])]\n",
    "    if len(converged_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    seed42_data = converged_data[converged_data['seed'] == 42]\n",
    "    if len(seed42_data) > 0:\n",
    "        seed42_savings = seed42_data['pct_savings'].values[0]\n",
    "        mean_savings = np.mean(converged_data['pct_savings'].values)\n",
    "        std_savings = np.std(converged_data['pct_savings'].values)\n",
    "        z_score = (seed42_savings - mean_savings) / std_savings if std_savings > 0 else 0\n",
    "        \n",
    "        if abs(z_score) > 2:\n",
    "            verdict = \"YES - Statistical outlier\"\n",
    "        elif stats.percentileofscore(converged_data['pct_savings'].values, seed42_savings) > 75:\n",
    "            verdict = \"NO - Above average but not outlier\"\n",
    "        else:\n",
    "            verdict = \"NO - Typical performance\"\n",
    "        \n",
    "        print(f\"      {config['name']}: {verdict}\")\n",
    "\n",
    "print(\"\\n  Q2: True success rate?\")\n",
    "for config in dns_ga_configs:\n",
    "    config_data = convergence_df[convergence_df['config_name'] == config['name']]\n",
    "    converged_data = config_data[config_data['converged']]\n",
    "    \n",
    "    if len(converged_data) > 0:\n",
    "        overall_success = (converged_data['pct_savings'] > 0).sum()\n",
    "        overall_rate = (overall_success / len(config_data)) * 100\n",
    "        print(f\"      {config['name']}: {overall_rate:.1f}%\")\n",
    "\n",
    "print(\"\\n  Q3: Which config is better?\")\n",
    "g300_data = convergence_df[convergence_df['config_name'] == 'DNS-GA_g300_gen2']\n",
    "g1000_data = convergence_df[convergence_df['config_name'] == 'DNS-GA_g1000_gen4']\n",
    "g300_converged = g300_data[g300_data['converged']]\n",
    "g1000_converged = g1000_data[g1000_data['converged']]\n",
    "\n",
    "if len(g300_converged) > 0 and len(g1000_converged) > 0:\n",
    "    g300_success = (g300_converged['pct_savings'] > 0).sum() / len(g300_data) * 100\n",
    "    g1000_success = (g1000_converged['pct_savings'] > 0).sum() / len(g1000_data) * 100\n",
    "    \n",
    "    if g300_success > g1000_success:\n",
    "        print(f\"      g300_gen2 (success rate: {g300_success:.1f}% vs {g1000_success:.1f}%)\")\n",
    "    elif g1000_success > g300_success:\n",
    "        print(f\"      g1000_gen4 (success rate: {g1000_success:.1f}% vs {g300_success:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"      Tie (both {g300_success:.1f}%)\")\n",
    "\n",
    "print(\"\\n  Q4: What predicts success?\")\n",
    "for config in dns_ga_configs:\n",
    "    converged_data = convergence_df[(convergence_df['config_name'] == config['name']) & \n",
    "                                   (convergence_df['converged'])]\n",
    "    if len(converged_data) >= 3:\n",
    "        corr = np.corrcoef(converged_data['baseline_convergence_qd'].values,\n",
    "                          converged_data['pct_savings'].values)[0, 1]\n",
    "        \n",
    "        if abs(corr) > 0.5:\n",
    "            strength = \"Strong\"\n",
    "        elif abs(corr) > 0.3:\n",
    "            strength = \"Moderate\"\n",
    "        else:\n",
    "            strength = \"Weak\"\n",
    "        \n",
    "        direction = \"positive\" if corr > 0 else \"negative\"\n",
    "        print(f\"      {config['name']}: {strength} {direction} correlation (r={corr:.3f})\")\n",
    "\n",
    "print(\"\\nðŸ’¡ IMPLICATIONS FOR PUBLICATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "  1. HONEST TITLE: Use 'overall success rate' in abstract/title\n",
    "     Example: \"Competition-GA accelerates convergence in X% of cases\"\n",
    "  \n",
    "  2. KEY CLAIM: Report mean savings WITH standard deviation\n",
    "     Example: \"On successful seeds, Competition-GA saves X% Â± Y% evaluations\"\n",
    "  \n",
    "  3. LIMITATIONS: Acknowledge seed dependency\n",
    "     Example: \"Success rate varies significantly with random seed (X% overall)\"\n",
    "  \n",
    "  4. RECOMMENDATION: Specify when Competition-GA is beneficial\n",
    "     - If correlation is strong: \"Most effective when baseline QD is high/low\"\n",
    "     - If success rate < 50%: \"Recommend exploratory use, not production\"\n",
    "     - If success rate > 50%: \"Reliable improvement in majority of cases\"\n",
    "  \n",
    "  5. FUTURE WORK: Address seed dependency\n",
    "     - Investigate seed characteristics that predict success\n",
    "     - Develop adaptive GA triggering based on convergence trajectory\n",
    "     - Test ensemble methods across multiple seeds\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“ STEP 8 COMPLETE - All research questions answered!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4698682",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ NOTEBOOK COMPLETE\n",
    "\n",
    "**All 8 steps implemented:**\n",
    "- âœ… Step 1: Setup and Configuration\n",
    "- âœ… Step 2: Helper Functions\n",
    "- âœ… Step 3: Parallel Experiment Runner\n",
    "- âœ… Step 4: Build Queue and Execute (94 experiments)\n",
    "- âœ… Step 5: Convergence Efficiency Analysis\n",
    "- âœ… Step 6: Statistical Tests\n",
    "- âœ… Step 7: Visualizations\n",
    "- âœ… Step 8: Final Research Conclusions\n",
    "\n",
    "**To run the full experiment:**\n",
    "1. Execute all cells in order (Runtime â†’ Run all)\n",
    "2. Experiments will take ~2.6 hours (2 parallel workers)\n",
    "3. Results saved to `seed_variability_logs/`\n",
    "4. All visualizations and statistics auto-generated\n",
    "\n",
    "**Key outputs:**\n",
    "- `random_seeds.json` - The 31 random seeds used\n",
    "- `all_results_{timestamp}.json` - Raw experiment results\n",
    "- `convergence_analysis_{timestamp}.csv` - Convergence efficiency per seed\n",
    "- `boxplot_savings_{timestamp}.png` - Distribution visualization\n",
    "- `histograms_savings_{timestamp}.png` - Detailed distributions\n",
    "- `success_rates_{timestamp}.png` - Success rate comparison\n",
    "- `scatter_baseline_vs_savings_{timestamp}.png` - Predictive analysis\n",
    "\n",
    "**Research questions answered:**\n",
    "1. Is seed 42 an outlier? â†’ Statistical analysis with z-scores\n",
    "2. True success rate? â†’ Overall % of seeds with positive savings\n",
    "3. Better config? â†’ g300_gen2 vs g1000_gen4 comparison\n",
    "4. Predictive factors? â†’ Correlation analysis\n",
    "\n",
    "Ready to discover Competition-GA's true performance across 31 seeds! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
